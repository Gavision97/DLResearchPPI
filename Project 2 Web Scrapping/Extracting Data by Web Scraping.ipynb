{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9213a50-9b83-469c-8e45-393bd4df4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbd95f05-1b84-4109-9e25-79a3cc8ab52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] += r'C:\\Users\\gavvi\\ChromeDrivers\\chrome-win64\\chrome-win64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b0ae8dc-31c1-41f9-b8c0-9596fb89c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url = \"https://skb-insilico.com/dlip/compound-search/ppi-library/rule-based\"\n",
    "driver.get(url)\n",
    "\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a470c44-c7cb-44ea-a30d-0f8671cedd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on the \"All\" button\n",
    "\n",
    "all_btn = driver.find_element(By.XPATH, \"//button[text()=' All']\")\n",
    "all_btn.click()\n",
    "\n",
    "driver.implicitly_wait(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1c7ec75-a2d6-469f-afd2-d7279719cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.CLASS_NAME, 'btn-green'))\n",
    ")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a76ea2dd-6b9f-44b8-8379-c725fc4c8642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DLiP-ID Mol Image       MW  XLogP HBA HBD     PSA nRotatableBonds nRings\n",
      "0      D00000            499.449   3.49   4   1    50.8               4      5\n",
      "1      D00001            490.579   4.05   5   0   67.67               6      5\n",
      "2      D00002             505.08  3.868   5   2    84.5               7      4\n",
      "3      D00003            450.583  2.666   5   3    98.9               7      4\n",
      "4      D00004            451.567  2.902   5   3    90.9               7      4\n",
      "...       ...       ...      ...    ...  ..  ..     ...             ...    ...\n",
      "15223  D00BQH            598.748    4.2   7   3  125.55              11      5\n",
      "15224  D00BQI             643.76  3.907   7   2   113.1              11      6\n",
      "15225  D00BQJ            620.238  6.435   4   4  102.57               8      6\n",
      "15226  D00BQK             625.77  5.002   7   2   113.1              10      6\n",
      "15227  D00BQL            645.751  5.476   7   2   97.72              10      6\n",
      "\n",
      "[15228 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the HTML content of the table\n",
    "table_html = table.get_attribute('outerHTML')\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(table_html, 'html.parser')\n",
    "\n",
    "# Extract column headers\n",
    "header = [th.text for th in soup.find_all('th')]\n",
    "\n",
    "# Extract table data manually\n",
    "data = []\n",
    "for row in soup.find_all('tr')[1:]:  # Skip the first row as it contains headers\n",
    "    row_data = [td.text for td in row.find_all('td')]\n",
    "    data.append(row_data)\n",
    "\n",
    "# Construct DataFrame\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Loop through pages\n",
    "page_number = 1\n",
    "while True:\n",
    "    try:\n",
    "        # Wait for the loading overlay to disappear\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.invisibility_of_element_located((By.CLASS_NAME, \"loadingoverlay\"))\n",
    "        )\n",
    "        \n",
    "        # Find the \"Next\" button\n",
    "        next_button = driver.find_element(By.XPATH, '//*[@id=\"compound-list-table_next\"]/a')\n",
    "        \n",
    "        # Click on the \"Next\" button\n",
    "        next_button.click()\n",
    "\n",
    "        # Wait for the table to be present on the next page\n",
    "        time.sleep(3)  # Adjust this sleep time as needed\n",
    "        table = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"dataTables_scrollBody\"))\n",
    "        )\n",
    "\n",
    "        # Extract the HTML content of the table\n",
    "        table_html = table.get_attribute('outerHTML')\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')\n",
    "\n",
    "        # Extract table data manually\n",
    "        for row in soup.find_all('tr')[1:]:\n",
    "            row_data = [td.text for td in row.find_all('td')]\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Increment the page number\n",
    "        page_number += 1\n",
    "\n",
    "        # Check if there are more pages\n",
    "        if 'disabled' in next_button.get_attribute('class') or page_number > 609:\n",
    "            break\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        continue\n",
    "\n",
    "# Construct the final DataFrame\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0820f2c2-4f57-4566-9dc6-c7d94b75b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = \"DLiP_AD.csv\"\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38ea3d-e973-40b6-8e1f-9619d480d0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
