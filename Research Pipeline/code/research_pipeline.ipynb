{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcec6a4-5f75-4ba0-860f-a42b21a6cb6a",
   "metadata": {},
   "source": [
    "## Import Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23e64c9-915f-420c-b3ea-cc887128bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8154c685-2c72-416a-99c2-c0405da05707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "2024-10-21 14:14:57.721581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-21 14:14:57.734281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-21 14:14:57.738219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-21 14:14:57.748942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'wrapt'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'wrapt'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/home/gavrilev/.conda/envs/chemprop/lib/python3.11/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/gavrilev/.conda/envs/chemprop/lib/python3.11/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'wrapt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Abstract classes\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Chemprop and DeepChem\n",
    "import chemprop\n",
    "from chemprop import data, featurizers, models\n",
    "import deepchem as dc\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Hugging Face Transformers for ChemBERTa\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaModel, RobertaConfig, \n",
    "    AdamW, get_linear_schedule_with_warmup, BertModel\n",
    ")\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Pandas and NumPy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Draw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f205d5-fc7f-4041-98fe-d733db10df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRINT() -> None: print(f\"{'-'*80}\\nDone\\n{'-'*80}\")\n",
    "def PRINTC() -> None: print(f\"{'-'*80}\")\n",
    "def PRINTM(M) -> None: print(f\"{'-'*80}\\n{M}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292be2e8-c26d-47c5-875e-ec1d8b8a42b1",
   "metadata": {},
   "source": [
    "## Verify GPU Availability ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca0e2bd-ebdf-469d-b3a3-2719d508ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 21 14:15:00 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   36C    P8             17W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b9a93-d655-4fc5-8113-9e492c4d34d5",
   "metadata": {},
   "source": [
    "For this task, we'll use the BGU cluster GPU `NVIDIA RTX 6000 Ada Generation` to achieve better performance during the training of our pre-trained and fine-tuned models, allowing for more efficient processing of large datasets and complex computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94354f56-7aa2-4105-a2e0-8a0a8efc3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "GPU is available.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    PRINTM(f\"GPU is available.\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    PRINTM(f\"GPU is not available. Using CPU instead.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953e7dc3-c267-4368-b488-d467d19ac35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch version: 2.3.1+cu121\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "CUDA available: True\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "CUDA version:  12.1\n",
      "--------------------------------------------------------------------------------\n",
      "CUDA device: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "PRINTM(f\"PyTorch version: {torch.__version__}\")\n",
    "PRINTM(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "PRINTM(f\"CUDA version:  {torch.version.cuda}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No CUDA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38e20ae-6976-485e-9d68-b1b6329f3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_uniprot_ids(dataset, mapping_df):\n",
    "    # Create a dictionary from the mapping dataframe\n",
    "    mapping_dict = mapping_df.set_index('From')['Entry'].to_dict()\n",
    "\n",
    "    # Map the uniprot_id1 and uniprot_id2 columns to their respective Entry values\n",
    "    dataset['uniprot_id1'] = dataset['uniprot_id1'].map(mapping_dict)\n",
    "    dataset['uniprot_id2'] = dataset['uniprot_id2'].map(mapping_dict)\n",
    "    return dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061d1ac9-545e-4bce-9244-799ce5390949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedChempropModel(nn.Module):\n",
    "    def __init__(self, checkpoints_path, batch_size):\n",
    "        super(PretrainedChempropModel, self).__init__()\n",
    "        self.mpnn = self.load_pretrained_model(checkpoints_path)\n",
    "        self.featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, smiles):\n",
    "        # Prepare the data in order to generate embeddings from modulators SMILES\n",
    "        self.smiles_data = [data.MoleculeDatapoint.from_smi(smi) for smi in smiles]\n",
    "        self.smiles_dset = data.MoleculeDataset(self.smiles_data, featurizer=self.featurizer)\n",
    "        self.smiles_loader = data.build_dataloader(self.smiles_dset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        embeddings = [\n",
    "            # Etract the embedding from the last FFN layer, i.e., before the final prediction(thus i=-1)\n",
    "            self.mpnn.predictor.encode(self.fingerprints_from_batch_molecular_graph(batch, self.mpnn), i=-1) \n",
    "            for batch in self.smiles_loader\n",
    "        ]\n",
    "        #print(embeddings)\n",
    "        if not embeddings:\n",
    "             return torch.empty(0, device=device)\n",
    "        embeddings = torch.cat(embeddings, 0).to(device)\n",
    "        return embeddings\n",
    "\n",
    "    def fingerprints_from_batch_molecular_graph(self, batch, mpnn):\n",
    "        batch.bmg.to(device)\n",
    "        H_v = mpnn.message_passing(batch.bmg, batch.V_d)\n",
    "        H = mpnn.agg(H_v, batch.bmg.batch)\n",
    "        H = mpnn.bn(H)\n",
    "        fingerprints = H if batch.X_d is None else torch.cat((H, mpnn.batch.X_d_transform(X_d)), 1)\n",
    "        return fingerprints\n",
    "\n",
    "    def load_pretrained_model(self, checkpoints_path):\n",
    "        mpnn = models.MPNN.load_from_checkpoint(checkpoints_path).to(device)\n",
    "        return mpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6afa3216-ac3d-4af1-b53d-a7c9357f9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemBERTaPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChemBERTaPT, self).__init__()\n",
    "        self.model_name = \"DeepChem/ChemBERTa-77M-MTR\"\n",
    "        self.chemberta = RobertaModel.from_pretrained(self.model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.chemberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return bert_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fe2ba-6fa4-466e-9cb1-6643de52c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor 'a' with specified values for testing\n",
    "# First embedding: [1, 1, 1, 1] repeated 16 times (to make 64 samples of size 4)\n",
    "# Second embedding: [0, 0, 0, 0] repeated 16 times\n",
    "first_embedding = torch.tensor([[1, 1, 1, 1]] * 16)  # shape: [16, 4]\n",
    "second_embedding = torch.tensor([[0, 0, 0, 0]] * 16)  # shape: [16, 4]\n",
    "\n",
    "# Concatenate both embeddings along the second dimension\n",
    "a = torch.cat((first_embedding, second_embedding), dim=1)  # shape: [16, 8]\n",
    "print(f'Vector \"a\" shape -> {a.shape}')\n",
    "\n",
    "# Split the tensor into two parts along the second dimension\n",
    "a1, a2 = torch.split(a, 4, dim=1)  # shape of each: [16, 4]\n",
    "\n",
    "print(f'Vector \"a1\" and \"a2\" shape -> {a1.shape}')\n",
    "\n",
    "# Print the results\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ee975c-52f9-46f1-919a-d249cd21e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fingerprints(unique_smiles_list: List[str], radius: int = 2, n_bits: int = 1024) -> Dict[str, AllChem.rdchem.Mol]:\n",
    "    \"\"\"Compute Morgan fingerprints for a list of unique SMILES strings.\"\"\"\n",
    "    fingerprints = {}\n",
    "    for smi in unique_smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        else:\n",
    "            # Handle invalid SMILES strings by adding a zeroed fingerprint\n",
    "            zero_mol = Chem.MolFromSmiles('')\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(zero_mol, radius, nBits=n_bits)\n",
    "        fingerprints[smi] = fp\n",
    "    return fingerprints\n",
    "\n",
    "def cluster_fingerprints(fingerprints: Dict[str, AllChem.rdchem.Mol], cutoff: float = 0.6) -> List[Tuple[int]]:\n",
    "    \"\"\"Cluster fingerprints using the Butina algorithm.\"\"\"\n",
    "    fps = list(fingerprints.values())\n",
    "    n_fps = len(fps)\n",
    "    dists = []\n",
    "    for i in range(1, n_fps):\n",
    "        sims = AllChem.DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n",
    "        dists.extend([1 - x for x in sims])\n",
    "    clusters = Butina.ClusterData(dists, n_fps, cutoff, isDistData=True)\n",
    "    return clusters\n",
    "\n",
    "def custom_butina_splitter(\n",
    "    df: pd.DataFrame,\n",
    "    clusters: List[Tuple[int]],\n",
    "    unique_smiles_list: List[str],\n",
    "    smiles_col: str = 'smiles',\n",
    "    frac_train: float = 0.78,\n",
    "    frac_test: float = 0.22,\n",
    "    random_state: int = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Perform a custom Butina split into train and test sets with randomness.\"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Shuffle clusters to introduce randomness\n",
    "    cluster_indices = list(range(len(clusters)))\n",
    "    rng.shuffle(cluster_indices)\n",
    "\n",
    "    # Desired split sizes based on molecule counts\n",
    "    total_molecules = len(df)\n",
    "    desired_train_size = int(frac_train * total_molecules)\n",
    "    desired_test_size = total_molecules - desired_train_size  # Ensure total sums up\n",
    "\n",
    "    # Initialize cumulative counts and splits\n",
    "    train_smiles = set()\n",
    "    test_smiles = set()\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "\n",
    "    # Iterate over clusters and assign to splits\n",
    "    for idx in cluster_indices:\n",
    "        cluster = clusters[idx]\n",
    "        cluster_smiles = [unique_smiles_list[i] for i in cluster]\n",
    "\n",
    "        # Count the number of molecules in the cluster (including duplicates in df)\n",
    "        cluster_molecule_count = df[df[smiles_col].isin(cluster_smiles)].shape[0]\n",
    "\n",
    "        # Decide where to assign the cluster\n",
    "        # Compute remaining needed molecules for each split\n",
    "        remaining_train = desired_train_size - train_count\n",
    "        remaining_test = desired_test_size - test_count\n",
    "\n",
    "        # Prepare a dictionary of remaining counts\n",
    "        remaining = {'train': remaining_train, 'test': remaining_test}\n",
    "\n",
    "        # Assign to the split with the largest remaining count\n",
    "        possible_splits = {k: v for k, v in remaining.items() if v > 0}\n",
    "\n",
    "        if not possible_splits:\n",
    "            # All splits have reached or exceeded desired sizes\n",
    "            # Assign to the split with the least total molecules\n",
    "            total_counts = {'train': train_count, 'test': test_count}\n",
    "            split = min(total_counts, key=total_counts.get)\n",
    "        else:\n",
    "            # Assign to the split needing the most molecules\n",
    "            split = max(possible_splits, key=possible_splits.get)\n",
    "\n",
    "        # Assign the cluster to the chosen split\n",
    "        if split == 'train':\n",
    "            train_smiles.update(cluster_smiles)\n",
    "            train_count += cluster_molecule_count\n",
    "        else:\n",
    "            test_smiles.update(cluster_smiles)\n",
    "            test_count += cluster_molecule_count\n",
    "\n",
    "    # Use isin to get the splits from the original dataframe\n",
    "    train = df[df[smiles_col].isin(train_smiles)].copy()\n",
    "    test_combined = df[df[smiles_col].isin(test_smiles)].copy()\n",
    "\n",
    "    return train, test_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca8b273-3b7f-4037-89f1-17839cc05222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomButinaSplitter:\n",
    "    def __init__(self, smiles_col='smiles', label_col='label', cutoff=0.6):\n",
    "        self.smiles_col = smiles_col\n",
    "        self.label_col = label_col\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def split_dataset(self, df: pd.DataFrame) -> List[Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]]:\n",
    "        unique_smiles = df[self.smiles_col].drop_duplicates().reset_index(drop=True)\n",
    "        print(f\"Number of unique SMILES: {len(unique_smiles)}\")\n",
    "\n",
    "        # Compute fingerprints for unique SMILES (only once)\n",
    "        unique_smiles_list = unique_smiles.tolist()\n",
    "        fingerprints_dict = self._compute_fingerprints(unique_smiles_list)\n",
    "\n",
    "        # Cluster the fingerprints (only once)\n",
    "        clusters = self._cluster_fingerprints(fingerprints_dict, cutoff=self.cutoff)\n",
    "\n",
    "        # Prepare for splitting\n",
    "        splits = []\n",
    "        for random_seed in [11, 7, 8, 1, 4]:\n",
    "            print(f\"Random Seed: {random_seed}\")\n",
    "\n",
    "            # Perform initial train/test split using Butina clustering\n",
    "            train, test_combined = self._custom_butina_splitter(\n",
    "                df,\n",
    "                clusters,\n",
    "                unique_smiles_list,\n",
    "                smiles_col=self.smiles_col,\n",
    "                frac_train=0.78,\n",
    "                frac_test=0.22,\n",
    "                random_state=random_seed\n",
    "            )\n",
    "\n",
    "            # Now split test_combined into validation and test sets using stratified split\n",
    "            labels = test_combined[self.label_col]\n",
    "\n",
    "            # Split test_combined into validation and test sets (each 50% of test_combined)\n",
    "            valid, test = train_test_split(\n",
    "                test_combined,\n",
    "                test_size=0.5,\n",
    "                random_state=random_seed,  # Use the same random seed for reproducibility\n",
    "                stratify=labels\n",
    "            )\n",
    "\n",
    "            splits.append((train, valid, test))\n",
    "\n",
    "            # Verify that the splits do not overlap\n",
    "            train_indices = set(train.index)\n",
    "            valid_indices = set(valid.index)\n",
    "            test_indices = set(test.index)\n",
    "            assert len(train_indices & valid_indices) == 0\n",
    "            assert len(train_indices & test_indices) == 0\n",
    "            assert len(valid_indices & test_indices) == 0\n",
    "\n",
    "            # Calculate actual split sizes\n",
    "            total_molecules = len(df)\n",
    "            actual_train_frac = len(train) / total_molecules\n",
    "            actual_valid_frac = len(valid) / total_molecules\n",
    "            actual_test_frac = len(test) / total_molecules\n",
    "\n",
    "            print(f\"Train size: {len(train)} ({actual_train_frac:.2%}), \"\n",
    "                  f\"Valid size: {len(valid)} ({actual_valid_frac:.2%}), \"\n",
    "                  f\"Test size: {len(test)} ({actual_test_frac:.2%})\")\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def _compute_fingerprints(self, unique_smiles_list: List[str], radius: int = 2, n_bits: int = 1024) -> Dict[str, AllChem.rdchem.Mol]:\n",
    "        \"\"\"Private method to compute Morgan fingerprints for a list of unique SMILES strings.\"\"\"\n",
    "        fingerprints = {}\n",
    "        for smi in unique_smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol:\n",
    "                fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "            else:\n",
    "                # Handle invalid SMILES strings by adding a zeroed fingerprint\n",
    "                zero_mol = Chem.MolFromSmiles('')\n",
    "                fp = AllChem.GetMorganFingerprintAsBitVect(zero_mol, radius, nBits=n_bits)\n",
    "            fingerprints[smi] = fp\n",
    "        return fingerprints\n",
    "\n",
    "    def _cluster_fingerprints(self, fingerprints: Dict[str, AllChem.rdchem.Mol], cutoff: float) -> List[Tuple[int]]:\n",
    "        \"\"\"Private method to cluster fingerprints using the Butina algorithm.\"\"\"\n",
    "        fps = list(fingerprints.values())\n",
    "        n_fps = len(fps)\n",
    "        dists = []\n",
    "        for i in range(1, n_fps):\n",
    "            sims = AllChem.DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n",
    "            dists.extend([1 - x for x in sims])\n",
    "        clusters = Butina.ClusterData(dists, n_fps, cutoff, isDistData=True)\n",
    "        return clusters\n",
    "\n",
    "    def _custom_butina_splitter(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        clusters: List[Tuple[int]],\n",
    "        unique_smiles_list: List[str],\n",
    "        smiles_col: str = 'smiles',\n",
    "        frac_train: float = 0.78,\n",
    "        frac_test: float = 0.22,\n",
    "        random_state: int = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Private method to perform a custom Butina split into train and test sets.\"\"\"\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "        # Shuffle clusters to introduce randomness\n",
    "        cluster_indices = list(range(len(clusters)))\n",
    "        rng.shuffle(cluster_indices)\n",
    "\n",
    "        total_molecules = len(df)\n",
    "        desired_train_size = int(frac_train * total_molecules)\n",
    "        desired_test_size = total_molecules - desired_train_size\n",
    "\n",
    "        train_smiles = set()\n",
    "        test_smiles = set()\n",
    "        train_count = 0\n",
    "        test_count = 0\n",
    "\n",
    "        # Iterate over clusters and assign to splits\n",
    "        for idx in cluster_indices:\n",
    "            cluster = clusters[idx]\n",
    "            cluster_smiles = [unique_smiles_list[i] for i in cluster]\n",
    "            cluster_molecule_count = df[df[smiles_col].isin(cluster_smiles)].shape[0]\n",
    "\n",
    "            remaining_train = desired_train_size - train_count\n",
    "            remaining_test = desired_test_size - test_count\n",
    "\n",
    "            remaining = {'train': remaining_train, 'test': remaining_test}\n",
    "            possible_splits = {k: v for k, v in remaining.items() if v > 0}\n",
    "\n",
    "            if not possible_splits:\n",
    "                total_counts = {'train': train_count, 'test': test_count}\n",
    "                split = min(total_counts, key=total_counts.get)\n",
    "            else:\n",
    "                split = max(possible_splits, key=possible_splits.get)\n",
    "\n",
    "            if split == 'train':\n",
    "                train_smiles.update(cluster_smiles)\n",
    "                train_count += cluster_molecule_count\n",
    "            else:\n",
    "                test_smiles.update(cluster_smiles)\n",
    "                test_count += cluster_molecule_count\n",
    "\n",
    "        train = df[df[smiles_col].isin(train_smiles)].copy()\n",
    "        test_combined = df[df[smiles_col].isin(test_smiles)].copy()\n",
    "\n",
    "        return train, test_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb8e1937-eb97-4680-8700-ba37efacdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/10 - for ablation table\n",
    "class AbstractModel(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AbstractModel, self).__init__()\n",
    "        self.early_stopping_patience = 5\n",
    "        self.delta = 0.001\n",
    "        self.butinaSplitter = CustomButinaSplitter()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, cpe, esm, fegs, gae, cbae, morgan_fingerprints, chemical_descriptors):\n",
    "        pass\n",
    "          \n",
    "    def train_val_test_model(self, dataset, num_epochs, optimizer, criterion, \n",
    "                        batch_size=32, device='cuda', num_workers=5):\n",
    "            \n",
    "            best_val_auc = float('-inf')\n",
    "            no_improve_epochs = 0\n",
    "            best_model_state_dict = None\n",
    "            splits = []\n",
    "            smiles_df = dataset[[\"smiles\"]].drop_duplicates()\n",
    "            smiles_col = 'smiles'\n",
    "            \n",
    "            # split dataset into 5 folds of (train, val, test) dataframes using custom butina splitter obj.\n",
    "            splits = self.butinaSplitter.split_dataset(dataset)\n",
    "                    \n",
    "            for fold_number, (train_subset, val_subset, test_subset) in enumerate(splits, 1):\n",
    "                print(f\"fold number {fold_number}\")\n",
    "                PRINTC()\n",
    "                train_df, val_df, test_df = train_subset, val_subset, test_subset\n",
    "                test_dataset = MoleculeDataset(test_df)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "                \n",
    "                bootstrap_valid_aucs = []\n",
    "                bootstrap_test_aucs = []\n",
    "                                \n",
    "                for bootsrap in range(5):\n",
    "                    print(f\"bootsrap number: {bootsrap + 1}\")\n",
    "                    PRINTC()\n",
    "                    seed_train = fold_number*1000 + bootsrap + 1\n",
    "                    labels_list = train_df['label'].values\n",
    "                    train_b = resample(train_df, random_state=seed_train, stratify=labels_list)\n",
    "\n",
    "                    train_dataset = MoleculeDataset(train_b)                    \n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "                    val_dataset = MoleculeDataset(val_df)                    \n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "                    last_test_auc = 0  # Initialize the last test AUC for this fold\n",
    "                    for epoch in range(num_epochs):\n",
    "                        start_time = time.time()\n",
    "                        self.train()\n",
    "                        epoch_loss = 0\n",
    "                        all_preds = []\n",
    "                        all_labels = []\n",
    "                        running_loss = 0.0\n",
    "                        for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                             batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                            \n",
    "                            batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                            batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                            batch_esm_features = batch_esm_features.to(device)\n",
    "                            batch_fegs_features = batch_fegs_features.to(device)\n",
    "                            batch_gae_features = batch_gae_features.to(device)\n",
    "                            batch_morgan = batch_morgan.to(device)\n",
    "                            batch_chem_desc = batch_chem_desc.to(device)\n",
    "                            batch_labels = batch_labels.to(device)\n",
    "                                \n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                                           batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc) \n",
    "                \n",
    "                            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            epoch_loss += loss.item()\n",
    "                \n",
    "                            all_labels.extend(batch_labels.cpu().numpy())\n",
    "                            all_preds.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                            \n",
    "                        train_auc = roc_auc_score(all_labels, all_preds)\n",
    "                                \n",
    "                        # Evaluate the model on the validation set\n",
    "                        all_val_labels = []\n",
    "                        all_val_outputs = []\n",
    "                        self.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                                \n",
    "                                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                                batch_esm_features = batch_esm_features.to(device)\n",
    "                                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                                batch_gae_features = batch_gae_features.to(device)\n",
    "                                batch_morgan = batch_morgan.to(device)\n",
    "                                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                                batch_labels = batch_labels.to(device)\n",
    "                                                \n",
    "                                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                                \n",
    "                                all_val_labels.extend(batch_labels.cpu().numpy())\n",
    "                                all_val_outputs.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                                \n",
    "                        all_val_labels = np.array(all_val_labels)\n",
    "                        all_val_outputs = np.array(all_val_outputs)\n",
    "                        \n",
    "                        # Perform bootstrapping on predictions and labels (validation phase)\n",
    "                        current_b_aucs = []\n",
    "                        N_test = all_val_labels.shape[0]\n",
    "                        for b in range(1000):\n",
    "                            seed_value = epoch * 1000 + b + (bootsrap+1)*1000  # or any function of your parameters\n",
    "                            np.random.seed(seed_value)\n",
    "                            indices = np.random.randint(0, N_test, size=N_test)\n",
    "                            y_valid_pred_b = all_val_outputs[indices]\n",
    "                            y_valid_b = all_val_labels[indices]\n",
    "                            valid_auc = roc_auc_score(y_valid_b, y_valid_pred_b)\n",
    "                            current_b_aucs.append(valid_auc)\n",
    "                \n",
    "                        mean_val_auc = np.mean(current_b_aucs)\n",
    "                        end_time = time.time()\n",
    "                        epoch_time = (end_time - start_time) / 60\n",
    "                        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {(epoch_loss/len(train_loader)):.4f}, Train AUC: {train_auc:.4f}, Mean Validation AUC: {mean_val_auc:.4f}, Epoch Time: {epoch_time:.4f}')\n",
    "\n",
    "                        # Early stopping logic\n",
    "                        if mean_val_auc > best_val_auc:\n",
    "                            best_val_auc = mean_val_auc\n",
    "                            epochs_without_improvement = 0\n",
    "                            # Save the best model state dict\n",
    "                            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "                        else:\n",
    "                            epochs_without_improvement += 1\n",
    "            \n",
    "                        if epochs_without_improvement >= self.early_stopping_patience:\n",
    "                            print(\"Early stopping triggered\")\n",
    "                            break\n",
    "                            \n",
    "                    # Load the best model in order to evaluate it on the test set\n",
    "                    self.load_state_dict(best_model_state_dict)\n",
    "                    \n",
    "                    all_test_labels = []\n",
    "                    all_test_outputs = []\n",
    "                    self.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                                batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                                \n",
    "                            batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                            batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                            batch_esm_features = batch_esm_features.to(device)\n",
    "                            batch_fegs_features = batch_fegs_features.to(device)\n",
    "                            batch_gae_features = batch_gae_features.to(device)\n",
    "                            batch_morgan = batch_morgan.to(device)\n",
    "                            batch_chem_desc = batch_chem_desc.to(device)\n",
    "                            batch_labels = batch_labels.to(device)\n",
    "                                                \n",
    "                            outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                                           batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                                \n",
    "                            all_test_labels.extend(batch_labels.cpu().numpy())\n",
    "                            all_test_outputs.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                                \n",
    "                    all_test_labels = np.array(all_test_labels)\n",
    "                    all_test_outputs = np.array(all_test_outputs)                 \n",
    "\n",
    "                    # Perform bootstrapping on predictions and labels (test phase)\n",
    "                    current_b_aucs = []\n",
    "                    N_test = all_test_labels.shape[0]\n",
    "                    for b in range(1000):\n",
    "                        seed_value = epoch * 1000 + b + (bootsrap+1)*1000  # or any function of your parameters\n",
    "                        np.random.seed(seed_value)\n",
    "                        indices = np.random.randint(0, N_test, size=N_test)\n",
    "                        y_test_pred_b = all_test_outputs[indices]\n",
    "                        y_test_b = all_test_labels[indices]\n",
    "                        test_auc = roc_auc_score(y_test_b, y_test_pred_b)\n",
    "                        current_b_aucs.append(test_auc)\n",
    "                \n",
    "                    mean_test_auc = np.mean(current_b_aucs)\n",
    "                    print(f'Bootstrap {bootsrap}, Mean Test AUC: {mean_test_auc:.4f}')\n",
    "\n",
    "                    # Store the best validation and test AUCs for this bootstrap\n",
    "                    bootstrap_valid_aucs.append(best_val_auc)\n",
    "                    bootstrap_test_aucs.append(mean_test_auc)\n",
    "                \n",
    "                # Compute mean validation and test AUCs for the current fold\n",
    "                current_fold_mean_valid_auc = np.mean(bootstrap_valid_aucs)\n",
    "                current_fold_mean_test_auc = np.mean(bootstrap_test_aucs)\n",
    "                print(f\"Fold {fold_number} Mean Validation AUC: {current_fold_mean_valid_auc:.4f}\")\n",
    "                print(f\"Fold {fold_number} Mean Test AUC: {current_fold_mean_test_auc:.4f}\")\n",
    "                all_folds_valid_aucs.append(current_fold_mean_valid_auc)\n",
    "                all_folds_test_aucs.append(current_fold_mean_test_auc)\n",
    "            \n",
    "            PRINTC()               \n",
    "            print(\"Final Mean Validation AUC across all folds:\", np.mean(all_folds_valid_aucs))\n",
    "            print(\"Validation AUCs for all folds:\", all_folds_valid_aucs)\n",
    "            print(\"Final Mean Test AUC across all folds:\", np.mean(all_folds_test_aucs))\n",
    "            print(\"Test AUCs for all folds:\", all_folds_test_aucs)                                    \n",
    "\n",
    "    def train_model(self, fold, num_epochs, dataset, optimizer, criterion, \n",
    "                    batch_size=32, device='cuda', num_workers=5):\n",
    "        \n",
    "        train_dataset = MoleculeDataset(dataset)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        print(f'Start training {fold} for {num_epochs} epochs !')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc) \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Calculate epoch loss & AUC\n",
    "            train_loss /= len(train_loader)\n",
    "            train_auc = roc_auc_score(all_labels, all_outputs)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "            print(f\"Epoch {epoch+1} Time: {epoch_time:.2f},Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "\n",
    "\n",
    "        \n",
    "    def train_val_model(self, fold, num_epochs, dataset, optimizer, criterion, \n",
    "                    batch_size=32, device='cuda', num_workers=5):\n",
    "        best_val_auc = float('-inf')\n",
    "        no_improve_epochs = 0\n",
    "\n",
    "        X = dataset.copy().drop(columns=['smiles'])\n",
    "        ids = dataset['smiles']\n",
    "        # extract labels for scaffold split (in order to make sure we got balance train & validation sets)\n",
    "        labels = dataset['label'].values\n",
    "        dc_dataset = dc.data.DiskDataset.from_numpy(X=X ,y=labels ,w=np.zeros(len(X)),ids=ids)\n",
    "        splitter = dc.splits.ScaffoldSplitter()\n",
    "        train_idx, val_idx, _ = splitter.split(dc_dataset, frac_train=0.8, frac_valid=0.2, frac_test=0)\n",
    "\n",
    "        #train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, stratify=labels, shuffle=shuffle)\n",
    "        \n",
    "        train_subset = dataset.iloc[train_idx].reset_index(drop=True)\n",
    "        val_subset = dataset.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = MoleculeDataset(train_subset)\n",
    "        val_dataset = MoleculeDataset(val_subset)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)                 \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.5f}, \"\n",
    "                  f\"Validation Accuracy: {val_accuracy:.2f}, Validation AUC: {val_auc:.5f}, Epoch Time: {epoch_time:.2f}\")\n",
    "            # Check whether val_auc > best_val_auc + delta\n",
    "            if val_auc > best_val_auc + self.delta:\n",
    "                best_val_auc = val_auc\n",
    "                train_epoch = epoch+1\n",
    "                no_improve_epochs = 0 \n",
    "                print(f\"Current best val_auc -> {val_auc:.5f}, at epoch {epoch+1}\")\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= self.early_stopping_patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        print(f'Train the model for -> {train_epoch}, best validation auc: {best_val_auc:.5f}')\n",
    "                \n",
    "    def test_model(self, test_dataset, criterion, batch_size, device, num_workers):\n",
    "        test_dataset = MoleculeDataset(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        self.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in test_loader:\n",
    "                \n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        test_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        PRINTC()\n",
    "        print(f\"Test AUC: {test_auc:.4f}\")\n",
    "        PRINTC()\n",
    "        return round(test_auc, 4)\n",
    "\n",
    "    def validate_model(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "\n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)                \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        return val_loss, accuracy, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a27d9f63-2b73-4652-ac3a-701ab96c5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTF(nn.Module):\n",
    "    def __init__(self, channels=128, r=4):\n",
    "        super(DTF, self).__init__()\n",
    "        inter_channels = int(channels // r)\n",
    "\n",
    "        self.att1 = nn.Sequential(\n",
    "            nn.Linear(channels, inter_channels),\n",
    "            nn.BatchNorm1d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(inter_channels, channels),\n",
    "            nn.BatchNorm1d(channels)\n",
    "        )\n",
    "\n",
    "        self.att2 = nn.Sequential(\n",
    "            nn.Linear(channels, inter_channels),\n",
    "            nn.BatchNorm1d(inter_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(inter_channels, channels),\n",
    "            nn.BatchNorm1d(channels)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, fd, fp):\n",
    "        w1 = self.sigmoid(self.att1(fd + fp))\n",
    "        fout1 = fd * w1 + fp * (1 - w1)\n",
    "\n",
    "        w2 = self.sigmoid(self.att2(fout1))\n",
    "        fout2 = fd * w2 + fp * (1 - w2)\n",
    "        return fout2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47a5c52c-186b-4bad-bb36-cad23b2cae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20/10 - Final expirements - all features & without finetune and attention\n",
    "class AUVG_PPI(AbstractModel):\n",
    "    def __init__(self ,dropout):\n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1750, out_features=1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1000, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )        \n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 3, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1200, out_features=1024), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        # Morgan fingerprints & chemical descriptors MLP layers\n",
    "        self.mfp_cd_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 + 194, out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        self.chemberta_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=384, out_features= 256)\n",
    "        )\n",
    "\n",
    "        # MLP for smiles_embeddings\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 3 , out_features= 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=256 + 256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=1),\n",
    "        )\n",
    "        \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, cpe, esm, fegs, gae, cbae, morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.fp_mlp(cpe)\n",
    "        cbae = self.chemberta_mlp(cbae)\n",
    "        #chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp_chem_descriptors = torch.cat([morgan_fingerprints, chemical_descriptors], dim=1)\n",
    "        mfp_chem_descriptors = self.mfp_cd_mlp(mfp_chem_descriptors)\n",
    "        \n",
    "        # Concatenate all 3 smiles embeddings along a new dimension (3x384) \n",
    "        smiles_embeddings = torch.cat([cp_fingerprints, cbae, mfp_chem_descriptors], dim=1).to(device)  # shape ->> (batch_size, 3*384)\n",
    "        smiles_embeddings = self.smiles_mlp(smiles_embeddings)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        #ppi_features = proteins.to(device)\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "\n",
    "        # Concatenate all 3 ppi embeddings along a new dimension (3x512) \n",
    "        ppi_embeddings = torch.cat([esm_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 3*512)\n",
    "        ppi_features = self.ppi_mlp(ppi_embeddings)\n",
    "\n",
    "        combined_embeddings = torch.cat([smiles_embeddings, ppi_features], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e520d4be-de77-4145-b10d-857a5d42dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20/10 - Final expirements  - without finetune & with protein attention (from last papaer)\n",
    "class AUVG_PPI(AbstractModel):\n",
    "    def __init__(self ,dropout):\n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        PRINTM('All features without finetune & with protein attention (from last papaer)')\n",
    "        self.dropout = dropout\n",
    "        self.esm_dtf = DTF(channels=1280, r=4)\n",
    "        self.gae_dtf = DTF(channels=500, r=4)\n",
    "        self.fegs_dtf = DTF(channels=578, r=2)\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 , out_features=750),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 ,out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )        \n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500, out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 3, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1200, out_features=1024), \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        # Morgan fingerprints & chemical descriptors MLP layers\n",
    "        self.mfp_cd_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 + 194, out_features= 750),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        self.chemberta_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=384, out_features= 256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        # MLP for smiles_embeddings\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 3 , out_features= 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=256 + 256, out_features=256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=1),\n",
    "        )\n",
    "        \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, cpe, esm, fegs, gae, cbae, morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.fp_mlp(cpe)\n",
    "        cbae = self.chemberta_mlp(cbae)\n",
    "        #chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp_chem_descriptors = torch.cat([morgan_fingerprints, chemical_descriptors], dim=1)\n",
    "        mfp_chem_descriptors = self.mfp_cd_mlp(mfp_chem_descriptors)\n",
    "        \n",
    "        # Concatenate all 3 smiles embeddings along a new dimension (3x384) \n",
    "        smiles_embeddings = torch.cat([cp_fingerprints, cbae, mfp_chem_descriptors], dim=1).to(device)  # shape ->> (batch_size, 3*384)\n",
    "        smiles_embeddings = self.smiles_mlp(smiles_embeddings)\n",
    "\n",
    "        # Pass all PPI features through their DTF module and then through MLP layer\n",
    "        # in order to reduce shape to (batch_size, 256)\n",
    "        esm_embedding_p1, esm_embedding_p2 = torch.split(esm, esm.shape[1] // 2, dim=1)\n",
    "        esm_embeddings = self.esm_mlp(self.esm_dtf(esm_embedding_p1, esm_embedding_p2))\n",
    "        \n",
    "        fegs_embedding_p1, fegs_embedding_p2 = torch.split(fegs, fegs.shape[1] // 2, dim=1)\n",
    "        fegs_embeddings = self.fegs_mlp(self.fegs_dtf(fegs_embedding_p1, fegs_embedding_p2))\n",
    "        \n",
    "        gae_embedding_p1, gae_embedding_p2 = torch.split(gae, gae.shape[1] // 2, dim=1)\n",
    "        gae_embeddings = self.gae_mlp(self.gae_dtf(gae_embedding_p1, gae_embedding_p2))\n",
    "\n",
    "        # Concatenate all 3 ppi embeddings along a new dimension (3x256) \n",
    "        ppi_embeddings = torch.cat([esm_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 3*512)\n",
    "        ppi_features = self.ppi_mlp(ppi_embeddings)\n",
    "\n",
    "        combined_embeddings = torch.cat([smiles_embeddings, ppi_features], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4353b779-d516-4609-91e8-9338656a0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bootstrap and new training architecture - 18/10\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, ds_):\n",
    "        self.data = ds_\n",
    "        self.mapping_df = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "        self.esm = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'esm_features.csv'))\n",
    "        self.fegs = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'fegs_features.csv'))\n",
    "\n",
    "        # In all predicted values, use zero vectors (after expirements that proved that)\n",
    "        gae_path = f'GAE_FEATURES_WITH_PREDICTED_alpha_0.csv'            \n",
    "        self.gae = pd.read_csv(os.path.join('datasets', 'GAE', gae_path))\n",
    "        self.gae.loc[self.gae['predicted'] == 1, self.gae.columns[9:509]] = 0\n",
    "        gae_features_columns = self.gae.iloc[:, 9:509]\n",
    "\n",
    "        gae_uniprot_column = self.gae[['From']].rename(columns={'From': 'UniProt_ID'})\n",
    "        self.gae = pd.concat([gae_uniprot_column, gae_features_columns], axis=1)\n",
    "        self.gae_features_ppi = self.merge_datasets(self.data, self.gae).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.esm_features_ppi = self.merge_datasets(self.data, self.esm).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.fegs_features_ppi = self.merge_datasets(self.data, self.fegs).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "\n",
    "         # SMILES RDKit features - Morgan Fingerprints (r=4, nbits=1024)  chemical descriptors, chemprop & chemBERTa\n",
    "        self.smiles_morgan_fingerprints = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_morgan_fingerprints_dataset.csv'))\n",
    "        self.smiles_chemical_descriptors = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_chem_descriptors_mapping_dataset.csv'))\n",
    "        self.chemprop = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'chemprop_features.csv'))\n",
    "        self.chemberta = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'chemBERTa_features.csv'))\n",
    "        # Necessary features for ChemBERTa model\n",
    "        self.smiles_list = self.data['smiles'].tolist()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "        self.encoded_smiles = self.tokenizer(self.smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def merge_datasets(self, dataset, features_df):\n",
    "        dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID'])\n",
    "        \n",
    "        features_df_renamed = features_df.add_suffix('_id2')\n",
    "        features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
    "        dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID', 'uniprot_id1', 'uniprot_id2'])\n",
    "        \n",
    "        # In order to avoid dropping duplicated rows that holds only zeros (in gae when there is zero vectors), which can be represents embeddings of ppi vector when\n",
    "        # specifying to reset the rows to hold only zeros\n",
    "        dataset['zero_count'] = (dataset == 0).any(axis=1).astype(int)\n",
    "        count = 1\n",
    "        for index in dataset.index:\n",
    "            if dataset.at[index, 'zero_count'] == 1:\n",
    "                dataset.at[index, 'zero_count'] = count\n",
    "                count += 1\n",
    "                \n",
    "        # Fill null values with 0\n",
    "        dataset.fillna(0, inplace=True)\n",
    "        #dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "        return dataset.drop(columns=['zero_count'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx, 0]\n",
    "        label = np.array(self.data.iloc[idx, -1], dtype=np.float32)  \n",
    "        esm_features = np.array(self.esm_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        fegs_features = np.array(self.fegs_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        gae_features = np.array(self.gae_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "\n",
    "        input_ids = self.encoded_smiles[\"input_ids\"][idx]\n",
    "        attention_mask = self.encoded_smiles[\"attention_mask\"][idx]\n",
    "\n",
    "        # Retrieve precomputed RDKit Morgan fingerprints\n",
    "        morgan_fingerprint = self.smiles_morgan_fingerprints.loc[self.smiles_morgan_fingerprints['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemical_descriptors = self.smiles_chemical_descriptors.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemprop_features = self.chemprop.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemberta_features = self.chemberta.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        \n",
    "        return (chemprop_features, esm_features, fegs_features, gae_features, \n",
    "                chemberta_features, morgan_fingerprint, chemical_descriptors, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77518d8-3ec0-4f60-955f-023f3ca577c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e6ac7-5132-43c9-bd93-9efd9893fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ablation table 12/10\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, ds_):\n",
    "        self.data = ds_\n",
    "        self.mapping_df = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "        self.esm = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'esm_features.csv'))\n",
    "        self.fegs = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'fegs_features.csv'))\n",
    "\n",
    "        # In all predicted values, use zero vectors (after expirements that proved that)\n",
    "        gae_path = f'GAE_FEATURES_WITH_PREDICTED_alpha_0.csv'            \n",
    "        self.gae = pd.read_csv(os.path.join('datasets', 'GAE', gae_path))\n",
    "        self.gae.loc[self.gae['predicted'] == 1, self.gae.columns[9:509]] = 0\n",
    "        gae_features_columns = self.gae.iloc[:, 9:509]\n",
    "\n",
    "        gae_uniprot_column = self.gae[['From']].rename(columns={'From': 'UniProt_ID'})\n",
    "        self.gae = pd.concat([gae_uniprot_column, gae_features_columns], axis=1)\n",
    "        self.gae_features_ppi = self.merge_datasets(self.data, self.gae).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.esm_features_ppi = self.merge_datasets(self.data, self.esm).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.fegs_features_ppi = self.merge_datasets(self.data, self.fegs).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        \n",
    "        # SMILES RDKit features - Morgan Fingerprints (r=4, nbits=1024) & chemical descriptors\n",
    "        self.smiles_morgan_fingerprints = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_morgan_fingerprints_dataset.csv'))\n",
    "        self.smiles_chemical_descriptors = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_chem_descriptors_mapping_dataset.csv'))\n",
    "\n",
    "        # Necessary features for ChemBERTa model\n",
    "        self.smiles_list = self.data['smiles'].tolist()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "        self.encoded_smiles = self.tokenizer(self.smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def merge_datasets(self, dataset, features_df):\n",
    "        dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID'])\n",
    "        \n",
    "        features_df_renamed = features_df.add_suffix('_id2')\n",
    "        features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
    "        dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID', 'uniprot_id1', 'uniprot_id2'])\n",
    "        \n",
    "        # In order to avoid dropping duplicated rows that holds only zeros (in gae when there is zero vectors), which can be represents embeddings of ppi vector when\n",
    "        # specifying to reset the rows to hold only zeros\n",
    "        dataset['zero_count'] = (dataset == 0).any(axis=1).astype(int)\n",
    "        count = 1\n",
    "        for index in dataset.index:\n",
    "            if dataset.at[index, 'zero_count'] == 1:\n",
    "                dataset.at[index, 'zero_count'] = count\n",
    "                count += 1\n",
    "                \n",
    "        # Fill null values with 0\n",
    "        dataset.fillna(0, inplace=True)\n",
    "        dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "        return dataset.drop(columns=['zero_count'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx, 0]\n",
    "        label = np.array(self.data.iloc[idx, -1], dtype=np.float32)  \n",
    "        esm_features = np.array(self.esm_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        fegs_features = np.array(self.fegs_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        gae_features = np.array(self.gae_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "\n",
    "        input_ids = self.encoded_smiles[\"input_ids\"][idx]\n",
    "        attention_mask = self.encoded_smiles[\"attention_mask\"][idx]\n",
    "\n",
    "        # Retrieve precomputed RDKit Morgan fingerprints\n",
    "        morgan_fingerprint = self.smiles_morgan_fingerprints.loc[self.smiles_morgan_fingerprints['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemical_descriptors = self.smiles_chemical_descriptors.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        \n",
    "        return (smiles, esm_features, fegs_features, gae_features, \n",
    "                input_ids, attention_mask, morgan_fingerprint, chemical_descriptors, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e569e55-f80f-468c-9e95-14524143e452",
   "metadata": {},
   "source": [
    "## Train and Validate ##\n",
    "\n",
    "Next we'll train the model on 80% of the data and validate it on 20% on the data in order to decide the exact number of epochs to train each fold in the training process using scaffold split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd14c3-a534-43c5-bdc2-1aa6bccd110e",
   "metadata": {},
   "source": [
    "### Load & Prepare the Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b276610-4b98-48a7-80ed-b290391d67e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Folder content:\n",
      "\n",
      "['train_fold5_5_0.75.csv', 'train_fold4_5_0.75.csv', 'train_fold3_5_0.75.csv', 'train_fold2_5_0.75.csv', 'train_fold1_5_0.75.csv', 'test_fold5_5_0.75.csv', 'test_fold4_5_0.75.csv', 'test_fold3_5_0.75.csv', 'test_fold2_5_0.75.csv', 'test_fold1_5_0.75.csv']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_folder_path = os.path.join('datasets', 'test_dataset', 'train_test_5_0.75')\n",
    "all_files = os.listdir(ds_folder_path)\n",
    "\n",
    "PRINTM(f'Folder content:\\n\\n{all_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9295d4c-ae37-40f4-8e4b-f3a4f1fad797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# Read each CSV file into a dataframe and store it in the dictionary\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(ds_folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_name = file.replace('_5_0.75.csv', '_df')\n",
    "    dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d447c08-ad11-4177-96c9-b56a85a268c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_mapping = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d601d174-176f-46c9-bdb6-e792ac9eaf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done inverse mapping & merging successfully !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df_name in dataframes.keys():\n",
    "    dataframes[df_name] = convert_uniprot_ids(dataframes[df_name], uniprot_mapping)\n",
    "    #dataframes[df_name] = merge_datasets(dataframes[df_name], ppi_features_df)\n",
    "\n",
    "# Access each dataframe using its name\n",
    "train_fold1_df = dataframes['train_fold1_df']\n",
    "train_fold2_df = dataframes['train_fold2_df']\n",
    "train_fold3_df = dataframes['train_fold3_df']\n",
    "train_fold4_df = dataframes['train_fold4_df']\n",
    "train_fold5_df = dataframes['train_fold5_df']\n",
    "test_fold1_df = dataframes['test_fold1_df']\n",
    "test_fold2_df = dataframes['test_fold2_df']\n",
    "test_fold3_df = dataframes['test_fold3_df']\n",
    "test_fold4_df = dataframes['test_fold4_df']\n",
    "test_fold5_df = dataframes['test_fold5_df']\n",
    "\n",
    "PRINTM(f'Done inverse mapping & merging successfully !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0197c469-d6b0-4966-ad51-703dcf96f036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>uniprot_id1</th>\n",
       "      <th>uniprot_id2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC1(C)CCC(c2ccc(Cl)cc2)=C(CN2CCN(c3ccc(C(=O)NS...</td>\n",
       "      <td>P06756</td>\n",
       "      <td>P18084</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1C(=O)CNC(=O)...</td>\n",
       "      <td>Q96SW2</td>\n",
       "      <td>Q9UBN7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCOc1ccc(C(C)=O)cc1Nc1cc(C(=O)OC)cc(C(=O)OC)c1</td>\n",
       "      <td>O00255</td>\n",
       "      <td>Q03164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1ccc(NC(=O)CCCCCCCSc2nnc(Cc3nn(C)c(=O)c4cccc...</td>\n",
       "      <td>Q07820</td>\n",
       "      <td>Q07812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=C1C(Cl)=C(NCCN2CCOCC2)C(=O)c2ccccc21</td>\n",
       "      <td>P10415</td>\n",
       "      <td>Q07812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C[C@H]1CC[C@@H](Oc2cccc(Sc3ccc(/C=C/C(=O)N4CCO...</td>\n",
       "      <td>P62942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O=C1[C@H](c2ccc(Cl)cc2)N(Cc2ccc(Cl)cc2F)C(=O)c...</td>\n",
       "      <td>Q07817</td>\n",
       "      <td>Q92934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CC[C@H](N)C(=O)N[C@@H]1C(=O)N2[C@H](CC[C@H]2C(...</td>\n",
       "      <td>O00255</td>\n",
       "      <td>Q03164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles uniprot_id1 uniprot_id2  \\\n",
       "0  CC1(C)CCC(c2ccc(Cl)cc2)=C(CN2CCN(c3ccc(C(=O)NS...      P06756      P18084   \n",
       "1  CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1C(=O)CNC(=O)...      Q96SW2      Q9UBN7   \n",
       "2     CCOc1ccc(C(C)=O)cc1Nc1cc(C(=O)OC)cc(C(=O)OC)c1      O00255      Q03164   \n",
       "3  Cc1ccc(NC(=O)CCCCCCCSc2nnc(Cc3nn(C)c(=O)c4cccc...      Q07820      Q07812   \n",
       "4             O=C1C(Cl)=C(NCCN2CCOCC2)C(=O)c2ccccc21      P10415      Q07812   \n",
       "5  C[C@H]1CC[C@@H](Oc2cccc(Sc3ccc(/C=C/C(=O)N4CCO...      P62942         NaN   \n",
       "6  O=C1[C@H](c2ccc(Cl)cc2)N(Cc2ccc(Cl)cc2F)C(=O)c...      Q07817      Q92934   \n",
       "7  CC[C@H](N)C(=O)N[C@@H]1C(=O)N2[C@H](CC[C@H]2C(...      O00255      Q03164   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fold1_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47dcf2b7-e656-46ed-b9f1-bbac31b689ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(batch_size,\n",
    "                  dropout) -> nn.Module:\n",
    "    checkpoints_path = os.path.join('pt_chemprop_checkpoint_r4_', 'fold_0', 'model_0', 'checkpoints', 'best-epoch=39-val_loss=0.39.ckpt')\n",
    "    pretrained_chemprop_model = PretrainedChempropModel(checkpoints_path, batch_size)\n",
    "    chemberta_model = ChemBERTaPT()\n",
    "    model = AUVG_PPI(pretrained_chemprop_model, chemberta_model, dropout).to(device)\n",
    "\n",
    "    PRINTM('Generated model successfully !')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce1fad41-52ca-406a-9a0b-05b85b994d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(batch_size,\n",
    "                  dropout) -> nn.Module:\n",
    "    model = AUVG_PPI(dropout).to(device)\n",
    "\n",
    "    PRINTM('Generated model successfully !')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d54bc157-4057-48f8-97a7-bc06ca5f249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbfd4ae7-aeb1-4508-a504-1b1117ee90c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>uniprot_id1</th>\n",
       "      <th>uniprot_id2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clc1ccc(C(c2ccc(Cl)cc2)[n+]2ccn(CC(OCc3ccc(Cl)...</td>\n",
       "      <td>Q07817</td>\n",
       "      <td>Q16611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCCN(CCCC)C(=O)c1nn(c(C)c1Cl)c2ccc(cc2C(=O)N3...</td>\n",
       "      <td>O00255</td>\n",
       "      <td>Q03164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCN(C)c1c(N[C@@H](Cc2ccc(NC(=O)c3c(Cl)cncc3Cl)...</td>\n",
       "      <td>P98170</td>\n",
       "      <td>Q9NR28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N=C(N)NCCCC(=O)N1CCC[C@@H]1C(=O)NCCC(=O)O</td>\n",
       "      <td>P25440</td>\n",
       "      <td>Q07912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC[C@@H]1CC[C@@H](C(=O)N2CCN(C)CC2)N1C(=O)C1=C...</td>\n",
       "      <td>P04637</td>\n",
       "      <td>Q00987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80215</th>\n",
       "      <td>CCOc1cc2c(cc1OCC)C(c1ccc(Cl)cc1)N(c1ccc(Cl)cc1...</td>\n",
       "      <td>P04637</td>\n",
       "      <td>Q00987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80216</th>\n",
       "      <td>C[C@H](CNC(=O)c1c(O)c(O)cc2c(O)c(c(C)cc12)c3c(...</td>\n",
       "      <td>P06756</td>\n",
       "      <td>P05106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80217</th>\n",
       "      <td>O=C(NCc1ccccc1)c1cc(Cl)c(O)c(S(=O)(=O)N(Cc2ccc...</td>\n",
       "      <td>O00255</td>\n",
       "      <td>Q03164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80218</th>\n",
       "      <td>CCOC(=O)CSc1ccc(C(=O)c2[nH]c(Cl)c(Cl)c2-n2c(C(...</td>\n",
       "      <td>Q07817</td>\n",
       "      <td>O43521</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80219</th>\n",
       "      <td>CCC(C)[C@@H](Nc1ccc(N(CC(=O)O)S(=O)(=O)c2ccc(O...</td>\n",
       "      <td>P08514</td>\n",
       "      <td>P05106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80159 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  smiles uniprot_id1  \\\n",
       "0      Clc1ccc(C(c2ccc(Cl)cc2)[n+]2ccn(CC(OCc3ccc(Cl)...      Q07817   \n",
       "1      CCCCN(CCCC)C(=O)c1nn(c(C)c1Cl)c2ccc(cc2C(=O)N3...      O00255   \n",
       "2      CCN(C)c1c(N[C@@H](Cc2ccc(NC(=O)c3c(Cl)cncc3Cl)...      P98170   \n",
       "3              N=C(N)NCCCC(=O)N1CCC[C@@H]1C(=O)NCCC(=O)O      P25440   \n",
       "4      CC[C@@H]1CC[C@@H](C(=O)N2CCN(C)CC2)N1C(=O)C1=C...      P04637   \n",
       "...                                                  ...         ...   \n",
       "80215  CCOc1cc2c(cc1OCC)C(c1ccc(Cl)cc1)N(c1ccc(Cl)cc1...      P04637   \n",
       "80216  C[C@H](CNC(=O)c1c(O)c(O)cc2c(O)c(c(C)cc12)c3c(...      P06756   \n",
       "80217  O=C(NCc1ccccc1)c1cc(Cl)c(O)c(S(=O)(=O)N(Cc2ccc...      O00255   \n",
       "80218  CCOC(=O)CSc1ccc(C(=O)c2[nH]c(Cl)c(Cl)c2-n2c(C(...      Q07817   \n",
       "80219  CCC(C)[C@@H](Nc1ccc(N(CC(=O)O)S(=O)(=O)c2ccc(O...      P08514   \n",
       "\n",
       "      uniprot_id2  label  \n",
       "0          Q16611      0  \n",
       "1          Q03164      0  \n",
       "2          Q9NR28      0  \n",
       "3          Q07912      0  \n",
       "4          Q00987      1  \n",
       "...           ...    ...  \n",
       "80215      Q00987      1  \n",
       "80216      P05106      0  \n",
       "80217      Q03164      0  \n",
       "80218      O43521      1  \n",
       "80219      P05106      0  \n",
       "\n",
       "[80159 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset = pd.read_csv(r'datasets/test_dataset/final_dataset_5_0.75_25_09_2024_without_long_uncategorized_PPIs.csv')\n",
    "final_dataset = convert_uniprot_ids(final_dataset, uniprot_mapping)\n",
    "\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565d9f2-4888-4da5-af36-41767204f69d",
   "metadata": {},
   "source": [
    "### Train, val & test the model ###\n",
    "\n",
    "The next step is to decide on the models hyperparameters. We'll train the model on 5 custom splits, and each split will be trained 5 times, with a newly initialized model each time. As a result, each fold will be bootstrapped 5 times, meaning it will be trained and validated on the validation set, where we will save the best model state during each epoch. After training (for up to 100 epochs, or until early stopping is triggered), the model will be tested on the test set. Both validation and testing will be performed using bootstrapping (resampling) 1,000 times, after which the mean AUC will be calculated. We will use a custom Butina splitter object to divide our data. In total, there will be 5 x 5 = 25 experiments: 5 folds, with each fold being trained 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1991b4f6-c3be-42e7-af92-d287e70455d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_model(dataset, num_epochs, dropout, lr, weight_decay, criterion, \n",
    "                        batch_size=64, device='cuda', num_workers=5):\n",
    "            all_folds_valid_aucs = []\n",
    "            all_folds_test_aucs = []\n",
    "            splits = []\n",
    "            smiles_df = dataset[[\"smiles\"]].drop_duplicates()\n",
    "            smiles_col = 'smiles'\n",
    "            \n",
    "            # split dataset into 5 folds of (train, val, test) dataframes using custom butina splitter obj.\n",
    "            butinaSplitter = CustomButinaSplitter()\n",
    "            splits = butinaSplitter.split_dataset(dataset)\n",
    "                    \n",
    "            for fold_number, (train_subset, val_subset, test_subset) in enumerate(splits, 1):\n",
    "                PRINTC()\n",
    "                print(f\"fold number {fold_number}\")\n",
    "                PRINTC()\n",
    "                train_df, val_df, test_df = train_subset, val_subset, test_subset\n",
    "                test_dataset = MoleculeDataset(test_df)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "                \n",
    "                bootstrap_valid_aucs = []\n",
    "                bootstrap_test_aucs = []\n",
    "                                \n",
    "                for bootsrap in range(5):\n",
    "                    best_val_auc = float('-inf')\n",
    "                    no_improve_epochs = 0\n",
    "                    early_stopping_patience = 5\n",
    "                    best_model_state_dict = None\n",
    "                    \n",
    "                    print(f\"bootsrap number: {bootsrap + 1}\")\n",
    "                    model = generate_model(batch_size=batch_size, dropout=dropout)\n",
    "                    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "                    PRINTC()\n",
    "                    \n",
    "                    seed_train = fold_number*1000 + bootsrap + 1\n",
    "                    labels_list = train_df['label'].values\n",
    "                    train_b = resample(train_df, random_state=seed_train, stratify=labels_list)\n",
    "\n",
    "                    train_dataset = MoleculeDataset(train_b)                    \n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "                    val_dataset = MoleculeDataset(val_df)                    \n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "                    last_test_auc = 0  # Initialize the last test AUC for this fold\n",
    "                    for epoch in range(num_epochs):\n",
    "                        start_time = time.time()\n",
    "                        model.train()\n",
    "                        epoch_loss = 0\n",
    "                        all_preds = []\n",
    "                        all_labels = []\n",
    "                        running_loss = 0.0\n",
    "                        for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                             batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "\n",
    "                            # Move all tensors to device\n",
    "                            batches = [batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc, batch_labels]\n",
    "                            batches = [batch.to(device) for batch in batches]\n",
    "                            batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc, batch_labels = batches\n",
    "                                \n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                                           batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc) \n",
    "                \n",
    "                            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            epoch_loss += loss.item()\n",
    "                \n",
    "                            all_labels.extend(batch_labels.cpu().numpy())\n",
    "                            all_preds.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                            \n",
    "                        train_auc = roc_auc_score(all_labels, all_preds)\n",
    "                                \n",
    "                        # Evaluate the model on the validation set\n",
    "                        all_val_labels = []\n",
    "                        all_val_outputs = []\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                                \n",
    "                                # Move all tensors to device\n",
    "                                batches = [batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc, batch_labels]\n",
    "                                batches = [batch.to(device) for batch in batches]\n",
    "                                batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc, batch_labels = batches\n",
    "                                                \n",
    "                                outputs = model(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                                \n",
    "                                all_val_labels.extend(batch_labels.cpu().numpy())\n",
    "                                all_val_outputs.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                                \n",
    "                        all_val_labels = np.array(all_val_labels)\n",
    "                        all_val_outputs = np.array(all_val_outputs)\n",
    "                        \n",
    "                        # Perform bootstrapping on predictions and labels (validation phase)\n",
    "                        current_b_aucs = []\n",
    "                        N_test = all_val_labels.shape[0]\n",
    "                        for b in range(1000):\n",
    "                            seed_value = epoch * 1000 + b + (bootsrap+1)*1000  # or any function of your parameters\n",
    "                            np.random.seed(seed_value)\n",
    "                            indices = np.random.randint(0, N_test, size=N_test)\n",
    "                            y_valid_pred_b = all_val_outputs[indices]\n",
    "                            y_valid_b = all_val_labels[indices]\n",
    "                            valid_auc = roc_auc_score(y_valid_b, y_valid_pred_b)\n",
    "                            current_b_aucs.append(valid_auc)\n",
    "                \n",
    "                        mean_val_auc = np.mean(current_b_aucs)\n",
    "                        end_time = time.time()\n",
    "                        epoch_time = (end_time - start_time) / 60\n",
    "                        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {(epoch_loss/len(train_loader)):.4f}, Train AUC: {train_auc:.4f}, Mean Validation AUC: {mean_val_auc:.4f}, Epoch Time: {epoch_time:.4f}')\n",
    "\n",
    "                        # Early stopping logic\n",
    "                        if mean_val_auc > best_val_auc:\n",
    "                            best_val_auc = mean_val_auc\n",
    "                            epochs_without_improvement = 0\n",
    "                            # Save the best model state dict\n",
    "                            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "                        else:\n",
    "                            epochs_without_improvement += 1\n",
    "            \n",
    "                        if epochs_without_improvement >= early_stopping_patience:\n",
    "                            print(\"Early stopping triggered\")\n",
    "                            break\n",
    "                            \n",
    "                    # Load the best model in order to evaluate it on the test set\n",
    "                    model.load_state_dict(best_model_state_dict)\n",
    "                    \n",
    "                    all_test_labels = []\n",
    "                    all_test_outputs = []\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                                batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                                \n",
    "                            # Move all tensors to device\n",
    "                            batches = [batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc, batch_labels]\n",
    "                            batches = [batch.to(device) for batch in batches]\n",
    "                            batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc, batch_labels = batches\n",
    "                                                \n",
    "                            outputs = model(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                                           batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                                \n",
    "                            all_test_labels.extend(batch_labels.cpu().numpy())\n",
    "                            all_test_outputs.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                                \n",
    "                    all_test_labels = np.array(all_test_labels)\n",
    "                    all_test_outputs = np.array(all_test_outputs)                 \n",
    "\n",
    "                    # Perform bootstrapping on predictions and labels (test phase)\n",
    "                    current_b_aucs = []\n",
    "                    N_test = all_test_labels.shape[0]\n",
    "                    for b in range(1000):\n",
    "                        seed_value = epoch * 1000 + b + (bootsrap+1)*1000  # or any function of your parameters\n",
    "                        np.random.seed(seed_value)\n",
    "                        indices = np.random.randint(0, N_test, size=N_test)\n",
    "                        y_test_pred_b = all_test_outputs[indices]\n",
    "                        y_test_b = all_test_labels[indices]\n",
    "                        test_auc = roc_auc_score(y_test_b, y_test_pred_b)\n",
    "                        current_b_aucs.append(test_auc)\n",
    "                \n",
    "                    mean_test_auc = np.mean(current_b_aucs)\n",
    "                    print(f'Bootstrap {bootsrap}, Mean Test AUC: {mean_test_auc:.4f}')\n",
    "\n",
    "                    # Store the best validation and test AUCs for this bootstrap\n",
    "                    bootstrap_valid_aucs.append(best_val_auc)\n",
    "                    bootstrap_test_aucs.append(mean_test_auc)\n",
    "                \n",
    "                # Compute mean validation and test AUCs for the current fold\n",
    "                current_fold_mean_valid_auc = np.mean(bootstrap_valid_aucs)\n",
    "                current_fold_mean_test_auc = np.mean(bootstrap_test_aucs)\n",
    "                print(f\"Fold {fold_number} Mean Validation AUC: {current_fold_mean_valid_auc:.4f}\")\n",
    "                print(f\"Fold {fold_number} Mean Test AUC: {current_fold_mean_test_auc:.4f}\")\n",
    "                all_folds_valid_aucs.append(current_fold_mean_valid_auc)\n",
    "                all_folds_test_aucs.append(current_fold_mean_test_auc)\n",
    "            \n",
    "            PRINTC()               \n",
    "            print(\"Final Mean Validation AUC across all folds:\", np.mean(all_folds_valid_aucs))\n",
    "            print(\"Validation AUCs for all folds:\", all_folds_valid_aucs)\n",
    "            print(\"Final Mean Test AUC across all folds:\", np.mean(all_folds_test_aucs))\n",
    "            print(\"Test AUCs for all folds:\", all_folds_test_aucs)                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "240bd908-157a-4533-bde8-041018b95835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique SMILES: 10720\n",
      "Random Seed: 11\n",
      "Train size: 61862 (77.17%), Valid size: 9148 (11.41%), Test size: 9149 (11.41%)\n",
      "Random Seed: 7\n",
      "Train size: 62524 (78.00%), Valid size: 8817 (11.00%), Test size: 8818 (11.00%)\n",
      "Random Seed: 8\n",
      "Train size: 62568 (78.05%), Valid size: 8795 (10.97%), Test size: 8796 (10.97%)\n",
      "Random Seed: 1\n",
      "Train size: 62576 (78.06%), Valid size: 8791 (10.97%), Test size: 8792 (10.97%)\n",
      "Random Seed: 4\n",
      "Train size: 62782 (78.32%), Valid size: 8688 (10.84%), Test size: 8689 (10.84%)\n",
      "--------------------------------------------------------------------------------\n",
      "fold number 1\n",
      "--------------------------------------------------------------------------------\n",
      "bootsrap number: 1\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6309, Train AUC: 0.6225, Mean Validation AUC: 0.6952, Epoch Time: 0.9682\n",
      "Epoch 2/100, Loss: 0.4745, Train AUC: 0.7783, Mean Validation AUC: 0.7276, Epoch Time: 0.9780\n",
      "Epoch 3/100, Loss: 0.3757, Train AUC: 0.8611, Mean Validation AUC: 0.7933, Epoch Time: 0.9899\n",
      "Epoch 4/100, Loss: 0.3099, Train AUC: 0.9104, Mean Validation AUC: 0.8441, Epoch Time: 0.9945\n",
      "Epoch 5/100, Loss: 0.2581, Train AUC: 0.9439, Mean Validation AUC: 0.8818, Epoch Time: 0.9819\n",
      "Epoch 6/100, Loss: 0.2170, Train AUC: 0.9617, Mean Validation AUC: 0.8983, Epoch Time: 0.9831\n",
      "Epoch 7/100, Loss: 0.1852, Train AUC: 0.9728, Mean Validation AUC: 0.9233, Epoch Time: 0.9997\n",
      "Epoch 8/100, Loss: 0.1613, Train AUC: 0.9793, Mean Validation AUC: 0.9304, Epoch Time: 0.9777\n",
      "Epoch 9/100, Loss: 0.1416, Train AUC: 0.9840, Mean Validation AUC: 0.9323, Epoch Time: 0.9769\n",
      "Epoch 10/100, Loss: 0.1264, Train AUC: 0.9865, Mean Validation AUC: 0.9425, Epoch Time: 0.9906\n",
      "Epoch 11/100, Loss: 0.1125, Train AUC: 0.9896, Mean Validation AUC: 0.9526, Epoch Time: 0.9971\n",
      "Epoch 12/100, Loss: 0.1009, Train AUC: 0.9916, Mean Validation AUC: 0.9505, Epoch Time: 0.9910\n",
      "Epoch 13/100, Loss: 0.0913, Train AUC: 0.9932, Mean Validation AUC: 0.9547, Epoch Time: 0.9815\n",
      "Epoch 14/100, Loss: 0.0827, Train AUC: 0.9942, Mean Validation AUC: 0.9538, Epoch Time: 0.9866\n",
      "Epoch 15/100, Loss: 0.0771, Train AUC: 0.9948, Mean Validation AUC: 0.9497, Epoch Time: 0.9843\n",
      "Epoch 16/100, Loss: 0.0698, Train AUC: 0.9958, Mean Validation AUC: 0.9591, Epoch Time: 0.9884\n",
      "Epoch 17/100, Loss: 0.0643, Train AUC: 0.9963, Mean Validation AUC: 0.9547, Epoch Time: 0.9791\n",
      "Epoch 18/100, Loss: 0.0588, Train AUC: 0.9968, Mean Validation AUC: 0.9562, Epoch Time: 0.9799\n",
      "Epoch 19/100, Loss: 0.0538, Train AUC: 0.9974, Mean Validation AUC: 0.9583, Epoch Time: 0.9885\n",
      "Epoch 20/100, Loss: 0.0510, Train AUC: 0.9975, Mean Validation AUC: 0.9570, Epoch Time: 0.9772\n",
      "Epoch 21/100, Loss: 0.0477, Train AUC: 0.9978, Mean Validation AUC: 0.9615, Epoch Time: 0.9923\n",
      "Epoch 22/100, Loss: 0.0437, Train AUC: 0.9980, Mean Validation AUC: 0.9488, Epoch Time: 0.9763\n",
      "Epoch 23/100, Loss: 0.0419, Train AUC: 0.9983, Mean Validation AUC: 0.9576, Epoch Time: 0.9886\n",
      "Epoch 24/100, Loss: 0.0383, Train AUC: 0.9986, Mean Validation AUC: 0.9559, Epoch Time: 0.9855\n",
      "Epoch 25/100, Loss: 0.0368, Train AUC: 0.9986, Mean Validation AUC: 0.9546, Epoch Time: 0.9928\n",
      "Epoch 26/100, Loss: 0.0350, Train AUC: 0.9987, Mean Validation AUC: 0.9514, Epoch Time: 0.9969\n",
      "Early stopping triggered\n",
      "Bootstrap 0, Mean Test AUC: 0.9617\n",
      "bootsrap number: 2\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5090, Train AUC: 0.6291, Mean Validation AUC: 0.7038, Epoch Time: 0.9755\n",
      "Epoch 2/100, Loss: 0.4029, Train AUC: 0.7997, Mean Validation AUC: 0.7815, Epoch Time: 1.0213\n",
      "Epoch 3/100, Loss: 0.3284, Train AUC: 0.8834, Mean Validation AUC: 0.8225, Epoch Time: 1.0032\n",
      "Epoch 4/100, Loss: 0.2739, Train AUC: 0.9294, Mean Validation AUC: 0.8745, Epoch Time: 0.9882\n",
      "Epoch 5/100, Loss: 0.2293, Train AUC: 0.9545, Mean Validation AUC: 0.9077, Epoch Time: 1.0062\n",
      "Epoch 6/100, Loss: 0.1943, Train AUC: 0.9685, Mean Validation AUC: 0.9234, Epoch Time: 1.0083\n",
      "Epoch 7/100, Loss: 0.1685, Train AUC: 0.9761, Mean Validation AUC: 0.9430, Epoch Time: 0.9992\n",
      "Epoch 8/100, Loss: 0.1492, Train AUC: 0.9812, Mean Validation AUC: 0.9503, Epoch Time: 0.9962\n",
      "Epoch 9/100, Loss: 0.1331, Train AUC: 0.9850, Mean Validation AUC: 0.9577, Epoch Time: 1.0078\n",
      "Epoch 10/100, Loss: 0.1193, Train AUC: 0.9879, Mean Validation AUC: 0.9598, Epoch Time: 1.0005\n",
      "Epoch 11/100, Loss: 0.1080, Train AUC: 0.9899, Mean Validation AUC: 0.9615, Epoch Time: 1.0049\n",
      "Epoch 12/100, Loss: 0.0980, Train AUC: 0.9917, Mean Validation AUC: 0.9566, Epoch Time: 1.0046\n",
      "Epoch 13/100, Loss: 0.0891, Train AUC: 0.9930, Mean Validation AUC: 0.9586, Epoch Time: 1.0169\n",
      "Epoch 14/100, Loss: 0.0808, Train AUC: 0.9943, Mean Validation AUC: 0.9597, Epoch Time: 1.0053\n",
      "Epoch 15/100, Loss: 0.0741, Train AUC: 0.9951, Mean Validation AUC: 0.9530, Epoch Time: 0.9982\n",
      "Epoch 16/100, Loss: 0.0691, Train AUC: 0.9957, Mean Validation AUC: 0.9506, Epoch Time: 1.0074\n",
      "Early stopping triggered\n",
      "Bootstrap 1, Mean Test AUC: 0.9617\n",
      "bootsrap number: 3\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5390, Train AUC: 0.6190, Mean Validation AUC: 0.7082, Epoch Time: 1.0285\n",
      "Epoch 2/100, Loss: 0.4237, Train AUC: 0.7801, Mean Validation AUC: 0.7559, Epoch Time: 1.0349\n",
      "Epoch 3/100, Loss: 0.3406, Train AUC: 0.8770, Mean Validation AUC: 0.7923, Epoch Time: 1.0269\n",
      "Epoch 4/100, Loss: 0.2839, Train AUC: 0.9224, Mean Validation AUC: 0.8499, Epoch Time: 1.0265\n",
      "Epoch 5/100, Loss: 0.2362, Train AUC: 0.9514, Mean Validation AUC: 0.9001, Epoch Time: 1.0371\n",
      "Epoch 6/100, Loss: 0.1978, Train AUC: 0.9672, Mean Validation AUC: 0.9333, Epoch Time: 1.0466\n",
      "Epoch 7/100, Loss: 0.1694, Train AUC: 0.9759, Mean Validation AUC: 0.9403, Epoch Time: 1.0276\n",
      "Epoch 8/100, Loss: 0.1475, Train AUC: 0.9817, Mean Validation AUC: 0.9439, Epoch Time: 1.0404\n",
      "Epoch 9/100, Loss: 0.1305, Train AUC: 0.9858, Mean Validation AUC: 0.9506, Epoch Time: 1.0352\n",
      "Epoch 10/100, Loss: 0.1157, Train AUC: 0.9889, Mean Validation AUC: 0.9514, Epoch Time: 1.0467\n",
      "Epoch 11/100, Loss: 0.1041, Train AUC: 0.9908, Mean Validation AUC: 0.9524, Epoch Time: 1.0393\n",
      "Epoch 12/100, Loss: 0.0950, Train AUC: 0.9921, Mean Validation AUC: 0.9582, Epoch Time: 1.0213\n",
      "Epoch 13/100, Loss: 0.0850, Train AUC: 0.9937, Mean Validation AUC: 0.9535, Epoch Time: 1.0290\n",
      "Epoch 14/100, Loss: 0.0793, Train AUC: 0.9943, Mean Validation AUC: 0.9566, Epoch Time: 1.0211\n",
      "Epoch 15/100, Loss: 0.0723, Train AUC: 0.9955, Mean Validation AUC: 0.9481, Epoch Time: 1.0369\n",
      "Epoch 16/100, Loss: 0.0665, Train AUC: 0.9960, Mean Validation AUC: 0.9536, Epoch Time: 1.0341\n",
      "Epoch 17/100, Loss: 0.0634, Train AUC: 0.9962, Mean Validation AUC: 0.9536, Epoch Time: 1.0310\n",
      "Early stopping triggered\n",
      "Bootstrap 2, Mean Test AUC: 0.9581\n",
      "bootsrap number: 4\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6576, Train AUC: 0.6111, Mean Validation AUC: 0.6974, Epoch Time: 1.0105\n",
      "Epoch 3/100, Loss: 0.3925, Train AUC: 0.8538, Mean Validation AUC: 0.7747, Epoch Time: 1.0032\n",
      "Epoch 4/100, Loss: 0.3218, Train AUC: 0.9064, Mean Validation AUC: 0.8149, Epoch Time: 0.9934\n",
      "Epoch 5/100, Loss: 0.2672, Train AUC: 0.9408, Mean Validation AUC: 0.8633, Epoch Time: 1.0142\n",
      "Epoch 6/100, Loss: 0.2244, Train AUC: 0.9601, Mean Validation AUC: 0.9064, Epoch Time: 1.0117\n",
      "Epoch 7/100, Loss: 0.1903, Train AUC: 0.9719, Mean Validation AUC: 0.9157, Epoch Time: 0.9997\n",
      "Epoch 8/100, Loss: 0.1639, Train AUC: 0.9790, Mean Validation AUC: 0.9245, Epoch Time: 1.0075\n",
      "Epoch 9/100, Loss: 0.1435, Train AUC: 0.9838, Mean Validation AUC: 0.9408, Epoch Time: 1.0021\n",
      "Epoch 10/100, Loss: 0.1283, Train AUC: 0.9863, Mean Validation AUC: 0.9421, Epoch Time: 1.0236\n",
      "Epoch 11/100, Loss: 0.1148, Train AUC: 0.9892, Mean Validation AUC: 0.9484, Epoch Time: 1.0130\n",
      "Epoch 12/100, Loss: 0.1044, Train AUC: 0.9909, Mean Validation AUC: 0.9538, Epoch Time: 1.0087\n",
      "Epoch 13/100, Loss: 0.0938, Train AUC: 0.9926, Mean Validation AUC: 0.9453, Epoch Time: 1.0102\n",
      "Epoch 14/100, Loss: 0.0860, Train AUC: 0.9933, Mean Validation AUC: 0.9523, Epoch Time: 1.0079\n",
      "Epoch 15/100, Loss: 0.0772, Train AUC: 0.9948, Mean Validation AUC: 0.9513, Epoch Time: 1.0204\n",
      "Epoch 16/100, Loss: 0.0712, Train AUC: 0.9955, Mean Validation AUC: 0.9522, Epoch Time: 1.0102\n",
      "Epoch 17/100, Loss: 0.0647, Train AUC: 0.9964, Mean Validation AUC: 0.9567, Epoch Time: 1.0043\n",
      "Epoch 18/100, Loss: 0.0607, Train AUC: 0.9966, Mean Validation AUC: 0.9549, Epoch Time: 1.0093\n",
      "Epoch 19/100, Loss: 0.0554, Train AUC: 0.9972, Mean Validation AUC: 0.9583, Epoch Time: 1.0022\n",
      "Epoch 20/100, Loss: 0.0518, Train AUC: 0.9976, Mean Validation AUC: 0.9514, Epoch Time: 1.0119\n",
      "Epoch 22/100, Loss: 0.0451, Train AUC: 0.9981, Mean Validation AUC: 0.9440, Epoch Time: 1.0343\n",
      "Epoch 23/100, Loss: 0.0420, Train AUC: 0.9984, Mean Validation AUC: 0.9532, Epoch Time: 0.9937\n",
      "Epoch 24/100, Loss: 0.0385, Train AUC: 0.9986, Mean Validation AUC: 0.9518, Epoch Time: 1.0041\n",
      "Early stopping triggered\n",
      "Bootstrap 3, Mean Test AUC: 0.9584\n",
      "bootsrap number: 5\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6148, Train AUC: 0.6177, Mean Validation AUC: 0.6866, Epoch Time: 1.0050\n",
      "Epoch 2/100, Loss: 0.4702, Train AUC: 0.7763, Mean Validation AUC: 0.7430, Epoch Time: 1.0007\n",
      "Epoch 3/100, Loss: 0.3717, Train AUC: 0.8674, Mean Validation AUC: 0.8254, Epoch Time: 1.0141\n",
      "Epoch 4/100, Loss: 0.3025, Train AUC: 0.9222, Mean Validation AUC: 0.8659, Epoch Time: 1.0105\n",
      "Epoch 5/100, Loss: 0.2507, Train AUC: 0.9509, Mean Validation AUC: 0.8969, Epoch Time: 1.0032\n",
      "Epoch 6/100, Loss: 0.2099, Train AUC: 0.9673, Mean Validation AUC: 0.9117, Epoch Time: 1.0153\n",
      "Epoch 7/100, Loss: 0.1798, Train AUC: 0.9756, Mean Validation AUC: 0.9285, Epoch Time: 1.0179\n",
      "Epoch 8/100, Loss: 0.1562, Train AUC: 0.9817, Mean Validation AUC: 0.9290, Epoch Time: 1.0164\n",
      "Epoch 9/100, Loss: 0.1389, Train AUC: 0.9847, Mean Validation AUC: 0.9222, Epoch Time: 1.0149\n",
      "Epoch 10/100, Loss: 0.1232, Train AUC: 0.9878, Mean Validation AUC: 0.9332, Epoch Time: 1.0240\n",
      "Epoch 11/100, Loss: 0.1108, Train AUC: 0.9899, Mean Validation AUC: 0.9339, Epoch Time: 1.0185\n",
      "Epoch 12/100, Loss: 0.1005, Train AUC: 0.9916, Mean Validation AUC: 0.9373, Epoch Time: 1.0053\n",
      "Epoch 13/100, Loss: 0.0904, Train AUC: 0.9932, Mean Validation AUC: 0.9380, Epoch Time: 1.0169\n",
      "Epoch 14/100, Loss: 0.0815, Train AUC: 0.9944, Mean Validation AUC: 0.9304, Epoch Time: 1.0207\n",
      "Epoch 15/100, Loss: 0.0755, Train AUC: 0.9953, Mean Validation AUC: 0.9413, Epoch Time: 1.0089\n",
      "Epoch 16/100, Loss: 0.0707, Train AUC: 0.9955, Mean Validation AUC: 0.9417, Epoch Time: 1.0102\n",
      "Epoch 17/100, Loss: 0.0631, Train AUC: 0.9965, Mean Validation AUC: 0.9381, Epoch Time: 1.0185\n",
      "Epoch 18/100, Loss: 0.0601, Train AUC: 0.9966, Mean Validation AUC: 0.9276, Epoch Time: 1.0246\n",
      "Epoch 19/100, Loss: 0.0544, Train AUC: 0.9974, Mean Validation AUC: 0.9428, Epoch Time: 1.0051\n",
      "Epoch 20/100, Loss: 0.0536, Train AUC: 0.9973, Mean Validation AUC: 0.9418, Epoch Time: 1.0115\n",
      "Epoch 21/100, Loss: 0.0483, Train AUC: 0.9979, Mean Validation AUC: 0.9410, Epoch Time: 1.0058\n",
      "Epoch 22/100, Loss: 0.0457, Train AUC: 0.9981, Mean Validation AUC: 0.9324, Epoch Time: 1.0079\n",
      "Epoch 23/100, Loss: 0.0424, Train AUC: 0.9984, Mean Validation AUC: 0.9363, Epoch Time: 1.0196\n",
      "Epoch 24/100, Loss: 0.0404, Train AUC: 0.9985, Mean Validation AUC: 0.9342, Epoch Time: 1.0166\n",
      "Early stopping triggered\n",
      "Bootstrap 4, Mean Test AUC: 0.9427\n",
      "Fold 1 Mean Validation AUC: 0.9565\n",
      "Fold 1 Mean Test AUC: 0.9565\n",
      "--------------------------------------------------------------------------------\n",
      "fold number 2\n",
      "--------------------------------------------------------------------------------\n",
      "bootsrap number: 1\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5333, Train AUC: 0.6415, Mean Validation AUC: 0.7214, Epoch Time: 1.0013\n",
      "Epoch 2/100, Loss: 0.4085, Train AUC: 0.8129, Mean Validation AUC: 0.8130, Epoch Time: 1.0099\n",
      "Epoch 3/100, Loss: 0.3243, Train AUC: 0.8985, Mean Validation AUC: 0.8585, Epoch Time: 1.0193\n",
      "Epoch 4/100, Loss: 0.2662, Train AUC: 0.9391, Mean Validation AUC: 0.8846, Epoch Time: 1.0100\n",
      "Epoch 5/100, Loss: 0.2207, Train AUC: 0.9615, Mean Validation AUC: 0.9020, Epoch Time: 1.0221\n",
      "Epoch 8/100, Loss: 0.1372, Train AUC: 0.9852, Mean Validation AUC: 0.9323, Epoch Time: 1.0100\n",
      "Epoch 9/100, Loss: 0.1211, Train AUC: 0.9880, Mean Validation AUC: 0.9363, Epoch Time: 1.0115\n",
      "Epoch 10/100, Loss: 0.1072, Train AUC: 0.9907, Mean Validation AUC: 0.9346, Epoch Time: 1.0222\n",
      "Epoch 11/100, Loss: 0.0967, Train AUC: 0.9922, Mean Validation AUC: 0.9418, Epoch Time: 1.0158\n",
      "Epoch 12/100, Loss: 0.0871, Train AUC: 0.9935, Mean Validation AUC: 0.9396, Epoch Time: 1.0239\n",
      "Epoch 13/100, Loss: 0.0788, Train AUC: 0.9946, Mean Validation AUC: 0.9429, Epoch Time: 1.0117\n",
      "Epoch 14/100, Loss: 0.0713, Train AUC: 0.9956, Mean Validation AUC: 0.9408, Epoch Time: 1.0273\n",
      "Epoch 15/100, Loss: 0.0657, Train AUC: 0.9963, Mean Validation AUC: 0.9468, Epoch Time: 1.0135\n",
      "Epoch 16/100, Loss: 0.0606, Train AUC: 0.9968, Mean Validation AUC: 0.9443, Epoch Time: 1.0151\n",
      "Epoch 17/100, Loss: 0.0553, Train AUC: 0.9973, Mean Validation AUC: 0.9439, Epoch Time: 1.0168\n",
      "Epoch 18/100, Loss: 0.0521, Train AUC: 0.9975, Mean Validation AUC: 0.9467, Epoch Time: 1.0427\n",
      "Epoch 19/100, Loss: 0.0499, Train AUC: 0.9976, Mean Validation AUC: 0.9439, Epoch Time: 1.0178\n",
      "Epoch 20/100, Loss: 0.0452, Train AUC: 0.9980, Mean Validation AUC: 0.9439, Epoch Time: 1.0256\n",
      "Early stopping triggered\n",
      "Bootstrap 0, Mean Test AUC: 0.9468\n",
      "bootsrap number: 2\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6167, Train AUC: 0.6422, Mean Validation AUC: 0.7034, Epoch Time: 1.0090\n",
      "Epoch 2/100, Loss: 0.4604, Train AUC: 0.8046, Mean Validation AUC: 0.7985, Epoch Time: 1.0190\n",
      "Epoch 3/100, Loss: 0.3571, Train AUC: 0.8872, Mean Validation AUC: 0.8504, Epoch Time: 1.0156\n",
      "Epoch 4/100, Loss: 0.2834, Train AUC: 0.9360, Mean Validation AUC: 0.8802, Epoch Time: 1.0194\n",
      "Epoch 5/100, Loss: 0.2300, Train AUC: 0.9603, Mean Validation AUC: 0.9053, Epoch Time: 1.0175\n",
      "Epoch 6/100, Loss: 0.1908, Train AUC: 0.9735, Mean Validation AUC: 0.9166, Epoch Time: 1.0347\n",
      "Epoch 7/100, Loss: 0.1623, Train AUC: 0.9802, Mean Validation AUC: 0.9291, Epoch Time: 1.0147\n",
      "Epoch 8/100, Loss: 0.1398, Train AUC: 0.9851, Mean Validation AUC: 0.9353, Epoch Time: 1.0144\n",
      "Epoch 9/100, Loss: 0.1233, Train AUC: 0.9880, Mean Validation AUC: 0.9393, Epoch Time: 1.0238\n",
      "Epoch 10/100, Loss: 0.1079, Train AUC: 0.9909, Mean Validation AUC: 0.9415, Epoch Time: 1.0264\n",
      "Epoch 11/100, Loss: 0.0959, Train AUC: 0.9926, Mean Validation AUC: 0.9464, Epoch Time: 1.0072\n",
      "Epoch 12/100, Loss: 0.0862, Train AUC: 0.9939, Mean Validation AUC: 0.9470, Epoch Time: 1.0153\n",
      "Epoch 13/100, Loss: 0.0791, Train AUC: 0.9946, Mean Validation AUC: 0.9489, Epoch Time: 1.0171\n",
      "Epoch 14/100, Loss: 0.0705, Train AUC: 0.9960, Mean Validation AUC: 0.9485, Epoch Time: 1.0260\n",
      "Epoch 15/100, Loss: 0.0652, Train AUC: 0.9964, Mean Validation AUC: 0.9484, Epoch Time: 1.0208\n",
      "Epoch 16/100, Loss: 0.0605, Train AUC: 0.9967, Mean Validation AUC: 0.9518, Epoch Time: 1.0139\n",
      "Epoch 17/100, Loss: 0.0554, Train AUC: 0.9972, Mean Validation AUC: 0.9496, Epoch Time: 1.0265\n",
      "Epoch 18/100, Loss: 0.0514, Train AUC: 0.9975, Mean Validation AUC: 0.9534, Epoch Time: 1.0129\n",
      "Epoch 19/100, Loss: 0.0487, Train AUC: 0.9978, Mean Validation AUC: 0.9542, Epoch Time: 1.0208\n",
      "Epoch 20/100, Loss: 0.0430, Train AUC: 0.9984, Mean Validation AUC: 0.9535, Epoch Time: 1.0187\n",
      "Epoch 21/100, Loss: 0.0405, Train AUC: 0.9984, Mean Validation AUC: 0.9563, Epoch Time: 1.0137\n",
      "Epoch 22/100, Loss: 0.0387, Train AUC: 0.9985, Mean Validation AUC: 0.9552, Epoch Time: 1.0045\n",
      "Epoch 23/100, Loss: 0.0363, Train AUC: 0.9988, Mean Validation AUC: 0.9520, Epoch Time: 1.0080\n",
      "Epoch 24/100, Loss: 0.0336, Train AUC: 0.9990, Mean Validation AUC: 0.9551, Epoch Time: 1.0104\n",
      "Epoch 25/100, Loss: 0.0322, Train AUC: 0.9990, Mean Validation AUC: 0.9575, Epoch Time: 1.0161\n",
      "Epoch 26/100, Loss: 0.0308, Train AUC: 0.9990, Mean Validation AUC: 0.9550, Epoch Time: 1.0106\n",
      "Epoch 27/100, Loss: 0.0286, Train AUC: 0.9992, Mean Validation AUC: 0.9540, Epoch Time: 1.0196\n",
      "Epoch 28/100, Loss: 0.0256, Train AUC: 0.9994, Mean Validation AUC: 0.9537, Epoch Time: 1.0272\n",
      "Epoch 29/100, Loss: 0.0258, Train AUC: 0.9994, Mean Validation AUC: 0.9547, Epoch Time: 1.0250\n",
      "Epoch 30/100, Loss: 0.0245, Train AUC: 0.9994, Mean Validation AUC: 0.9552, Epoch Time: 1.0176\n",
      "Early stopping triggered\n",
      "Bootstrap 1, Mean Test AUC: 0.9574\n",
      "bootsrap number: 3\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6050, Train AUC: 0.6380, Mean Validation AUC: 0.7082, Epoch Time: 1.0235\n",
      "Epoch 2/100, Loss: 0.4579, Train AUC: 0.8002, Mean Validation AUC: 0.8091, Epoch Time: 1.0383\n",
      "Epoch 3/100, Loss: 0.3577, Train AUC: 0.8877, Mean Validation AUC: 0.8455, Epoch Time: 1.0372\n",
      "Epoch 4/100, Loss: 0.2886, Train AUC: 0.9323, Mean Validation AUC: 0.8801, Epoch Time: 1.0310\n",
      "Epoch 5/100, Loss: 0.2357, Train AUC: 0.9586, Mean Validation AUC: 0.8979, Epoch Time: 1.0304\n",
      "Epoch 6/100, Loss: 0.1955, Train AUC: 0.9720, Mean Validation AUC: 0.9104, Epoch Time: 1.0417\n",
      "Epoch 7/100, Loss: 0.1664, Train AUC: 0.9788, Mean Validation AUC: 0.9219, Epoch Time: 1.0371\n",
      "Epoch 8/100, Loss: 0.1438, Train AUC: 0.9840, Mean Validation AUC: 0.9260, Epoch Time: 1.0147\n",
      "Epoch 9/100, Loss: 0.1267, Train AUC: 0.9870, Mean Validation AUC: 0.9307, Epoch Time: 1.0318\n",
      "Epoch 10/100, Loss: 0.1125, Train AUC: 0.9898, Mean Validation AUC: 0.9349, Epoch Time: 1.0197\n",
      "Epoch 11/100, Loss: 0.1001, Train AUC: 0.9918, Mean Validation AUC: 0.9361, Epoch Time: 1.0389\n",
      "Epoch 12/100, Loss: 0.0911, Train AUC: 0.9931, Mean Validation AUC: 0.9341, Epoch Time: 1.0221\n",
      "Epoch 13/100, Loss: 0.0818, Train AUC: 0.9945, Mean Validation AUC: 0.9379, Epoch Time: 1.0325\n",
      "Epoch 14/100, Loss: 0.0754, Train AUC: 0.9950, Mean Validation AUC: 0.9384, Epoch Time: 1.0279\n",
      "Epoch 15/100, Loss: 0.0689, Train AUC: 0.9959, Mean Validation AUC: 0.9367, Epoch Time: 1.0257\n",
      "Epoch 16/100, Loss: 0.0634, Train AUC: 0.9965, Mean Validation AUC: 0.9390, Epoch Time: 1.0404\n",
      "Epoch 17/100, Loss: 0.0587, Train AUC: 0.9968, Mean Validation AUC: 0.9415, Epoch Time: 1.0255\n",
      "Epoch 18/100, Loss: 0.0552, Train AUC: 0.9972, Mean Validation AUC: 0.9432, Epoch Time: 1.0261\n",
      "Epoch 19/100, Loss: 0.0511, Train AUC: 0.9974, Mean Validation AUC: 0.9410, Epoch Time: 1.0276\n",
      "Epoch 20/100, Loss: 0.0474, Train AUC: 0.9978, Mean Validation AUC: 0.9439, Epoch Time: 1.0465\n",
      "Epoch 21/100, Loss: 0.0442, Train AUC: 0.9982, Mean Validation AUC: 0.9423, Epoch Time: 1.0253\n",
      "Epoch 22/100, Loss: 0.0401, Train AUC: 0.9986, Mean Validation AUC: 0.9398, Epoch Time: 1.0273\n",
      "Epoch 23/100, Loss: 0.0385, Train AUC: 0.9987, Mean Validation AUC: 0.9374, Epoch Time: 1.0389\n",
      "Epoch 24/100, Loss: 0.0364, Train AUC: 0.9986, Mean Validation AUC: 0.9404, Epoch Time: 1.0281\n",
      "Epoch 25/100, Loss: 0.0339, Train AUC: 0.9989, Mean Validation AUC: 0.9399, Epoch Time: 1.0262\n",
      "Early stopping triggered\n",
      "Bootstrap 2, Mean Test AUC: 0.9438\n",
      "bootsrap number: 4\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.7057, Train AUC: 0.6225, Mean Validation AUC: 0.7029, Epoch Time: 1.0330\n",
      "Epoch 2/100, Loss: 0.5223, Train AUC: 0.7772, Mean Validation AUC: 0.7923, Epoch Time: 1.0296\n",
      "Epoch 3/100, Loss: 0.3993, Train AUC: 0.8676, Mean Validation AUC: 0.8301, Epoch Time: 1.0406\n",
      "Epoch 4/100, Loss: 0.3190, Train AUC: 0.9175, Mean Validation AUC: 0.8685, Epoch Time: 1.0261\n",
      "Epoch 5/100, Loss: 0.2582, Train AUC: 0.9497, Mean Validation AUC: 0.8911, Epoch Time: 1.0223\n",
      "Epoch 6/100, Loss: 0.2129, Train AUC: 0.9671, Mean Validation AUC: 0.9103, Epoch Time: 1.0263\n",
      "Epoch 7/100, Loss: 0.1787, Train AUC: 0.9773, Mean Validation AUC: 0.9207, Epoch Time: 1.0280\n",
      "Epoch 8/100, Loss: 0.1521, Train AUC: 0.9836, Mean Validation AUC: 0.9316, Epoch Time: 1.0296\n",
      "Epoch 9/100, Loss: 0.1328, Train AUC: 0.9870, Mean Validation AUC: 0.9384, Epoch Time: 1.0284\n",
      "Epoch 10/100, Loss: 0.1163, Train AUC: 0.9899, Mean Validation AUC: 0.9422, Epoch Time: 1.0323\n",
      "Epoch 11/100, Loss: 0.1030, Train AUC: 0.9917, Mean Validation AUC: 0.9450, Epoch Time: 1.0300\n",
      "Epoch 12/100, Loss: 0.0921, Train AUC: 0.9933, Mean Validation AUC: 0.9466, Epoch Time: 1.0217\n",
      "Epoch 13/100, Loss: 0.0829, Train AUC: 0.9944, Mean Validation AUC: 0.9475, Epoch Time: 1.0248\n",
      "Epoch 14/100, Loss: 0.0741, Train AUC: 0.9956, Mean Validation AUC: 0.9485, Epoch Time: 1.0351\n",
      "Epoch 15/100, Loss: 0.0678, Train AUC: 0.9962, Mean Validation AUC: 0.9478, Epoch Time: 1.0370\n",
      "Epoch 16/100, Loss: 0.0613, Train AUC: 0.9969, Mean Validation AUC: 0.9493, Epoch Time: 1.0260\n",
      "Epoch 17/100, Loss: 0.0561, Train AUC: 0.9974, Mean Validation AUC: 0.9439, Epoch Time: 1.0301\n",
      "Epoch 18/100, Loss: 0.0524, Train AUC: 0.9977, Mean Validation AUC: 0.9413, Epoch Time: 1.0232\n",
      "Epoch 19/100, Loss: 0.0486, Train AUC: 0.9979, Mean Validation AUC: 0.9473, Epoch Time: 1.0328\n",
      "Epoch 20/100, Loss: 0.0458, Train AUC: 0.9981, Mean Validation AUC: 0.9451, Epoch Time: 1.0250\n",
      "Epoch 21/100, Loss: 0.0434, Train AUC: 0.9983, Mean Validation AUC: 0.9408, Epoch Time: 1.0350\n",
      "Early stopping triggered\n",
      "Bootstrap 3, Mean Test AUC: 0.9494\n",
      "bootsrap number: 5\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.7541, Train AUC: 0.6339, Mean Validation AUC: 0.6927, Epoch Time: 1.0139\n",
      "Epoch 2/100, Loss: 0.5536, Train AUC: 0.7813, Mean Validation AUC: 0.7811, Epoch Time: 1.0167\n",
      "Epoch 3/100, Loss: 0.4227, Train AUC: 0.8689, Mean Validation AUC: 0.8356, Epoch Time: 1.0252\n",
      "Epoch 4/100, Loss: 0.3351, Train AUC: 0.9180, Mean Validation AUC: 0.8746, Epoch Time: 1.0273\n",
      "Epoch 5/100, Loss: 0.2694, Train AUC: 0.9505, Mean Validation AUC: 0.8957, Epoch Time: 1.0360\n",
      "Epoch 6/100, Loss: 0.2211, Train AUC: 0.9670, Mean Validation AUC: 0.9082, Epoch Time: 1.0256\n",
      "Epoch 7/100, Loss: 0.1852, Train AUC: 0.9759, Mean Validation AUC: 0.9211, Epoch Time: 1.0325\n",
      "Epoch 8/100, Loss: 0.1589, Train AUC: 0.9816, Mean Validation AUC: 0.9246, Epoch Time: 1.0491\n",
      "Epoch 9/100, Loss: 0.1380, Train AUC: 0.9857, Mean Validation AUC: 0.9294, Epoch Time: 1.0305\n",
      "Epoch 10/100, Loss: 0.1218, Train AUC: 0.9889, Mean Validation AUC: 0.9356, Epoch Time: 1.0168\n",
      "Epoch 11/100, Loss: 0.1074, Train AUC: 0.9914, Mean Validation AUC: 0.9336, Epoch Time: 1.0329\n",
      "Epoch 12/100, Loss: 0.0974, Train AUC: 0.9925, Mean Validation AUC: 0.9368, Epoch Time: 1.0379\n",
      "Epoch 13/100, Loss: 0.0877, Train AUC: 0.9937, Mean Validation AUC: 0.9409, Epoch Time: 1.0214\n",
      "Epoch 14/100, Loss: 0.0801, Train AUC: 0.9947, Mean Validation AUC: 0.9366, Epoch Time: 1.0196\n",
      "Epoch 15/100, Loss: 0.0732, Train AUC: 0.9954, Mean Validation AUC: 0.9364, Epoch Time: 1.0262\n",
      "Epoch 16/100, Loss: 0.0662, Train AUC: 0.9961, Mean Validation AUC: 0.9382, Epoch Time: 1.0261\n",
      "Epoch 17/100, Loss: 0.0616, Train AUC: 0.9967, Mean Validation AUC: 0.9382, Epoch Time: 1.0362\n",
      "Epoch 18/100, Loss: 0.0563, Train AUC: 0.9972, Mean Validation AUC: 0.9410, Epoch Time: 1.0193\n",
      "Epoch 19/100, Loss: 0.0522, Train AUC: 0.9978, Mean Validation AUC: 0.9395, Epoch Time: 1.0530\n",
      "Epoch 20/100, Loss: 0.0487, Train AUC: 0.9979, Mean Validation AUC: 0.9402, Epoch Time: 1.0393\n",
      "Epoch 21/100, Loss: 0.0458, Train AUC: 0.9981, Mean Validation AUC: 0.9433, Epoch Time: 1.0262\n",
      "Epoch 22/100, Loss: 0.0426, Train AUC: 0.9983, Mean Validation AUC: 0.9418, Epoch Time: 1.0412\n",
      "Epoch 23/100, Loss: 0.0421, Train AUC: 0.9982, Mean Validation AUC: 0.9419, Epoch Time: 1.0246\n",
      "Epoch 24/100, Loss: 0.0386, Train AUC: 0.9986, Mean Validation AUC: 0.9444, Epoch Time: 1.0307\n",
      "Epoch 25/100, Loss: 0.0353, Train AUC: 0.9988, Mean Validation AUC: 0.9442, Epoch Time: 1.0202\n",
      "Epoch 26/100, Loss: 0.0340, Train AUC: 0.9989, Mean Validation AUC: 0.9421, Epoch Time: 1.0388\n",
      "Epoch 27/100, Loss: 0.0335, Train AUC: 0.9989, Mean Validation AUC: 0.9449, Epoch Time: 1.0388\n",
      "Epoch 28/100, Loss: 0.0307, Train AUC: 0.9991, Mean Validation AUC: 0.9385, Epoch Time: 1.0268\n",
      "Epoch 29/100, Loss: 0.0295, Train AUC: 0.9992, Mean Validation AUC: 0.9464, Epoch Time: 1.0262\n",
      "Epoch 30/100, Loss: 0.0279, Train AUC: 0.9993, Mean Validation AUC: 0.9432, Epoch Time: 1.0275\n",
      "Epoch 31/100, Loss: 0.0259, Train AUC: 0.9994, Mean Validation AUC: 0.9455, Epoch Time: 1.0263\n",
      "Epoch 32/100, Loss: 0.0263, Train AUC: 0.9993, Mean Validation AUC: 0.9457, Epoch Time: 1.0220\n",
      "Epoch 33/100, Loss: 0.0248, Train AUC: 0.9994, Mean Validation AUC: 0.9417, Epoch Time: 1.0411\n",
      "Epoch 34/100, Loss: 0.0231, Train AUC: 0.9995, Mean Validation AUC: 0.9427, Epoch Time: 1.0399\n",
      "Early stopping triggered\n",
      "Bootstrap 4, Mean Test AUC: 0.9465\n",
      "Fold 2 Mean Validation AUC: 0.9488\n",
      "Fold 2 Mean Test AUC: 0.9488\n",
      "--------------------------------------------------------------------------------\n",
      "fold number 3\n",
      "--------------------------------------------------------------------------------\n",
      "bootsrap number: 1\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6348, Train AUC: 0.6197, Mean Validation AUC: 0.6617, Epoch Time: 1.0541\n",
      "Epoch 2/100, Loss: 0.4810, Train AUC: 0.7645, Mean Validation AUC: 0.8450, Epoch Time: 1.0507\n",
      "Epoch 4/100, Loss: 0.3076, Train AUC: 0.9121, Mean Validation AUC: 0.9011, Epoch Time: 1.0528\n",
      "Epoch 5/100, Loss: 0.2586, Train AUC: 0.9419, Mean Validation AUC: 0.9185, Epoch Time: 1.0519\n",
      "Epoch 6/100, Loss: 0.2228, Train AUC: 0.9594, Mean Validation AUC: 0.9330, Epoch Time: 1.0677\n",
      "Epoch 7/100, Loss: 0.1927, Train AUC: 0.9702, Mean Validation AUC: 0.9429, Epoch Time: 1.0588\n",
      "Epoch 8/100, Loss: 0.1679, Train AUC: 0.9779, Mean Validation AUC: 0.9480, Epoch Time: 1.0598\n",
      "Epoch 9/100, Loss: 0.1461, Train AUC: 0.9835, Mean Validation AUC: 0.9500, Epoch Time: 1.0709\n",
      "Epoch 10/100, Loss: 0.1279, Train AUC: 0.9871, Mean Validation AUC: 0.9519, Epoch Time: 1.0560\n",
      "Epoch 11/100, Loss: 0.1131, Train AUC: 0.9900, Mean Validation AUC: 0.9517, Epoch Time: 1.0505\n",
      "Epoch 12/100, Loss: 0.0998, Train AUC: 0.9921, Mean Validation AUC: 0.9510, Epoch Time: 1.0528\n",
      "Epoch 13/100, Loss: 0.0900, Train AUC: 0.9932, Mean Validation AUC: 0.9534, Epoch Time: 1.0572\n",
      "Epoch 14/100, Loss: 0.0817, Train AUC: 0.9944, Mean Validation AUC: 0.9545, Epoch Time: 1.0501\n",
      "Epoch 15/100, Loss: 0.0746, Train AUC: 0.9951, Mean Validation AUC: 0.9501, Epoch Time: 1.0553\n",
      "Epoch 16/100, Loss: 0.0686, Train AUC: 0.9958, Mean Validation AUC: 0.9513, Epoch Time: 1.0486\n",
      "Epoch 17/100, Loss: 0.0621, Train AUC: 0.9966, Mean Validation AUC: 0.9501, Epoch Time: 1.0496\n",
      "Epoch 18/100, Loss: 0.0584, Train AUC: 0.9970, Mean Validation AUC: 0.9499, Epoch Time: 1.0610\n",
      "Epoch 19/100, Loss: 0.0528, Train AUC: 0.9974, Mean Validation AUC: 0.9559, Epoch Time: 1.0617\n",
      "Epoch 20/100, Loss: 0.0501, Train AUC: 0.9977, Mean Validation AUC: 0.9558, Epoch Time: 1.0547\n",
      "Epoch 21/100, Loss: 0.0472, Train AUC: 0.9979, Mean Validation AUC: 0.9502, Epoch Time: 1.0494\n",
      "Epoch 22/100, Loss: 0.0438, Train AUC: 0.9982, Mean Validation AUC: 0.9538, Epoch Time: 1.0613\n",
      "Epoch 23/100, Loss: 0.0404, Train AUC: 0.9985, Mean Validation AUC: 0.9545, Epoch Time: 1.0491\n",
      "Epoch 24/100, Loss: 0.0377, Train AUC: 0.9988, Mean Validation AUC: 0.9561, Epoch Time: 1.0563\n",
      "Epoch 25/100, Loss: 0.0350, Train AUC: 0.9989, Mean Validation AUC: 0.9569, Epoch Time: 1.0611\n",
      "Epoch 26/100, Loss: 0.0334, Train AUC: 0.9988, Mean Validation AUC: 0.9576, Epoch Time: 1.0472\n",
      "Epoch 27/100, Loss: 0.0315, Train AUC: 0.9990, Mean Validation AUC: 0.9536, Epoch Time: 1.0591\n",
      "Epoch 28/100, Loss: 0.0300, Train AUC: 0.9990, Mean Validation AUC: 0.9511, Epoch Time: 1.0528\n",
      "Epoch 29/100, Loss: 0.0291, Train AUC: 0.9990, Mean Validation AUC: 0.9588, Epoch Time: 1.0522\n",
      "Epoch 30/100, Loss: 0.0270, Train AUC: 0.9993, Mean Validation AUC: 0.9543, Epoch Time: 1.0525\n",
      "Epoch 31/100, Loss: 0.0262, Train AUC: 0.9992, Mean Validation AUC: 0.9524, Epoch Time: 1.0406\n",
      "Epoch 32/100, Loss: 0.0245, Train AUC: 0.9994, Mean Validation AUC: 0.9513, Epoch Time: 1.0552\n",
      "Epoch 33/100, Loss: 0.0239, Train AUC: 0.9994, Mean Validation AUC: 0.9526, Epoch Time: 1.0409\n",
      "Epoch 34/100, Loss: 0.0221, Train AUC: 0.9995, Mean Validation AUC: 0.9535, Epoch Time: 1.0645\n",
      "Early stopping triggered\n",
      "Bootstrap 0, Mean Test AUC: 0.9588\n",
      "bootsrap number: 2\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6654, Train AUC: 0.6222, Mean Validation AUC: 0.6487, Epoch Time: 1.0352\n",
      "Epoch 2/100, Loss: 0.4997, Train AUC: 0.7544, Mean Validation AUC: 0.7984, Epoch Time: 1.0300\n",
      "Epoch 3/100, Loss: 0.3900, Train AUC: 0.8440, Mean Validation AUC: 0.8482, Epoch Time: 1.0505\n",
      "Epoch 4/100, Loss: 0.3172, Train AUC: 0.9044, Mean Validation AUC: 0.8823, Epoch Time: 1.0410\n",
      "Epoch 5/100, Loss: 0.2650, Train AUC: 0.9405, Mean Validation AUC: 0.9112, Epoch Time: 1.0478\n",
      "Epoch 6/100, Loss: 0.2270, Train AUC: 0.9575, Mean Validation AUC: 0.9268, Epoch Time: 1.0408\n",
      "Epoch 7/100, Loss: 0.1955, Train AUC: 0.9698, Mean Validation AUC: 0.9287, Epoch Time: 1.0347\n",
      "Epoch 8/100, Loss: 0.1695, Train AUC: 0.9773, Mean Validation AUC: 0.9436, Epoch Time: 1.0273\n",
      "Epoch 9/100, Loss: 0.1485, Train AUC: 0.9825, Mean Validation AUC: 0.9438, Epoch Time: 1.0386\n",
      "Epoch 10/100, Loss: 0.1307, Train AUC: 0.9863, Mean Validation AUC: 0.9449, Epoch Time: 1.0320\n",
      "Epoch 11/100, Loss: 0.1167, Train AUC: 0.9893, Mean Validation AUC: 0.9497, Epoch Time: 1.0310\n",
      "Epoch 12/100, Loss: 0.1038, Train AUC: 0.9912, Mean Validation AUC: 0.9507, Epoch Time: 1.0348\n",
      "Epoch 13/100, Loss: 0.0933, Train AUC: 0.9931, Mean Validation AUC: 0.9525, Epoch Time: 1.0453\n",
      "Epoch 14/100, Loss: 0.0849, Train AUC: 0.9939, Mean Validation AUC: 0.9537, Epoch Time: 1.0423\n",
      "Epoch 15/100, Loss: 0.0760, Train AUC: 0.9951, Mean Validation AUC: 0.9524, Epoch Time: 1.0529\n",
      "Epoch 16/100, Loss: 0.0682, Train AUC: 0.9961, Mean Validation AUC: 0.9514, Epoch Time: 1.0548\n",
      "Epoch 17/100, Loss: 0.0627, Train AUC: 0.9966, Mean Validation AUC: 0.9512, Epoch Time: 1.0368\n",
      "Epoch 18/100, Loss: 0.0596, Train AUC: 0.9967, Mean Validation AUC: 0.9547, Epoch Time: 1.0311\n",
      "Epoch 19/100, Loss: 0.0541, Train AUC: 0.9975, Mean Validation AUC: 0.9516, Epoch Time: 1.0353\n",
      "Epoch 21/100, Loss: 0.0460, Train AUC: 0.9982, Mean Validation AUC: 0.9547, Epoch Time: 1.0390\n",
      "Epoch 22/100, Loss: 0.0428, Train AUC: 0.9984, Mean Validation AUC: 0.9517, Epoch Time: 1.0486\n",
      "Epoch 23/100, Loss: 0.0403, Train AUC: 0.9985, Mean Validation AUC: 0.9528, Epoch Time: 1.0452\n",
      "Epoch 24/100, Loss: 0.0393, Train AUC: 0.9984, Mean Validation AUC: 0.9543, Epoch Time: 1.0602\n",
      "Epoch 25/100, Loss: 0.0369, Train AUC: 0.9986, Mean Validation AUC: 0.9539, Epoch Time: 1.0315\n",
      "Epoch 26/100, Loss: 0.0350, Train AUC: 0.9988, Mean Validation AUC: 0.9550, Epoch Time: 1.0344\n",
      "Epoch 27/100, Loss: 0.0321, Train AUC: 0.9989, Mean Validation AUC: 0.9530, Epoch Time: 1.0422\n",
      "Epoch 28/100, Loss: 0.0311, Train AUC: 0.9990, Mean Validation AUC: 0.9565, Epoch Time: 1.0295\n",
      "Epoch 29/100, Loss: 0.0289, Train AUC: 0.9990, Mean Validation AUC: 0.9541, Epoch Time: 1.0266\n",
      "Epoch 30/100, Loss: 0.0274, Train AUC: 0.9993, Mean Validation AUC: 0.9530, Epoch Time: 1.0340\n",
      "Epoch 31/100, Loss: 0.0254, Train AUC: 0.9993, Mean Validation AUC: 0.9527, Epoch Time: 1.0482\n",
      "Epoch 32/100, Loss: 0.0252, Train AUC: 0.9993, Mean Validation AUC: 0.9565, Epoch Time: 1.0306\n",
      "Epoch 33/100, Loss: 0.0230, Train AUC: 0.9995, Mean Validation AUC: 0.9546, Epoch Time: 1.0511\n",
      "Epoch 34/100, Loss: 0.0224, Train AUC: 0.9995, Mean Validation AUC: 0.9539, Epoch Time: 1.0326\n",
      "Epoch 35/100, Loss: 0.0223, Train AUC: 0.9995, Mean Validation AUC: 0.9579, Epoch Time: 1.0359\n",
      "Epoch 36/100, Loss: 0.0196, Train AUC: 0.9996, Mean Validation AUC: 0.9534, Epoch Time: 1.0296\n",
      "Epoch 37/100, Loss: 0.0200, Train AUC: 0.9996, Mean Validation AUC: 0.9534, Epoch Time: 1.0293\n",
      "Epoch 38/100, Loss: 0.0189, Train AUC: 0.9996, Mean Validation AUC: 0.9545, Epoch Time: 1.0516\n",
      "Epoch 39/100, Loss: 0.0174, Train AUC: 0.9997, Mean Validation AUC: 0.9469, Epoch Time: 1.0282\n",
      "Epoch 40/100, Loss: 0.0170, Train AUC: 0.9997, Mean Validation AUC: 0.9519, Epoch Time: 1.0510\n",
      "Early stopping triggered\n",
      "Bootstrap 1, Mean Test AUC: 0.9578\n",
      "bootsrap number: 3\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5619, Train AUC: 0.6229, Mean Validation AUC: 0.6753, Epoch Time: 1.0567\n",
      "Epoch 2/100, Loss: 0.4375, Train AUC: 0.7710, Mean Validation AUC: 0.8481, Epoch Time: 1.0402\n",
      "Epoch 3/100, Loss: 0.3493, Train AUC: 0.8675, Mean Validation AUC: 0.8825, Epoch Time: 1.0281\n",
      "Epoch 4/100, Loss: 0.2895, Train AUC: 0.9203, Mean Validation AUC: 0.9024, Epoch Time: 1.0330\n",
      "Epoch 5/100, Loss: 0.2461, Train AUC: 0.9461, Mean Validation AUC: 0.9282, Epoch Time: 1.0307\n",
      "Epoch 6/100, Loss: 0.2127, Train AUC: 0.9605, Mean Validation AUC: 0.9420, Epoch Time: 1.0592\n",
      "Epoch 7/100, Loss: 0.1838, Train AUC: 0.9718, Mean Validation AUC: 0.9469, Epoch Time: 1.0436\n",
      "Epoch 8/100, Loss: 0.1606, Train AUC: 0.9786, Mean Validation AUC: 0.9495, Epoch Time: 1.0441\n",
      "Epoch 9/100, Loss: 0.1408, Train AUC: 0.9839, Mean Validation AUC: 0.9554, Epoch Time: 1.0516\n",
      "Epoch 10/100, Loss: 0.1218, Train AUC: 0.9885, Mean Validation AUC: 0.9536, Epoch Time: 1.0286\n",
      "Epoch 11/100, Loss: 0.1093, Train AUC: 0.9902, Mean Validation AUC: 0.9580, Epoch Time: 1.0504\n",
      "Epoch 12/100, Loss: 0.0970, Train AUC: 0.9923, Mean Validation AUC: 0.9590, Epoch Time: 1.0382\n",
      "Epoch 13/100, Loss: 0.0888, Train AUC: 0.9933, Mean Validation AUC: 0.9569, Epoch Time: 1.0444\n",
      "Epoch 14/100, Loss: 0.0792, Train AUC: 0.9948, Mean Validation AUC: 0.9575, Epoch Time: 1.0340\n",
      "Epoch 15/100, Loss: 0.0729, Train AUC: 0.9954, Mean Validation AUC: 0.9591, Epoch Time: 1.0332\n",
      "Epoch 16/100, Loss: 0.0673, Train AUC: 0.9959, Mean Validation AUC: 0.9581, Epoch Time: 1.0371\n",
      "Epoch 17/100, Loss: 0.0619, Train AUC: 0.9965, Mean Validation AUC: 0.9606, Epoch Time: 1.0339\n",
      "Epoch 18/100, Loss: 0.0570, Train AUC: 0.9972, Mean Validation AUC: 0.9606, Epoch Time: 1.0520\n",
      "Epoch 19/100, Loss: 0.0528, Train AUC: 0.9974, Mean Validation AUC: 0.9562, Epoch Time: 1.0302\n",
      "Epoch 20/100, Loss: 0.0500, Train AUC: 0.9976, Mean Validation AUC: 0.9591, Epoch Time: 1.0324\n",
      "Epoch 21/100, Loss: 0.0460, Train AUC: 0.9980, Mean Validation AUC: 0.9580, Epoch Time: 1.0291\n",
      "Epoch 22/100, Loss: 0.0426, Train AUC: 0.9984, Mean Validation AUC: 0.9576, Epoch Time: 1.0428\n",
      "Early stopping triggered\n",
      "Bootstrap 2, Mean Test AUC: 0.9604\n",
      "bootsrap number: 4\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.7053, Train AUC: 0.6131, Mean Validation AUC: 0.6253, Epoch Time: 1.0728\n",
      "Epoch 2/100, Loss: 0.5339, Train AUC: 0.7320, Mean Validation AUC: 0.7912, Epoch Time: 1.0790\n",
      "Epoch 3/100, Loss: 0.4139, Train AUC: 0.8326, Mean Validation AUC: 0.8418, Epoch Time: 1.0838\n",
      "Epoch 4/100, Loss: 0.3376, Train AUC: 0.8912, Mean Validation AUC: 0.8767, Epoch Time: 1.0558\n",
      "Epoch 5/100, Loss: 0.2808, Train AUC: 0.9302, Mean Validation AUC: 0.9111, Epoch Time: 1.0655\n",
      "Epoch 6/100, Loss: 0.2385, Train AUC: 0.9526, Mean Validation AUC: 0.9346, Epoch Time: 1.0829\n",
      "Epoch 7/100, Loss: 0.2041, Train AUC: 0.9669, Mean Validation AUC: 0.9452, Epoch Time: 1.0739\n",
      "Epoch 8/100, Loss: 0.1774, Train AUC: 0.9741, Mean Validation AUC: 0.9541, Epoch Time: 1.0747\n",
      "Epoch 9/100, Loss: 0.1535, Train AUC: 0.9816, Mean Validation AUC: 0.9564, Epoch Time: 1.0646\n",
      "Epoch 10/100, Loss: 0.1355, Train AUC: 0.9855, Mean Validation AUC: 0.9613, Epoch Time: 1.0657\n",
      "Epoch 11/100, Loss: 0.1195, Train AUC: 0.9888, Mean Validation AUC: 0.9634, Epoch Time: 1.0665\n",
      "Epoch 12/100, Loss: 0.1060, Train AUC: 0.9911, Mean Validation AUC: 0.9582, Epoch Time: 1.0749\n",
      "Epoch 13/100, Loss: 0.0947, Train AUC: 0.9929, Mean Validation AUC: 0.9577, Epoch Time: 1.0776\n",
      "Epoch 14/100, Loss: 0.0859, Train AUC: 0.9939, Mean Validation AUC: 0.9567, Epoch Time: 1.0853\n",
      "Epoch 15/100, Loss: 0.0777, Train AUC: 0.9949, Mean Validation AUC: 0.9606, Epoch Time: 1.0502\n",
      "Epoch 16/100, Loss: 0.0723, Train AUC: 0.9954, Mean Validation AUC: 0.9558, Epoch Time: 1.0666\n",
      "Early stopping triggered\n",
      "Bootstrap 3, Mean Test AUC: 0.9635\n",
      "bootsrap number: 5\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5683, Train AUC: 0.6335, Mean Validation AUC: 0.6430, Epoch Time: 1.0518\n",
      "Epoch 2/100, Loss: 0.4409, Train AUC: 0.7847, Mean Validation AUC: 0.8259, Epoch Time: 1.0504\n",
      "Epoch 3/100, Loss: 0.3535, Train AUC: 0.8750, Mean Validation AUC: 0.8743, Epoch Time: 1.0541\n",
      "Epoch 4/100, Loss: 0.2931, Train AUC: 0.9233, Mean Validation AUC: 0.9019, Epoch Time: 1.0674\n",
      "Epoch 5/100, Loss: 0.2497, Train AUC: 0.9482, Mean Validation AUC: 0.9217, Epoch Time: 1.0456\n",
      "Epoch 6/100, Loss: 0.2141, Train AUC: 0.9646, Mean Validation AUC: 0.9335, Epoch Time: 1.0548\n",
      "Epoch 7/100, Loss: 0.1832, Train AUC: 0.9747, Mean Validation AUC: 0.9457, Epoch Time: 1.0476\n",
      "Epoch 8/100, Loss: 0.1573, Train AUC: 0.9817, Mean Validation AUC: 0.9514, Epoch Time: 1.0549\n",
      "Epoch 9/100, Loss: 0.1376, Train AUC: 0.9853, Mean Validation AUC: 0.9599, Epoch Time: 1.0413\n",
      "Epoch 10/100, Loss: 0.1214, Train AUC: 0.9886, Mean Validation AUC: 0.9588, Epoch Time: 1.0427\n",
      "Epoch 11/100, Loss: 0.1087, Train AUC: 0.9907, Mean Validation AUC: 0.9617, Epoch Time: 1.0457\n",
      "Epoch 12/100, Loss: 0.0976, Train AUC: 0.9922, Mean Validation AUC: 0.9581, Epoch Time: 1.0505\n",
      "Epoch 13/100, Loss: 0.0867, Train AUC: 0.9939, Mean Validation AUC: 0.9588, Epoch Time: 1.0495\n",
      "Epoch 14/100, Loss: 0.0782, Train AUC: 0.9950, Mean Validation AUC: 0.9608, Epoch Time: 1.0473\n",
      "Epoch 15/100, Loss: 0.0733, Train AUC: 0.9953, Mean Validation AUC: 0.9561, Epoch Time: 1.0598\n",
      "Epoch 16/100, Loss: 0.0660, Train AUC: 0.9961, Mean Validation AUC: 0.9588, Epoch Time: 1.0495\n",
      "Early stopping triggered\n",
      "Bootstrap 4, Mean Test AUC: 0.9618\n",
      "Fold 3 Mean Validation AUC: 0.9605\n",
      "Fold 3 Mean Test AUC: 0.9605\n",
      "--------------------------------------------------------------------------------\n",
      "fold number 4\n",
      "--------------------------------------------------------------------------------\n",
      "bootsrap number: 1\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6697, Train AUC: 0.6331, Mean Validation AUC: 0.6600, Epoch Time: 1.0415\n",
      "Epoch 2/100, Loss: 0.5006, Train AUC: 0.7762, Mean Validation AUC: 0.8052, Epoch Time: 1.0265\n",
      "Epoch 3/100, Loss: 0.3891, Train AUC: 0.8610, Mean Validation AUC: 0.8431, Epoch Time: 1.0387\n",
      "Epoch 4/100, Loss: 0.3153, Train AUC: 0.9125, Mean Validation AUC: 0.8904, Epoch Time: 1.0351\n",
      "Epoch 5/100, Loss: 0.2624, Train AUC: 0.9435, Mean Validation AUC: 0.9170, Epoch Time: 1.0407\n",
      "Epoch 6/100, Loss: 0.2213, Train AUC: 0.9620, Mean Validation AUC: 0.9312, Epoch Time: 1.0366\n",
      "Epoch 7/100, Loss: 0.1898, Train AUC: 0.9720, Mean Validation AUC: 0.9386, Epoch Time: 1.0322\n",
      "Epoch 8/100, Loss: 0.1636, Train AUC: 0.9790, Mean Validation AUC: 0.9464, Epoch Time: 1.0508\n",
      "Epoch 9/100, Loss: 0.1412, Train AUC: 0.9847, Mean Validation AUC: 0.9477, Epoch Time: 1.0304\n",
      "Epoch 10/100, Loss: 0.1256, Train AUC: 0.9877, Mean Validation AUC: 0.9502, Epoch Time: 1.0322\n",
      "Epoch 11/100, Loss: 0.1119, Train AUC: 0.9899, Mean Validation AUC: 0.9544, Epoch Time: 1.0291\n",
      "Epoch 12/100, Loss: 0.0996, Train AUC: 0.9919, Mean Validation AUC: 0.9526, Epoch Time: 1.0328\n",
      "Epoch 13/100, Loss: 0.0889, Train AUC: 0.9935, Mean Validation AUC: 0.9523, Epoch Time: 1.0360\n",
      "Epoch 14/100, Loss: 0.0812, Train AUC: 0.9943, Mean Validation AUC: 0.9544, Epoch Time: 1.0448\n",
      "Epoch 15/100, Loss: 0.0746, Train AUC: 0.9951, Mean Validation AUC: 0.9513, Epoch Time: 1.0339\n",
      "Epoch 16/100, Loss: 0.0674, Train AUC: 0.9962, Mean Validation AUC: 0.9517, Epoch Time: 1.0272\n",
      "Epoch 17/100, Loss: 0.0622, Train AUC: 0.9966, Mean Validation AUC: 0.9503, Epoch Time: 1.0469\n",
      "Epoch 18/100, Loss: 0.0584, Train AUC: 0.9969, Mean Validation AUC: 0.9513, Epoch Time: 1.0288\n",
      "Epoch 19/100, Loss: 0.0540, Train AUC: 0.9974, Mean Validation AUC: 0.9483, Epoch Time: 1.0295\n",
      "Early stopping triggered\n",
      "Bootstrap 0, Mean Test AUC: 0.9545\n",
      "bootsrap number: 2\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6671, Train AUC: 0.6239, Mean Validation AUC: 0.6662, Epoch Time: 1.0614\n",
      "Epoch 2/100, Loss: 0.5000, Train AUC: 0.7760, Mean Validation AUC: 0.8039, Epoch Time: 1.0714\n",
      "Epoch 3/100, Loss: 0.3870, Train AUC: 0.8644, Mean Validation AUC: 0.8618, Epoch Time: 1.0936\n",
      "Epoch 4/100, Loss: 0.3088, Train AUC: 0.9198, Mean Validation AUC: 0.8887, Epoch Time: 1.0765\n",
      "Epoch 5/100, Loss: 0.2533, Train AUC: 0.9493, Mean Validation AUC: 0.9130, Epoch Time: 1.0828\n",
      "Epoch 6/100, Loss: 0.2137, Train AUC: 0.9655, Mean Validation AUC: 0.9225, Epoch Time: 1.0632\n",
      "Epoch 7/100, Loss: 0.1828, Train AUC: 0.9750, Mean Validation AUC: 0.9298, Epoch Time: 1.0746\n",
      "Epoch 8/100, Loss: 0.1590, Train AUC: 0.9803, Mean Validation AUC: 0.9364, Epoch Time: 1.0584\n",
      "Epoch 9/100, Loss: 0.1389, Train AUC: 0.9848, Mean Validation AUC: 0.9409, Epoch Time: 1.0683\n",
      "Epoch 10/100, Loss: 0.1233, Train AUC: 0.9880, Mean Validation AUC: 0.9437, Epoch Time: 1.0663\n",
      "Epoch 11/100, Loss: 0.1094, Train AUC: 0.9904, Mean Validation AUC: 0.9477, Epoch Time: 1.0724\n",
      "Epoch 12/100, Loss: 0.0975, Train AUC: 0.9923, Mean Validation AUC: 0.9499, Epoch Time: 1.0701\n",
      "Epoch 13/100, Loss: 0.0875, Train AUC: 0.9935, Mean Validation AUC: 0.9451, Epoch Time: 1.0674\n",
      "Epoch 14/100, Loss: 0.0789, Train AUC: 0.9947, Mean Validation AUC: 0.9401, Epoch Time: 1.0635\n",
      "Epoch 15/100, Loss: 0.0723, Train AUC: 0.9953, Mean Validation AUC: 0.9495, Epoch Time: 1.0656\n",
      "Epoch 16/100, Loss: 0.0654, Train AUC: 0.9960, Mean Validation AUC: 0.9477, Epoch Time: 1.0906\n",
      "Epoch 17/100, Loss: 0.0604, Train AUC: 0.9966, Mean Validation AUC: 0.9461, Epoch Time: 1.0768\n",
      "Early stopping triggered\n",
      "Bootstrap 1, Mean Test AUC: 0.9500\n",
      "bootsrap number: 3\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5621, Train AUC: 0.6376, Mean Validation AUC: 0.6644, Epoch Time: 1.0460\n",
      "Epoch 2/100, Loss: 0.4291, Train AUC: 0.7976, Mean Validation AUC: 0.8041, Epoch Time: 1.0499\n",
      "Epoch 3/100, Loss: 0.3415, Train AUC: 0.8824, Mean Validation AUC: 0.8562, Epoch Time: 1.0786\n",
      "Epoch 4/100, Loss: 0.2825, Train AUC: 0.9247, Mean Validation AUC: 0.8865, Epoch Time: 1.0618\n",
      "Epoch 5/100, Loss: 0.2362, Train AUC: 0.9524, Mean Validation AUC: 0.9066, Epoch Time: 1.0536\n",
      "Epoch 6/100, Loss: 0.2008, Train AUC: 0.9661, Mean Validation AUC: 0.9190, Epoch Time: 1.0477\n",
      "Epoch 7/100, Loss: 0.1719, Train AUC: 0.9759, Mean Validation AUC: 0.9336, Epoch Time: 1.0483\n",
      "Epoch 8/100, Loss: 0.1498, Train AUC: 0.9816, Mean Validation AUC: 0.9279, Epoch Time: 1.0651\n",
      "Epoch 9/100, Loss: 0.1315, Train AUC: 0.9856, Mean Validation AUC: 0.9399, Epoch Time: 1.0669\n",
      "Epoch 10/100, Loss: 0.1173, Train AUC: 0.9881, Mean Validation AUC: 0.9379, Epoch Time: 1.0444\n",
      "Epoch 11/100, Loss: 0.1046, Train AUC: 0.9906, Mean Validation AUC: 0.9337, Epoch Time: 1.0489\n",
      "Epoch 12/100, Loss: 0.0934, Train AUC: 0.9925, Mean Validation AUC: 0.9443, Epoch Time: 1.0347\n",
      "Epoch 13/100, Loss: 0.0831, Train AUC: 0.9943, Mean Validation AUC: 0.9436, Epoch Time: 1.0603\n",
      "Epoch 14/100, Loss: 0.0754, Train AUC: 0.9953, Mean Validation AUC: 0.9451, Epoch Time: 1.0433\n",
      "Epoch 15/100, Loss: 0.0697, Train AUC: 0.9957, Mean Validation AUC: 0.9423, Epoch Time: 1.0579\n",
      "Epoch 16/100, Loss: 0.0633, Train AUC: 0.9965, Mean Validation AUC: 0.9392, Epoch Time: 1.0494\n",
      "Epoch 17/100, Loss: 0.0587, Train AUC: 0.9969, Mean Validation AUC: 0.9457, Epoch Time: 1.0484\n",
      "Epoch 18/100, Loss: 0.0541, Train AUC: 0.9974, Mean Validation AUC: 0.9448, Epoch Time: 1.0541\n",
      "Epoch 19/100, Loss: 0.0491, Train AUC: 0.9978, Mean Validation AUC: 0.9452, Epoch Time: 1.0558\n",
      "Epoch 20/100, Loss: 0.0461, Train AUC: 0.9980, Mean Validation AUC: 0.9439, Epoch Time: 1.0504\n",
      "Epoch 21/100, Loss: 0.0426, Train AUC: 0.9983, Mean Validation AUC: 0.9438, Epoch Time: 1.0451\n",
      "Epoch 22/100, Loss: 0.0407, Train AUC: 0.9984, Mean Validation AUC: 0.9415, Epoch Time: 1.0437\n",
      "Early stopping triggered\n",
      "Bootstrap 2, Mean Test AUC: 0.9457\n",
      "bootsrap number: 4\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5645, Train AUC: 0.6494, Mean Validation AUC: 0.6971, Epoch Time: 1.0481\n",
      "Epoch 2/100, Loss: 0.4257, Train AUC: 0.8073, Mean Validation AUC: 0.8126, Epoch Time: 1.0456\n",
      "Epoch 3/100, Loss: 0.3346, Train AUC: 0.8921, Mean Validation AUC: 0.8723, Epoch Time: 1.0627\n",
      "Epoch 4/100, Loss: 0.2745, Train AUC: 0.9318, Mean Validation AUC: 0.9021, Epoch Time: 1.0525\n",
      "Epoch 5/100, Loss: 0.2299, Train AUC: 0.9551, Mean Validation AUC: 0.9225, Epoch Time: 1.0398\n",
      "Epoch 6/100, Loss: 0.1956, Train AUC: 0.9684, Mean Validation AUC: 0.9335, Epoch Time: 1.0528\n",
      "Epoch 7/100, Loss: 0.1678, Train AUC: 0.9776, Mean Validation AUC: 0.9391, Epoch Time: 1.0431\n",
      "Epoch 8/100, Loss: 0.1474, Train AUC: 0.9824, Mean Validation AUC: 0.9472, Epoch Time: 1.0394\n",
      "Epoch 9/100, Loss: 0.1290, Train AUC: 0.9867, Mean Validation AUC: 0.9508, Epoch Time: 1.0570\n",
      "Epoch 10/100, Loss: 0.1155, Train AUC: 0.9891, Mean Validation AUC: 0.9502, Epoch Time: 1.0492\n",
      "Epoch 11/100, Loss: 0.1021, Train AUC: 0.9917, Mean Validation AUC: 0.9494, Epoch Time: 1.0543\n",
      "Epoch 12/100, Loss: 0.0929, Train AUC: 0.9927, Mean Validation AUC: 0.9546, Epoch Time: 1.0402\n",
      "Epoch 13/100, Loss: 0.0842, Train AUC: 0.9941, Mean Validation AUC: 0.9561, Epoch Time: 1.0523\n",
      "Epoch 14/100, Loss: 0.0768, Train AUC: 0.9949, Mean Validation AUC: 0.9507, Epoch Time: 1.0464\n",
      "Epoch 15/100, Loss: 0.0700, Train AUC: 0.9958, Mean Validation AUC: 0.9495, Epoch Time: 1.0494\n",
      "Epoch 16/100, Loss: 0.0630, Train AUC: 0.9967, Mean Validation AUC: 0.9502, Epoch Time: 1.0306\n",
      "Epoch 17/100, Loss: 0.0595, Train AUC: 0.9969, Mean Validation AUC: 0.9518, Epoch Time: 1.0423\n",
      "Epoch 18/100, Loss: 0.0548, Train AUC: 0.9974, Mean Validation AUC: 0.9486, Epoch Time: 1.0571\n",
      "Early stopping triggered\n",
      "Bootstrap 3, Mean Test AUC: 0.9560\n",
      "bootsrap number: 5\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6324, Train AUC: 0.6433, Mean Validation AUC: 0.6946, Epoch Time: 1.0607\n",
      "Epoch 2/100, Loss: 0.4716, Train AUC: 0.8008, Mean Validation AUC: 0.8251, Epoch Time: 1.0547\n",
      "Epoch 3/100, Loss: 0.3649, Train AUC: 0.8836, Mean Validation AUC: 0.8816, Epoch Time: 1.0713\n",
      "Epoch 4/100, Loss: 0.2950, Train AUC: 0.9274, Mean Validation AUC: 0.9080, Epoch Time: 1.0568\n",
      "Epoch 5/100, Loss: 0.2421, Train AUC: 0.9549, Mean Validation AUC: 0.9189, Epoch Time: 1.0742\n",
      "Epoch 6/100, Loss: 0.2063, Train AUC: 0.9666, Mean Validation AUC: 0.9363, Epoch Time: 1.0817\n",
      "Epoch 7/100, Loss: 0.1750, Train AUC: 0.9770, Mean Validation AUC: 0.9449, Epoch Time: 1.0654\n",
      "Epoch 8/100, Loss: 0.1511, Train AUC: 0.9828, Mean Validation AUC: 0.9458, Epoch Time: 1.0773\n",
      "Epoch 9/100, Loss: 0.1317, Train AUC: 0.9868, Mean Validation AUC: 0.9531, Epoch Time: 1.0630\n",
      "Epoch 10/100, Loss: 0.1169, Train AUC: 0.9891, Mean Validation AUC: 0.9536, Epoch Time: 1.0737\n",
      "Epoch 11/100, Loss: 0.1032, Train AUC: 0.9913, Mean Validation AUC: 0.9515, Epoch Time: 1.0585\n",
      "Epoch 12/100, Loss: 0.0914, Train AUC: 0.9931, Mean Validation AUC: 0.9467, Epoch Time: 1.0690\n",
      "Epoch 13/100, Loss: 0.0825, Train AUC: 0.9942, Mean Validation AUC: 0.9520, Epoch Time: 1.0681\n",
      "Epoch 14/100, Loss: 0.0742, Train AUC: 0.9954, Mean Validation AUC: 0.9488, Epoch Time: 1.0770\n",
      "Epoch 15/100, Loss: 0.0693, Train AUC: 0.9957, Mean Validation AUC: 0.9486, Epoch Time: 1.0684\n",
      "Early stopping triggered\n",
      "Bootstrap 4, Mean Test AUC: 0.9537\n",
      "Fold 4 Mean Validation AUC: 0.9519\n",
      "Fold 4 Mean Test AUC: 0.9520\n",
      "--------------------------------------------------------------------------------\n",
      "fold number 5\n",
      "--------------------------------------------------------------------------------\n",
      "bootsrap number: 1\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6599, Train AUC: 0.6095, Mean Validation AUC: 0.6342, Epoch Time: 1.0500\n",
      "Epoch 2/100, Loss: 0.4967, Train AUC: 0.7639, Mean Validation AUC: 0.7557, Epoch Time: 1.0578\n",
      "Epoch 3/100, Loss: 0.3855, Train AUC: 0.8603, Mean Validation AUC: 0.8148, Epoch Time: 1.0605\n",
      "Epoch 4/100, Loss: 0.3103, Train AUC: 0.9149, Mean Validation AUC: 0.8643, Epoch Time: 1.0507\n",
      "Epoch 5/100, Loss: 0.2554, Train AUC: 0.9459, Mean Validation AUC: 0.8840, Epoch Time: 1.0350\n",
      "Epoch 6/100, Loss: 0.2138, Train AUC: 0.9638, Mean Validation AUC: 0.9027, Epoch Time: 1.0493\n",
      "Epoch 7/100, Loss: 0.1813, Train AUC: 0.9750, Mean Validation AUC: 0.9109, Epoch Time: 1.0579\n",
      "Epoch 8/100, Loss: 0.1545, Train AUC: 0.9817, Mean Validation AUC: 0.9194, Epoch Time: 1.0741\n",
      "Epoch 9/100, Loss: 0.1358, Train AUC: 0.9855, Mean Validation AUC: 0.9257, Epoch Time: 1.0584\n",
      "Epoch 10/100, Loss: 0.1200, Train AUC: 0.9889, Mean Validation AUC: 0.9269, Epoch Time: 1.0542\n",
      "Epoch 11/100, Loss: 0.1066, Train AUC: 0.9911, Mean Validation AUC: 0.9292, Epoch Time: 1.0612\n",
      "Epoch 12/100, Loss: 0.0963, Train AUC: 0.9924, Mean Validation AUC: 0.9302, Epoch Time: 1.0614\n",
      "Epoch 13/100, Loss: 0.0870, Train AUC: 0.9937, Mean Validation AUC: 0.9303, Epoch Time: 1.0517\n",
      "Epoch 14/100, Loss: 0.0789, Train AUC: 0.9949, Mean Validation AUC: 0.9306, Epoch Time: 1.0623\n",
      "Epoch 15/100, Loss: 0.0727, Train AUC: 0.9955, Mean Validation AUC: 0.9326, Epoch Time: 1.0485\n",
      "Epoch 16/100, Loss: 0.0667, Train AUC: 0.9962, Mean Validation AUC: 0.9351, Epoch Time: 1.0734\n",
      "Epoch 17/100, Loss: 0.0605, Train AUC: 0.9969, Mean Validation AUC: 0.9348, Epoch Time: 1.0736\n",
      "Epoch 18/100, Loss: 0.0571, Train AUC: 0.9971, Mean Validation AUC: 0.9340, Epoch Time: 1.0671\n",
      "Epoch 19/100, Loss: 0.0528, Train AUC: 0.9976, Mean Validation AUC: 0.9322, Epoch Time: 1.0430\n",
      "Epoch 20/100, Loss: 0.0503, Train AUC: 0.9976, Mean Validation AUC: 0.9398, Epoch Time: 1.0629\n",
      "Epoch 21/100, Loss: 0.0454, Train AUC: 0.9982, Mean Validation AUC: 0.9331, Epoch Time: 1.0642\n",
      "Epoch 22/100, Loss: 0.0443, Train AUC: 0.9981, Mean Validation AUC: 0.9357, Epoch Time: 1.0677\n",
      "Epoch 23/100, Loss: 0.0413, Train AUC: 0.9983, Mean Validation AUC: 0.9319, Epoch Time: 1.0547\n",
      "Epoch 24/100, Loss: 0.0388, Train AUC: 0.9986, Mean Validation AUC: 0.9412, Epoch Time: 1.0502\n",
      "Epoch 25/100, Loss: 0.0364, Train AUC: 0.9987, Mean Validation AUC: 0.9380, Epoch Time: 1.0599\n",
      "Epoch 26/100, Loss: 0.0343, Train AUC: 0.9989, Mean Validation AUC: 0.9436, Epoch Time: 1.0680\n",
      "Epoch 27/100, Loss: 0.0331, Train AUC: 0.9989, Mean Validation AUC: 0.9406, Epoch Time: 1.0607\n",
      "Epoch 28/100, Loss: 0.0307, Train AUC: 0.9990, Mean Validation AUC: 0.9407, Epoch Time: 1.0474\n",
      "Epoch 29/100, Loss: 0.0277, Train AUC: 0.9993, Mean Validation AUC: 0.9384, Epoch Time: 1.0503\n",
      "Epoch 30/100, Loss: 0.0269, Train AUC: 0.9992, Mean Validation AUC: 0.9364, Epoch Time: 1.0534\n",
      "Epoch 31/100, Loss: 0.0260, Train AUC: 0.9993, Mean Validation AUC: 0.9370, Epoch Time: 1.0621\n",
      "Early stopping triggered\n",
      "Bootstrap 0, Mean Test AUC: 0.9436\n",
      "bootsrap number: 2\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.6217, Train AUC: 0.6252, Mean Validation AUC: 0.6817, Epoch Time: 1.0486\n",
      "Epoch 2/100, Loss: 0.4678, Train AUC: 0.7906, Mean Validation AUC: 0.7424, Epoch Time: 1.0531\n",
      "Epoch 3/100, Loss: 0.3652, Train AUC: 0.8785, Mean Validation AUC: 0.7975, Epoch Time: 1.0445\n",
      "Epoch 4/100, Loss: 0.2955, Train AUC: 0.9269, Mean Validation AUC: 0.8570, Epoch Time: 1.0566\n",
      "Epoch 5/100, Loss: 0.2431, Train AUC: 0.9535, Mean Validation AUC: 0.8863, Epoch Time: 1.0441\n",
      "Epoch 6/100, Loss: 0.2027, Train AUC: 0.9693, Mean Validation AUC: 0.9034, Epoch Time: 1.0359\n",
      "Epoch 7/100, Loss: 0.1717, Train AUC: 0.9773, Mean Validation AUC: 0.9166, Epoch Time: 1.0523\n",
      "Epoch 8/100, Loss: 0.1493, Train AUC: 0.9820, Mean Validation AUC: 0.9219, Epoch Time: 1.0510\n",
      "Epoch 9/100, Loss: 0.1320, Train AUC: 0.9857, Mean Validation AUC: 0.9245, Epoch Time: 1.0561\n",
      "Epoch 10/100, Loss: 0.1177, Train AUC: 0.9883, Mean Validation AUC: 0.9260, Epoch Time: 1.0669\n",
      "Epoch 11/100, Loss: 0.1065, Train AUC: 0.9903, Mean Validation AUC: 0.9279, Epoch Time: 1.0414\n",
      "Epoch 12/100, Loss: 0.0958, Train AUC: 0.9922, Mean Validation AUC: 0.9316, Epoch Time: 1.0729\n",
      "Epoch 13/100, Loss: 0.0867, Train AUC: 0.9935, Mean Validation AUC: 0.9297, Epoch Time: 1.0431\n",
      "Epoch 14/100, Loss: 0.0792, Train AUC: 0.9944, Mean Validation AUC: 0.9319, Epoch Time: 1.0503\n",
      "Epoch 15/100, Loss: 0.0728, Train AUC: 0.9952, Mean Validation AUC: 0.9317, Epoch Time: 1.0431\n",
      "Epoch 16/100, Loss: 0.0659, Train AUC: 0.9960, Mean Validation AUC: 0.9337, Epoch Time: 1.0367\n",
      "Epoch 17/100, Loss: 0.0612, Train AUC: 0.9966, Mean Validation AUC: 0.9341, Epoch Time: 1.0519\n",
      "Epoch 18/100, Loss: 0.0571, Train AUC: 0.9970, Mean Validation AUC: 0.9336, Epoch Time: 1.0672\n",
      "Epoch 19/100, Loss: 0.0525, Train AUC: 0.9975, Mean Validation AUC: 0.9309, Epoch Time: 1.0464\n",
      "Epoch 20/100, Loss: 0.0493, Train AUC: 0.9977, Mean Validation AUC: 0.9271, Epoch Time: 1.0496\n",
      "Epoch 21/100, Loss: 0.0463, Train AUC: 0.9980, Mean Validation AUC: 0.9350, Epoch Time: 1.0464\n",
      "Epoch 22/100, Loss: 0.0420, Train AUC: 0.9984, Mean Validation AUC: 0.9385, Epoch Time: 1.0506\n",
      "Epoch 23/100, Loss: 0.0397, Train AUC: 0.9985, Mean Validation AUC: 0.9376, Epoch Time: 1.0566\n",
      "Epoch 24/100, Loss: 0.0381, Train AUC: 0.9985, Mean Validation AUC: 0.9428, Epoch Time: 1.0474\n",
      "Epoch 25/100, Loss: 0.0357, Train AUC: 0.9988, Mean Validation AUC: 0.9421, Epoch Time: 1.0417\n",
      "Epoch 26/100, Loss: 0.0335, Train AUC: 0.9989, Mean Validation AUC: 0.9367, Epoch Time: 1.0319\n",
      "Epoch 27/100, Loss: 0.0309, Train AUC: 0.9991, Mean Validation AUC: 0.9371, Epoch Time: 1.0511\n",
      "Epoch 28/100, Loss: 0.0288, Train AUC: 0.9992, Mean Validation AUC: 0.9390, Epoch Time: 1.0508\n",
      "Epoch 29/100, Loss: 0.0271, Train AUC: 0.9993, Mean Validation AUC: 0.9406, Epoch Time: 1.0556\n",
      "Early stopping triggered\n",
      "Bootstrap 1, Mean Test AUC: 0.9430\n",
      "bootsrap number: 3\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5531, Train AUC: 0.6314, Mean Validation AUC: 0.6887, Epoch Time: 1.0856\n",
      "Epoch 2/100, Loss: 0.4211, Train AUC: 0.8029, Mean Validation AUC: 0.7658, Epoch Time: 1.0797\n",
      "Epoch 3/100, Loss: 0.3351, Train AUC: 0.8833, Mean Validation AUC: 0.8085, Epoch Time: 1.0825\n",
      "Epoch 4/100, Loss: 0.2750, Train AUC: 0.9288, Mean Validation AUC: 0.8563, Epoch Time: 1.1037\n",
      "Epoch 5/100, Loss: 0.2299, Train AUC: 0.9544, Mean Validation AUC: 0.8778, Epoch Time: 1.0842\n",
      "Epoch 6/100, Loss: 0.1929, Train AUC: 0.9696, Mean Validation AUC: 0.8975, Epoch Time: 1.0850\n",
      "Epoch 7/100, Loss: 0.1646, Train AUC: 0.9781, Mean Validation AUC: 0.8993, Epoch Time: 1.0801\n",
      "Epoch 8/100, Loss: 0.1426, Train AUC: 0.9838, Mean Validation AUC: 0.9126, Epoch Time: 1.0874\n",
      "Epoch 9/100, Loss: 0.1246, Train AUC: 0.9875, Mean Validation AUC: 0.9202, Epoch Time: 1.0660\n",
      "Epoch 10/100, Loss: 0.1100, Train AUC: 0.9901, Mean Validation AUC: 0.9248, Epoch Time: 1.0734\n",
      "Epoch 11/100, Loss: 0.0986, Train AUC: 0.9919, Mean Validation AUC: 0.9248, Epoch Time: 1.0868\n",
      "Epoch 12/100, Loss: 0.0887, Train AUC: 0.9933, Mean Validation AUC: 0.9251, Epoch Time: 1.0776\n",
      "Epoch 13/100, Loss: 0.0821, Train AUC: 0.9939, Mean Validation AUC: 0.9278, Epoch Time: 1.0882\n",
      "Epoch 14/100, Loss: 0.0738, Train AUC: 0.9953, Mean Validation AUC: 0.9261, Epoch Time: 1.0883\n",
      "Epoch 15/100, Loss: 0.0695, Train AUC: 0.9957, Mean Validation AUC: 0.9300, Epoch Time: 1.0940\n",
      "Epoch 16/100, Loss: 0.0630, Train AUC: 0.9963, Mean Validation AUC: 0.9317, Epoch Time: 1.0852\n",
      "Epoch 17/100, Loss: 0.0579, Train AUC: 0.9968, Mean Validation AUC: 0.9324, Epoch Time: 1.0725\n",
      "Epoch 18/100, Loss: 0.0546, Train AUC: 0.9973, Mean Validation AUC: 0.9322, Epoch Time: 1.0805\n",
      "Epoch 19/100, Loss: 0.0499, Train AUC: 0.9977, Mean Validation AUC: 0.9356, Epoch Time: 1.0639\n",
      "Epoch 20/100, Loss: 0.0470, Train AUC: 0.9979, Mean Validation AUC: 0.9323, Epoch Time: 1.0993\n",
      "Epoch 21/100, Loss: 0.0457, Train AUC: 0.9980, Mean Validation AUC: 0.9313, Epoch Time: 1.1003\n",
      "Epoch 22/100, Loss: 0.0412, Train AUC: 0.9984, Mean Validation AUC: 0.9315, Epoch Time: 1.0800\n",
      "Epoch 23/100, Loss: 0.0392, Train AUC: 0.9986, Mean Validation AUC: 0.9336, Epoch Time: 1.0798\n",
      "Epoch 24/100, Loss: 0.0368, Train AUC: 0.9987, Mean Validation AUC: 0.9318, Epoch Time: 1.0657\n",
      "Early stopping triggered\n",
      "Bootstrap 2, Mean Test AUC: 0.9354\n",
      "bootsrap number: 4\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5694, Train AUC: 0.6297, Mean Validation AUC: 0.6776, Epoch Time: 1.0748\n",
      "Epoch 2/100, Loss: 0.4344, Train AUC: 0.7970, Mean Validation AUC: 0.7527, Epoch Time: 1.0523\n",
      "Epoch 3/100, Loss: 0.3441, Train AUC: 0.8859, Mean Validation AUC: 0.8086, Epoch Time: 1.0634\n",
      "Epoch 4/100, Loss: 0.2821, Train AUC: 0.9310, Mean Validation AUC: 0.8629, Epoch Time: 1.0570\n",
      "Epoch 5/100, Loss: 0.2343, Train AUC: 0.9569, Mean Validation AUC: 0.8898, Epoch Time: 1.0683\n",
      "Epoch 6/100, Loss: 0.1946, Train AUC: 0.9714, Mean Validation AUC: 0.9071, Epoch Time: 1.0554\n",
      "Epoch 7/100, Loss: 0.1660, Train AUC: 0.9788, Mean Validation AUC: 0.9204, Epoch Time: 1.0643\n",
      "Epoch 8/100, Loss: 0.1437, Train AUC: 0.9841, Mean Validation AUC: 0.9223, Epoch Time: 1.0604\n",
      "Epoch 9/100, Loss: 0.1252, Train AUC: 0.9876, Mean Validation AUC: 0.9282, Epoch Time: 1.0449\n",
      "Epoch 10/100, Loss: 0.1106, Train AUC: 0.9901, Mean Validation AUC: 0.9321, Epoch Time: 1.0587\n",
      "Epoch 11/100, Loss: 0.0993, Train AUC: 0.9919, Mean Validation AUC: 0.9346, Epoch Time: 1.0782\n",
      "Epoch 12/100, Loss: 0.0899, Train AUC: 0.9933, Mean Validation AUC: 0.9366, Epoch Time: 1.0576\n",
      "Epoch 13/100, Loss: 0.0810, Train AUC: 0.9942, Mean Validation AUC: 0.9383, Epoch Time: 1.0469\n",
      "Epoch 14/100, Loss: 0.0737, Train AUC: 0.9953, Mean Validation AUC: 0.9401, Epoch Time: 1.0709\n",
      "Epoch 15/100, Loss: 0.0680, Train AUC: 0.9959, Mean Validation AUC: 0.9432, Epoch Time: 1.0505\n",
      "Epoch 16/100, Loss: 0.0622, Train AUC: 0.9965, Mean Validation AUC: 0.9436, Epoch Time: 1.0544\n",
      "Epoch 17/100, Loss: 0.0578, Train AUC: 0.9970, Mean Validation AUC: 0.9404, Epoch Time: 1.0467\n",
      "Epoch 18/100, Loss: 0.0544, Train AUC: 0.9972, Mean Validation AUC: 0.9423, Epoch Time: 1.0511\n",
      "Epoch 19/100, Loss: 0.0502, Train AUC: 0.9977, Mean Validation AUC: 0.9427, Epoch Time: 1.0713\n",
      "Epoch 20/100, Loss: 0.0474, Train AUC: 0.9978, Mean Validation AUC: 0.9435, Epoch Time: 1.0471\n",
      "Epoch 21/100, Loss: 0.0439, Train AUC: 0.9982, Mean Validation AUC: 0.9432, Epoch Time: 1.0569\n",
      "Early stopping triggered\n",
      "Bootstrap 3, Mean Test AUC: 0.9436\n",
      "bootsrap number: 5\n",
      "--------------------------------------------------------------------------------\n",
      "All features without finetune & with protein attention (from last papaer)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Generated model successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/100, Loss: 0.5184, Train AUC: 0.6411, Mean Validation AUC: 0.7058, Epoch Time: 1.0657\n",
      "Epoch 2/100, Loss: 0.4014, Train AUC: 0.8103, Mean Validation AUC: 0.7471, Epoch Time: 1.0638\n",
      "Epoch 3/100, Loss: 0.3261, Train AUC: 0.8868, Mean Validation AUC: 0.8019, Epoch Time: 1.0538\n",
      "Epoch 4/100, Loss: 0.2722, Train AUC: 0.9284, Mean Validation AUC: 0.8581, Epoch Time: 1.0706\n",
      "Epoch 5/100, Loss: 0.2265, Train AUC: 0.9566, Mean Validation AUC: 0.8830, Epoch Time: 1.0723\n",
      "Epoch 6/100, Loss: 0.1907, Train AUC: 0.9697, Mean Validation AUC: 0.8971, Epoch Time: 1.0723\n",
      "Epoch 7/100, Loss: 0.1621, Train AUC: 0.9783, Mean Validation AUC: 0.9062, Epoch Time: 1.0587\n",
      "Epoch 8/100, Loss: 0.1401, Train AUC: 0.9836, Mean Validation AUC: 0.9130, Epoch Time: 1.0550\n",
      "Epoch 9/100, Loss: 0.1252, Train AUC: 0.9864, Mean Validation AUC: 0.9189, Epoch Time: 1.0717\n",
      "Epoch 10/100, Loss: 0.1099, Train AUC: 0.9897, Mean Validation AUC: 0.9234, Epoch Time: 1.0601\n",
      "Epoch 11/100, Loss: 0.0992, Train AUC: 0.9913, Mean Validation AUC: 0.9201, Epoch Time: 1.0666\n",
      "Epoch 12/100, Loss: 0.0894, Train AUC: 0.9929, Mean Validation AUC: 0.9275, Epoch Time: 1.0684\n",
      "Epoch 13/100, Loss: 0.0816, Train AUC: 0.9939, Mean Validation AUC: 0.9280, Epoch Time: 1.0715\n",
      "Epoch 14/100, Loss: 0.0749, Train AUC: 0.9947, Mean Validation AUC: 0.9275, Epoch Time: 1.0542\n",
      "Epoch 15/100, Loss: 0.0693, Train AUC: 0.9953, Mean Validation AUC: 0.9313, Epoch Time: 1.0723\n",
      "Epoch 16/100, Loss: 0.0631, Train AUC: 0.9962, Mean Validation AUC: 0.9344, Epoch Time: 1.0553\n",
      "Epoch 17/100, Loss: 0.0592, Train AUC: 0.9966, Mean Validation AUC: 0.9322, Epoch Time: 1.0579\n",
      "Epoch 18/100, Loss: 0.0541, Train AUC: 0.9973, Mean Validation AUC: 0.9322, Epoch Time: 1.0499\n",
      "Epoch 19/100, Loss: 0.0527, Train AUC: 0.9972, Mean Validation AUC: 0.9334, Epoch Time: 1.0609\n",
      "Epoch 20/100, Loss: 0.0492, Train AUC: 0.9975, Mean Validation AUC: 0.9270, Epoch Time: 1.0668\n",
      "Epoch 21/100, Loss: 0.0443, Train AUC: 0.9980, Mean Validation AUC: 0.9316, Epoch Time: 1.0584\n",
      "Early stopping triggered\n",
      "Bootstrap 4, Mean Test AUC: 0.9343\n",
      "Fold 5 Mean Validation AUC: 0.9400\n",
      "Fold 5 Mean Test AUC: 0.9400\n",
      "--------------------------------------------------------------------------------\n",
      "Final Mean Validation AUC across all folds: 0.9515258104225474\n",
      "Validation AUCs for all folds: [0.9564716637720609, 0.9487697763528102, 0.9604847784324642, 0.9519284464167177, 0.9399743871386839]\n",
      "Final Mean Test AUC across all folds: 0.9515426145271892\n",
      "Test AUCs for all folds: [0.9564830945117251, 0.9487758212348123, 0.9604757489871449, 0.9519970364786623, 0.9399813714236013]\n"
     ]
    }
   ],
   "source": [
    "train_val_test_model(dataset=final_dataset, num_epochs=100, dropout=0.3, lr=1e-5, weight_decay=1e-3,\n",
    "                              criterion=nn.BCEWithLogitsLoss(), batch_size=64, device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd373236-8209-4708-a4ff-9ca6ad40f710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.000049146644058'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = [0.9564830945117251, 0.9487758212348123, 0.9604757489871449, 0.9519970364786623, 0.9399813714236013]\n",
    "f'{np.var(ls):.15f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0dc25-199f-43ba-8373-d0711819a8c0",
   "metadata": {},
   "source": [
    "### Train & Validate the Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa8a8b-2663-4240-9308-bd7add830b46",
   "metadata": {},
   "source": [
    "#### Fold number 1 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fc0ef-be6e-42c4-872a-45070fdf2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1 = generate_model(batch_size=64, dropout=0.3)\n",
    "model_f1.train_val_model('fold1', num_epochs=50, dataset=train_fold1_df,\n",
    "                              optimizer=optim.AdamW(model_f1.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                              criterion=nn.BCEWithLogitsLoss(),\n",
    "                              batch_size=64, device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e36f8-8fb7-4de4-b50f-b033f05bd19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1.test_model(test_fold1_df,\n",
    "                    criterion= nn.BCEWithLogitsLoss() ,batch_size=64,\n",
    "                    device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6350de-2a48-4f7e-8ff8-5c19d1bbe0b7",
   "metadata": {},
   "source": [
    "#### Fold number 2 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32eab7-b219-4911-a13a-455146325bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f2 = generate_model(batch_size=64, dropout=0.3)\n",
    "model_f2.train_val_model('fold2', num_epochs=50, dataset=train_fold2_df,\n",
    "                              optimizer=optim.AdamW(model_f2.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                              criterion=nn.BCEWithLogitsLoss(),                              \n",
    "                              batch_size=64, device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6178d88-b2c8-4573-8e0b-3739ae1fe750",
   "metadata": {},
   "source": [
    "#### Fold number 3 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9364c-404a-4b32-bdba-d7bbf1f7a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f3 = generate_model(batch_size=64, dropout=0.3)\n",
    "model_f3.train_val_model('fold3', num_epochs=50, dataset=train_fold3_df,\n",
    "                              optimizer=optim.AdamW(model_f3.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                              criterion=nn.BCEWithLogitsLoss(),                              \n",
    "                              batch_size=64, device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6468a6-6d67-46b9-bf81-b5ffdaae7f55",
   "metadata": {},
   "source": [
    "#### Folder number 4 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16cb6a-86b8-4700-be5d-02a1cafccf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f4 = generate_model(batch_size=64, dropout=0.3)\n",
    "model_f4.train_val_model('fold4', num_epochs=50, dataset=train_fold4_df,\n",
    "                              optimizer=optim.AdamW(model_f4.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                              criterion=nn.BCEWithLogitsLoss(),                              \n",
    "                              batch_size=64, device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be051e-278d-4bd3-b8e2-d90098292394",
   "metadata": {},
   "source": [
    "#### Folder number 5 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401286d4-425d-4251-bd2d-f3b3c2b0031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f5 = generate_model(batch_size=64, dropout=0.3)\n",
    "model_f5.train_val_model('fold5', num_epochs=50, dataset=train_fold5_df,\n",
    "                              optimizer=optim.AdamW(model_f5.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                              criterion=nn.BCEWithLogitsLoss(),                              \n",
    "                              batch_size=64, device=device, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82a98d-bbd3-480c-9333-2d390f9aab07",
   "metadata": {},
   "source": [
    "### TRAIN - Average AUC on 5 Expirements ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e147b4d-9846-4bfb-bacb-397e9f1eca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_expirements_auc(num_epochs_list, n):\n",
    "    auc_dict = {f'exp{i+1}': [] for i in range(n)}\n",
    "    for exp_num in range(1, 6):\n",
    "        print(f\"Starting Experiment {exp_num}\")\n",
    "        \n",
    "        for fold_num in range(1, 6): \n",
    "            fold_name = f'fold{fold_num}'\n",
    "            train_fold, test_fold = dataframes[f'train_{fold_name}_df'], dataframes[f'test_{fold_name}_df'] \n",
    "            num_epochs = num_epochs_list[fold_num - 1] \n",
    "            model = generate_model(batch_size=64, dropout=0.3)\n",
    "            model.train_model(fold_name, num_epochs=num_epochs, dataset=train_fold,\n",
    "                                          optimizer=optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                                          criterion=nn.BCEWithLogitsLoss(),\n",
    "                                          batch_size=64, device=device, num_workers=16)\n",
    "    \n",
    "            auc = model.test_model(test_fold, \n",
    "                                      criterion=nn.BCEWithLogitsLoss(), batch_size=64, \n",
    "                                      device=device, num_workers=16)\n",
    "    \n",
    "            auc_dict[f'exp{exp_num}'].append(auc)\n",
    "    return auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1ded1-053d-41b1-9906-0207b5d56e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expirements_auc(num_epochs, fold_num):\n",
    "    fold_name = f'fold{fold_num}'\n",
    "    train_fold, test_fold = dataframes[f'train_{fold_name}_df'], dataframes[f'test_{fold_name}_df'] \n",
    "    model = generate_model(batch_size=64, dropout=0.3)\n",
    "    model.train_model(fold_name, num_epochs=num_epochs, dataset=train_fold,\n",
    "                      optimizer=optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-3),\n",
    "                      criterion=nn.BCEWithLogitsLoss(),\n",
    "                      batch_size=64, device=device, num_workers=16)\n",
    "    \n",
    "    model.test_model(test_fold, \n",
    "                     criterion=nn.BCEWithLogitsLoss(), batch_size=64, \n",
    "                     device=device, num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde8981-5a38-4966-a2bc-465c6c04f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/10 - for ablation table - all features\n",
    "class AUVG_PPI(AbstractModel):\n",
    "    def __init__(self, pretrained_chemprop_model, chemberta_model, dropout):\n",
    "        \n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.pretrained_chemprop_model = pretrained_chemprop_model\n",
    "        self.chemberta_model = chemberta_model\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1750, out_features=1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1000, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )        \n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 3, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=2100, out_features=1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=384)\n",
    "        )\n",
    "\n",
    "        # Morgan fingerprints & chemical descriptors MLP layers\n",
    "        self.mfp_cd_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 + 194, out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=384)\n",
    "        )\n",
    "\n",
    "        # MLP for smiles_embeddings\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=384 * 3 , out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=512 + 512, out_features=768),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=768, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=1),\n",
    "        )\n",
    "        \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bmg, esm, fegs, gae,\n",
    "                input_ids, attention_mask,\n",
    "                morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.pretrained_chemprop_model(bmg)\n",
    "        cp_fingerprints = self.fp_mlp(cp_fingerprints)\n",
    "\n",
    "        chemberta_embeddings = self.chemberta_model(input_ids, attention_mask)\n",
    "        #chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp_chem_descriptors = torch.cat([morgan_fingerprints, chemical_descriptors], dim=1)\n",
    "        mfp_chem_descriptors = self.mfp_cd_mlp(mfp_chem_descriptors)\n",
    "        \n",
    "        # Concatenate all 3 smiles embeddings along a new dimension (3x384) \n",
    "        smiles_embeddings = torch.cat([cp_fingerprints,chemberta_embeddings, mfp_chem_descriptors], dim=1).to(device)  # shape ->> (batch_size, 3*384)\n",
    "        smiles_embeddings = self.smiles_mlp(smiles_embeddings)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        #ppi_features = proteins.to(device)\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "\n",
    "        # Concatenate all 3 ppi embeddings along a new dimension (3x512) \n",
    "        ppi_embeddings = torch.cat([esm_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 3*512)\n",
    "        ppi_features = self.ppi_mlp(ppi_embeddings)\n",
    "\n",
    "        combined_embeddings = torch.cat([smiles_embeddings, ppi_features], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec7825-ac30-4e17-9230-25cb3b50f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/10 - for ablation table\n",
    "class AbstractModel(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AbstractModel, self).__init__()\n",
    "        self.early_stopping_patience = 5\n",
    "        self.delta = 0.001\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, bmg, esm, fegs, gae,\n",
    "                input_ids, attention_mask,\n",
    "                morgan_fingerprints, chemical_descriptors):\n",
    "        pass\n",
    "        \n",
    "    def train_model(self, fold, num_epochs, dataset, optimizer, criterion, \n",
    "                    batch_size=32, device='cuda', num_workers=5):\n",
    "        \n",
    "        train_dataset = MoleculeDataset(dataset)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        print(f'Start training {fold} for {num_epochs} epochs !')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_smiles, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mask = batch_attention_mask.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_smiles, batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc)               \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "            print(f\"Epoch {epoch+1} Time: {epoch_time:.2f}\")\n",
    "\n",
    "        \n",
    "    def train_val_model(self, fold, num_epochs, dataset, optimizer, criterion, \n",
    "                    batch_size=32, device='cuda', num_workers=5):\n",
    "        best_val_auc = float('-inf')\n",
    "        no_improve_epochs = 0\n",
    "\n",
    "        X = dataset.copy().drop(columns=['smiles'])\n",
    "        ids = dataset['smiles']\n",
    "        # extract labels for scaffold split (in order to make sure we got balance train & validation sets)\n",
    "        labels = dataset['label'].values\n",
    "        dc_dataset = dc.data.DiskDataset.from_numpy(X=X ,y=labels ,w=np.zeros(len(X)),ids=ids)\n",
    "        splitter = dc.splits.ScaffoldSplitter()\n",
    "        train_idx, val_idx, _ = splitter.split(dc_dataset, frac_train=0.8, frac_valid=0.2, frac_test=0)\n",
    "\n",
    "        #train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, stratify=labels, shuffle=shuffle)\n",
    "        \n",
    "        train_subset = dataset.iloc[train_idx].reset_index(drop=True)\n",
    "        val_subset = dataset.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = MoleculeDataset(train_subset)\n",
    "        val_dataset = MoleculeDataset(val_subset)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_smiles, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mask = batch_attention_mask.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_smiles, batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc)                 \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.5f}, \"\n",
    "                  f\"Validation Accuracy: {val_accuracy:.2f}, Validation AUC: {val_auc:.5f}, Epoch Time: {epoch_time:.2f}\")\n",
    "            # Check whether val_auc > best_val_auc + delta\n",
    "            if val_auc > best_val_auc + self.delta:\n",
    "                best_val_auc = val_auc\n",
    "                train_epoch = epoch+1\n",
    "                no_improve_epochs = 0 \n",
    "                print(f\"Current best val_auc -> {val_auc:.5f}, at epoch {epoch+1}\")\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= self.early_stopping_patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        print(f'Train the model for -> {train_epoch}, best validation auc: {best_val_auc:.5f}')\n",
    "                \n",
    "    def test_model(self, test_dataset, criterion, batch_size, device, num_workers):\n",
    "        test_dataset = MoleculeDataset(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        self.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc ,batch_labels) in test_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mask = batch_attention_mask.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                outputs = self(batch_smiles, batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc)                 \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        test_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        PRINTC()\n",
    "        print(f\"Test AUC: {test_auc:.4f}\")\n",
    "        PRINTC()\n",
    "        return round(test_auc, 4)\n",
    "\n",
    "    def validate_model(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mask = batch_attention_mask.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                 \n",
    "                outputs = self(batch_smiles, batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_input_ids, batch_attention_mask, batch_morgan, batch_chem_desc)                \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        return val_loss, accuracy, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c7f8c-542d-487b-b42d-bc48b109aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/10 - for ablation table - all features\n",
    "class AUVG_PPI(AbstractModel):\n",
    "    def __init__(self ,dropout):\n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1750, out_features=1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1000, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )        \n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 3, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1200, out_features=1024), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        # Morgan fingerprints & chemical descriptors MLP layers\n",
    "        self.mfp_cd_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 + 194, out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        self.chemberta_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=384, out_features= 256)\n",
    "        )\n",
    "\n",
    "        # MLP for smiles_embeddings\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 3 , out_features= 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256)\n",
    "        )\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=256 + 256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=1),\n",
    "        )\n",
    "        \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, cpe, esm, fegs, gae, cbae, morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.fp_mlp(cpe)\n",
    "        cbae = self.chemberta_mlp(cbae)\n",
    "        #chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp_chem_descriptors = torch.cat([morgan_fingerprints, chemical_descriptors], dim=1)\n",
    "        mfp_chem_descriptors = self.mfp_cd_mlp(mfp_chem_descriptors)\n",
    "        \n",
    "        # Concatenate all 3 smiles embeddings along a new dimension (3x384) \n",
    "        smiles_embeddings = torch.cat([cp_fingerprints, cbae, mfp_chem_descriptors], dim=1).to(device)  # shape ->> (batch_size, 3*384)\n",
    "        smiles_embeddings = self.smiles_mlp(smiles_embeddings)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        #ppi_features = proteins.to(device)\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "\n",
    "        # Concatenate all 3 ppi embeddings along a new dimension (3x512) \n",
    "        ppi_embeddings = torch.cat([esm_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 3*512)\n",
    "        ppi_features = self.ppi_mlp(ppi_embeddings)\n",
    "\n",
    "        combined_embeddings = torch.cat([smiles_embeddings, ppi_features], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3bb354-604d-4eb4-b642-6ab04b515f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/10 - for ablation table\n",
    "class AbstractModel(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AbstractModel, self).__init__()\n",
    "        self.early_stopping_patience = 5\n",
    "        self.delta = 0.001\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, cpe, esm, fegs, gae, cbae, morgan_fingerprints, chemical_descriptors):\n",
    "        pass\n",
    "        \n",
    "    def train_model(self, fold, num_epochs, dataset, optimizer, criterion, \n",
    "                    batch_size=32, device='cuda', num_workers=5):\n",
    "        \n",
    "        train_dataset = MoleculeDataset(dataset)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        print(f'Start training {fold} for {num_epochs} epochs !')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "            print(f\"Epoch {epoch+1} Time: {epoch_time:.2f}\")\n",
    "\n",
    "        \n",
    "    def train_val_model(self, fold, num_epochs, dataset, optimizer, criterion, \n",
    "                    batch_size=32, device='cuda', num_workers=5):\n",
    "        best_val_auc = float('-inf')\n",
    "        no_improve_epochs = 0\n",
    "\n",
    "        X = dataset.copy().drop(columns=['smiles'])\n",
    "        ids = dataset['smiles']\n",
    "        # extract labels for scaffold split (in order to make sure we got balance train & validation sets)\n",
    "        labels = dataset['label'].values\n",
    "        dc_dataset = dc.data.DiskDataset.from_numpy(X=X ,y=labels ,w=np.zeros(len(X)),ids=ids)\n",
    "        splitter = dc.splits.ScaffoldSplitter()\n",
    "        train_idx, val_idx, _ = splitter.split(dc_dataset, frac_train=0.8, frac_valid=0.2, frac_test=0)\n",
    "\n",
    "        #train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, stratify=labels, shuffle=shuffle)\n",
    "        \n",
    "        train_subset = dataset.iloc[train_idx].reset_index(drop=True)\n",
    "        val_subset = dataset.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = MoleculeDataset(train_subset)\n",
    "        val_dataset = MoleculeDataset(val_subset)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in train_loader:\n",
    "                \n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)                 \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                running_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "\n",
    "            print(f\"Epoch {epoch+1} - Validation Loss: {val_loss:.5f}, \"\n",
    "                  f\"Validation Accuracy: {val_accuracy:.2f}, Validation AUC: {val_auc:.5f}, Epoch Time: {epoch_time:.2f}\")\n",
    "            # Check whether val_auc > best_val_auc + delta\n",
    "            if val_auc > best_val_auc + self.delta:\n",
    "                best_val_auc = val_auc\n",
    "                train_epoch = epoch+1\n",
    "                no_improve_epochs = 0 \n",
    "                print(f\"Current best val_auc -> {val_auc:.5f}, at epoch {epoch+1}\")\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "                if no_improve_epochs >= self.early_stopping_patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        print(f'Train the model for -> {train_epoch}, best validation auc: {best_val_auc:.5f}')\n",
    "                \n",
    "    def test_model(self, test_dataset, criterion, batch_size, device, num_workers):\n",
    "        test_dataset = MoleculeDataset(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        self.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in test_loader:\n",
    "                \n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)               \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        test_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        PRINTC()\n",
    "        print(f\"Test AUC: {test_auc:.4f}\")\n",
    "        PRINTC()\n",
    "        return round(test_auc, 4)\n",
    "\n",
    "    def validate_model(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (batch_chemprop_features, batch_esm_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_chemberta_features, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "\n",
    "                batch_chemprop_features = batch_chemprop_features.to(device)\n",
    "                batch_chemberta_features = batch_chemberta_features.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                outputs = self(batch_chemprop_features , batch_esm_features, batch_fegs_features,\n",
    "                               batch_gae_features, batch_chemberta_features, batch_morgan, batch_chem_desc)                \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        return val_loss, accuracy, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca60b7-0a67-4603-b17b-818da1dbefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ablation table 17/10\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, ds_):\n",
    "        self.data = ds_\n",
    "        self.mapping_df = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "        self.esm = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'esm_features.csv'))\n",
    "        self.fegs = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'fegs_features.csv'))\n",
    "\n",
    "        # In all predicted values, use zero vectors (after expirements that proved that)\n",
    "        gae_path = f'GAE_FEATURES_WITH_PREDICTED_alpha_0.csv'            \n",
    "        self.gae = pd.read_csv(os.path.join('datasets', 'GAE', gae_path))\n",
    "        self.gae.loc[self.gae['predicted'] == 1, self.gae.columns[9:509]] = 0\n",
    "        gae_features_columns = self.gae.iloc[:, 9:509]\n",
    "\n",
    "        gae_uniprot_column = self.gae[['From']].rename(columns={'From': 'UniProt_ID'})\n",
    "        self.gae = pd.concat([gae_uniprot_column, gae_features_columns], axis=1)\n",
    "        self.gae_features_ppi = self.merge_datasets(self.data, self.gae).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.esm_features_ppi = self.merge_datasets(self.data, self.esm).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.fegs_features_ppi = self.merge_datasets(self.data, self.fegs).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "\n",
    "         # SMILES RDKit features - Morgan Fingerprints (r=4, nbits=1024)  chemical descriptors, chemprop & chemBERTa\n",
    "        self.smiles_morgan_fingerprints = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_morgan_fingerprints_dataset.csv'))\n",
    "        self.smiles_chemical_descriptors = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_chem_descriptors_mapping_dataset.csv'))\n",
    "        self.chemprop = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'chemprop_features.csv'))\n",
    "        self.chemberta = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'chemBERTa_features.csv'))\n",
    "        # Necessary features for ChemBERTa model\n",
    "        self.smiles_list = self.data['smiles'].tolist()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "        self.encoded_smiles = self.tokenizer(self.smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def merge_datasets(self, dataset, features_df):\n",
    "        dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID'])\n",
    "        \n",
    "        features_df_renamed = features_df.add_suffix('_id2')\n",
    "        features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
    "        dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID', 'uniprot_id1', 'uniprot_id2'])\n",
    "        \n",
    "        # In order to avoid dropping duplicated rows that holds only zeros (in gae when there is zero vectors), which can be represents embeddings of ppi vector when\n",
    "        # specifying to reset the rows to hold only zeros\n",
    "        dataset['zero_count'] = (dataset == 0).any(axis=1).astype(int)\n",
    "        count = 1\n",
    "        for index in dataset.index:\n",
    "            if dataset.at[index, 'zero_count'] == 1:\n",
    "                dataset.at[index, 'zero_count'] = count\n",
    "                count += 1\n",
    "                \n",
    "        # Fill null values with 0\n",
    "        dataset.fillna(0, inplace=True)\n",
    "        dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "        return dataset.drop(columns=['zero_count'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx, 0]\n",
    "        label = np.array(self.data.iloc[idx, -1], dtype=np.float32)  \n",
    "        esm_features = np.array(self.esm_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        fegs_features = np.array(self.fegs_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        gae_features = np.array(self.gae_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "\n",
    "        input_ids = self.encoded_smiles[\"input_ids\"][idx]\n",
    "        attention_mask = self.encoded_smiles[\"attention_mask\"][idx]\n",
    "\n",
    "        # Retrieve precomputed RDKit Morgan fingerprints\n",
    "        morgan_fingerprint = self.smiles_morgan_fingerprints.loc[self.smiles_morgan_fingerprints['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemical_descriptors = self.smiles_chemical_descriptors.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemprop_features = self.chemprop.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemberta_features = self.chemberta.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        \n",
    "        return (chemprop_features, esm_features, fegs_features, gae_features, \n",
    "                chemberta_features, morgan_fingerprint, chemical_descriptors, label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop v2",
   "language": "python",
   "name": "chemprop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
