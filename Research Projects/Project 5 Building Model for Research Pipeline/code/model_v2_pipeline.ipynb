{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12a9e9c-b901-4ed0-965a-c2dc4f3f837e",
   "metadata": {},
   "source": [
    "# Build Research Final Model Pipeline #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcec6a4-5f75-4ba0-860f-a42b21a6cb6a",
   "metadata": {},
   "source": [
    "## Import Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8154c685-2c72-416a-99c2-c0405da05707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import chemprop\n",
    "from chemprop import data, featurizers, models\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "# import necessary libraries for Chemberta model\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, AdamW, get_linear_schedule_with_warmup , BertModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Draw\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f205d5-fc7f-4041-98fe-d733db10df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRINT() -> None: print(f\"{'-'*80}\\nDone\\n{'-'*80}\")\n",
    "def PRINTC() -> None: print(f\"{'-'*80}\")\n",
    "def PRINTM(M) -> None: print(f\"{'-'*80}\\n{M}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292be2e8-c26d-47c5-875e-ec1d8b8a42b1",
   "metadata": {},
   "source": [
    "## Verify GPU Availability ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca0e2bd-ebdf-469d-b3a3-2719d508ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 14 08:50:43 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:21:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             26W /  300W |       1MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b9a93-d655-4fc5-8113-9e492c4d34d5",
   "metadata": {},
   "source": [
    "For this task, we'll use the BGU cluster GPU `NVIDIA RTX 6000 Ada Generation` to achieve better performance during the training of our pre-trained and fine-tuned models, allowing for more efficient processing of large datasets and complex computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94354f56-7aa2-4105-a2e0-8a0a8efc3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "GPU is available.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    PRINTM(f\"GPU is available.\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    PRINTM(f\"GPU is not available. Using CPU instead.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953e7dc3-c267-4368-b488-d467d19ac35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch version: 2.3.1+cu121\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "CUDA available: True\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "CUDA version:  12.1\n",
      "--------------------------------------------------------------------------------\n",
      "CUDA device: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "PRINTM(f\"PyTorch version: {torch.__version__}\")\n",
    "PRINTM(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "PRINTM(f\"CUDA version:  {torch.version.cuda}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No CUDA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e842c9-e3ed-4116-911e-16823263fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "uniprot_mapping = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "PRINT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14d951c-acb6-4bdb-ad69-1c16c768cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_uniprot_ids(dataset, mapping_df):\n",
    "    # Create a dictionary from the mapping dataframe\n",
    "    mapping_dict = mapping_df.set_index('From')['Entry'].to_dict()\n",
    "\n",
    "    # Map the uniprot_id1 and uniprot_id2 columns to their respective Entry values\n",
    "    dataset['uniprot_id1'] = dataset['uniprot_id1'].map(mapping_dict)\n",
    "    dataset['uniprot_id2'] = dataset['uniprot_id2'].map(mapping_dict)\n",
    "    return dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6afa3216-ac3d-4af1-b53d-a7c9357f9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemBERTaPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChemBERTaPT, self).__init__()\n",
    "        self.model_name = \"DeepChem/ChemBERTa-77M-MTR\"\n",
    "        self.chemberta = RobertaModel.from_pretrained(self.model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.chemberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return bert_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061d1ac9-545e-4bce-9244-799ce5390949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedChempropModel(nn.Module):\n",
    "    def __init__(self, checkpoints_path, batch_size):\n",
    "        super(PretrainedChempropModel, self).__init__()\n",
    "        self.mpnn = self.load_pretrained_model(checkpoints_path)\n",
    "        self.featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, smiles):\n",
    "        # Prepare the data in order to generate embeddings from modulators SMILES\n",
    "        self.smiles_data = [data.MoleculeDatapoint.from_smi(smi) for smi in smiles]\n",
    "        self.smiles_dset = data.MoleculeDataset(self.smiles_data, featurizer=self.featurizer)\n",
    "        self.smiles_loader = data.build_dataloader(self.smiles_dset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        embeddings = [\n",
    "            # Etract the embedding from the last FFN layer, i.e., before the final prediction(thus i=-1)\n",
    "            self.mpnn.predictor.encode(self.fingerprints_from_batch_molecular_graph(batch, self.mpnn), i=-1) \n",
    "            for batch in self.smiles_loader\n",
    "        ]\n",
    "        #print(embeddings)\n",
    "        if not embeddings:\n",
    "             return torch.empty(0, device=device)\n",
    "        embeddings = torch.cat(embeddings, 0).to(device)\n",
    "        return embeddings\n",
    "\n",
    "    def fingerprints_from_batch_molecular_graph(self, batch, mpnn):\n",
    "        batch.bmg.to(device)\n",
    "        H_v = mpnn.message_passing(batch.bmg, batch.V_d)\n",
    "        H = mpnn.agg(H_v, batch.bmg.batch)\n",
    "        H = mpnn.bn(H)\n",
    "        fingerprints = H if batch.X_d is None else torch.cat((H, mpnn.batch.X_d_transform(X_d)), 1)\n",
    "        return fingerprints\n",
    "\n",
    "    def load_pretrained_model(self, checkpoints_path):\n",
    "        mpnn = models.MPNN.load_from_checkpoint(checkpoints_path).to(device)\n",
    "        return mpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e61b12-de23-447f-b8df-f15083d826f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_with_uniprots_order_switchings(df):\n",
    "    # generate a copy of the DataFrame with swapped uniprot_id1 and uniprot_id2\n",
    "    swapped_df = df.copy()\n",
    "    swapped_df[['uniprot_id1', 'uniprot_id2']] = swapped_df[['uniprot_id2', 'uniprot_id1']]\n",
    "\n",
    "    # concatenate the original and swapped DataFrames & drop duplicated samples\n",
    "    combined_df = pd.concat([df, swapped_df])\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab5b0f-147b-44ee-9827-410d87974bc4",
   "metadata": {},
   "source": [
    "# Models v2 (with Attention) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb9a136-2a84-472f-8022-dd98a59e889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModel(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "    \tsuper(AbstractModel, self).__init__()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, bmg, bpsf1, bpsf2, esm, custom, fegs, gae,\n",
    "    \tinput_ids, attention_mask,\n",
    "    \tmorgan_fingerprints, chemical_descriptors):\n",
    "    \tpass\n",
    "\n",
    "    def train_model(self, num_epochs, train_loader, val_loader, optimizer, criterion, device):\n",
    "        PRINTM(f'Start training !')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_smiles, batch_psf1, batch_psf2, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc, batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_psf1 = batch_psf1.to(device)\n",
    "                batch_psf2 = batch_psf2.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_smiles, batch_psf1, batch_psf2, batch_esm_features,batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas,batch_morgan, batch_chem_desc)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "            # Validate the model on the validation set\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "            PRINTC()\n",
    "            print(f\"Epoch: {epoch+1}\")\n",
    "            print(f\"Validation BCEWithLogitsLoss: {val_loss:.5f}\")\n",
    "            print(f\"Validation Accuracy (>0.8): {val_accuracy:.2f}\")\n",
    "            print(f\"Validation AUC: {val_auc:.5f}\")\n",
    "            print(f\"Epoch time: {epoch_time:.2f} minutes\")\n",
    "            PRINTC()\n",
    "    \n",
    "        print(\"Finish training !\")\n",
    "\n",
    "    def test_model(self, test_dataset, criterion, batch_size, shuffle, device):\n",
    "        test_dataset = MoleculeDataset(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=test_dataset.collate_fn)\n",
    "        self.eval()\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, batch_psf1, batch_psf2, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc, batch_labels) in test_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_psf1 = batch_psf1.to(device)\n",
    "                batch_psf2 = batch_psf2.to(device)                \n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "    \n",
    "                outputs = self(batch_smiles, batch_psf1, batch_psf2, batch_esm_features, batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc)\n",
    "    \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())  \n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())  \n",
    "    \n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "    \n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        test_auc = roc_auc_score(all_labels, all_outputs) \n",
    "        PRINTC() \n",
    "        print(f\"Test AUC: {test_auc:.5f}\")\n",
    "        PRINTC()\n",
    "        return test_auc\n",
    "\n",
    "    def validate_model(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, batch_psf1, batch_psf2, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_psf1 = batch_psf1.to(device)\n",
    "                batch_psf2 = batch_psf2.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "    \n",
    "                outputs = self(batch_smiles, batch_psf1, batch_psf2, batch_esm_features,batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                all_labels.extend(batch_labels.cpu().numpy())  \n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())  \n",
    "    \n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_auc = roc_auc_score(all_labels, all_outputs)  \n",
    "        return val_loss, accuracy, val_auc\n",
    "\n",
    "    def cross_validate(self, dataset, num_folds=5,num_epochs=10, batch_size=32, learning_rate=0.0001, weight_decay=1e-5, shuffle=True, device='cuda'):\n",
    "        kf = KFold(n_splits=num_folds, shuffle=shuffle)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            \n",
    "            print(f\"Fold {fold+1}/{num_folds}\")\n",
    "            \n",
    "            # Split dataset\n",
    "            train_subset = dataset.iloc[train_idx].reset_index(drop=True)\n",
    "            val_subset = dataset.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "            train_dataset = MoleculeDataset(train_subset)\n",
    "            val_dataset = MoleculeDataset(val_subset)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=train_dataset.collate_fn)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=val_dataset.collate_fn)\n",
    "            \n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            \n",
    "            self.train_model(num_epochs, train_loader, val_loader, optimizer, criterion, device)\n",
    "            \n",
    "            # Validate the model\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            fold_results.append((val_loss, val_accuracy, val_auc))\n",
    "\n",
    "            PRINTC()\n",
    "            print(f\"Fold {fold+1} - Validation BCEWithLogitsLoss: {val_loss:.5f}, Accuracy: {val_accuracy:.2f}, AUC: {val_auc:.5f}\")\n",
    "            PRINTC()\n",
    "            \n",
    "        avg_val_loss = sum([result[0] for result in fold_results]) / num_folds\n",
    "        avg_val_accuracy = sum([result[1] for result in fold_results]) / num_folds\n",
    "        avg_val_auc = sum([result[2] for result in fold_results]) / num_folds\n",
    "        \n",
    "        print(f\"\\nAverage Validation BCEWithLogitsLoss: {avg_val_loss:.5f}\")\n",
    "        print(f\"Average Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "        print(f\"Average Validation AUC: {avg_val_auc:.5f}\")\n",
    "        \n",
    "        return fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d06e36c-668c-49fb-a5c6-b6f08be079e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_self_attention(nn.Module):\n",
    "    def __init__(self, embed_dim_, num_heads_, dropout_):\n",
    "        super(custom_self_attention, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=embed_dim_, num_heads=num_heads_, dropout= dropout_)\n",
    "        self.norm_layer = nn.LayerNorm(embed_dim_)\n",
    "\n",
    "    def forward(self, embeddings_mat):\n",
    "        # Apply self-attention for PPI\n",
    "        embeddings_mat = embeddings_mat.permute(1, 0, 2)  # Change to (num_heads, batch_size, embed_dim) for MultiheadAttention\n",
    "        attn_output, attn_weights = self.self_attention(embeddings_mat, embeddings_mat, embeddings_mat)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  # shape ->> (batch_size, num_heads, embed_dim)\n",
    "\n",
    "        # Add & Norm\n",
    "        embeddings_mat = embeddings_mat.permute(1, 0, 2)  # Back to original shape for residual connection\n",
    "        attn_output = (0.5*attn_output) + (0.5*embeddings_mat)  # Add (residual connection) & apply weighted residual connection \n",
    "        attn_output = self.norm_layer(attn_output)  # Apply LayerNorm\n",
    "\n",
    "        # Flatten the output for the next MLP layer\n",
    "        embeddings_mat = attn_output.flatten(start_dim=1)  # Shape: (batch_size, num_heads*embed_dim)\n",
    "        \n",
    "        return embeddings_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46693891-845c-4434-a09b-e02c21e517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Attention (first version) - without structure features\n",
    "class AUVG_PPI(AbstractModel):\n",
    "    def __init__(self, pretrained_chemprop_model, chemberta_model, dropout):\n",
    "        \n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.pretrained_chemprop_model = pretrained_chemprop_model\n",
    "        self.chemberta_model = chemberta_model\n",
    "        self.dropout = dropout\n",
    "        self.ppi_self_attention = custom_self_attention(512, 8, 0.2)\n",
    "        self.smiles_self_attention = custom_self_attention(384, 4, 0.2)\n",
    "        self.cross_attention = nn.MultiheadAttention(512, 8, 0.2)\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1750, out_features=1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1000, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )        \n",
    "\n",
    "        self.custom_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=4700 + 4700 , out_features=8000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=8000, out_features=6500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(6500),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=6500, out_features=5000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(5000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=5000, out_features=3500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(3500),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=3500, out_features=2000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=2000, out_features=1028),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1028),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1028, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 4 , out_features= 1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=2100, out_features=1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=384)\n",
    "        )\n",
    "\n",
    "        # Morgan fingerprints & chemical descriptors MLP layers\n",
    "        self.mfp_cd_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 + 194, out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=384)\n",
    "        )\n",
    "\n",
    "        # MLP for smiles_embeddings\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=384 * 3 , out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=256 + 256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "        \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bmg, esm, custom, fegs, gae,\n",
    "                input_ids, attention_mask,\n",
    "                morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.pretrained_chemprop_model(bmg)\n",
    "        cp_fingerprints = self.fp_mlp(cp_fingerprints)\n",
    "\n",
    "        chemberta_embeddings = self.chemberta_model(input_ids, attention_mask)\n",
    "        #chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp_chem_descriptors = torch.cat([morgan_fingerprints, chemical_descriptors], dim=1)\n",
    "        mfp_chem_descriptors = self.mfp_cd_mlp(mfp_chem_descriptors)\n",
    "        \n",
    "        # Concatenate all 3 smiles embeddings along a new dimension (3x384) & pass them throw self-attention layer\n",
    "        smiles_embeddings = torch.stack([cp_fingerprints, chemberta_embeddings, mfp_chem_descriptors], dim=1).to(device)  # shape ->> (batch_size, 3, 384)\n",
    "        smiles_features = self.smiles_self_attention(smiles_embeddings)\n",
    "        smiles_embeddings = self.smiles_mlp(smiles_features).unsqueeze(1)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        #ppi_features = proteins.to(device)\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        custom_embeddings = self.custom_mlp(custom)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "\n",
    "        # Concatenate all 4 ppi embeddings along a new dimension (4x512) & pass them throw self-attention layer\n",
    "        ppi_embeddings = torch.stack([esm_embeddings, custom_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 4, 320)\n",
    "        ppi_features = self.ppi_self_attention(ppi_embeddings)\n",
    "        ppi_features = self.ppi_mlp(ppi_features).unsqueeze(1)\n",
    "\n",
    "        #Cross-attention between smiles and PPI to capture the interaction relationships\n",
    "        ppi_QKV = ppi_features.permute(1, 0, 2)\n",
    "        smiles_QKV = smiles_embeddings.permute(1, 0, 2)\n",
    "        \n",
    "        smiles_att, _ = self.cross_attention(smiles_QKV, ppi_QKV, ppi_QKV)\n",
    "        ppi_att, _ = self.cross_attention(ppi_QKV, smiles_QKV, smiles_QKV)\n",
    "\n",
    "        # permute attention outputrs to match (batch_size, embed_dim, num_heads) shape\n",
    "        smiles_attn_output = (0.5* smiles_att.permute(1, 2, 0)) + (0.5* smiles_embeddings.permute(0, 2, 1))  # Add (residual connection) & apply weighted residual connection \n",
    "        ppi_attn_output = (0.5* ppi_att.permute(1, 2, 0)) + (0.5* ppi_features.permute(0, 2, 1))  # Add (residual connection) & apply weighted residual connection \n",
    "\n",
    "        # Drop the last dim in order to get (batch_size, embed_dim) & \n",
    "        # Pass cross-attention norm outputs throw max-pool layer before passing throw MLP layers\n",
    "        smiles_att = self.max_pool(smiles_attn_output.squeeze(2))\n",
    "        ppi_att = self.max_pool(ppi_attn_output.squeeze(2)) \n",
    "        combined_embeddings = torch.cat([smiles_att, ppi_att], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return output\n",
    "        #return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43be6800-b1da-4073-960b-0c9deda99b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureReducer_(nn.Module):\n",
    "    # Feature reducer for joint attention in PPI structure feature - in order to reduce tensors dim for math operations\n",
    "    # Use this class if |UniProt_NumOfAminoAcidComp| < 128\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FeatureReducer_, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, in_channels]\n",
    "        x = x.transpose(1, 2)  # Change shape to [batch_size, in_channels, sequence_length]\n",
    "        x = self.conv(x)       \n",
    "        x = x.transpose(1, 2)  # Change shape back to [batch_size, target_length, out_channels]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d65601-5b39-4363-80ec-1ee31151769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureReducer(nn.Module):\n",
    "    # Feature reducer for joint attention in PPI structure feature - in order to reduce tensors dim for math operations\n",
    "    # Use this class if |UniProt_NumOfAminoAcidComp| >= 128\n",
    "    def __init__(self, in_channels, out_channels, target_length):\n",
    "        super(FeatureReducer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(target_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, in_channels]\n",
    "        x = x.transpose(1, 2)  # Change shape to [batch_size, in_channels, sequence_length]\n",
    "        x = self.conv(x)    \n",
    "        x = self.pool(x) \n",
    "        x = x.transpose(1, 2)  # Change shape back to [batch_size, target_length, out_channels]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529ab83f-7f7b-4eb2-b786-24d19dd76c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Attention (second version) - wthi structure features\n",
    "class AUVG_PPI(AbstractModel):\n",
    "    def __init__(self, pretrained_chemprop_model, chemberta_model, dropout):\n",
    "        \n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.pretrained_chemprop_model = pretrained_chemprop_model\n",
    "        self.chemberta_model = chemberta_model\n",
    "        self.dropout = dropout\n",
    "        self.ppi_self_attention = custom_self_attention(512, 8, 0.2)\n",
    "        self.smiles_self_attention = custom_self_attention(384, 4, 0.2)\n",
    "        self.cross_attention = nn.MultiheadAttention(512, 8, 0.2)\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        self.compound_dim = 512\n",
    "        self.W_p1, self.W_p2 = nn.Linear(self.compound_dim, self.compound_dim), nn.Linear(self.compound_dim, self.compound_dim)\n",
    "\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1750, out_features=1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1000, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )        \n",
    "\n",
    "        self.custom_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=4700 + 4700 , out_features=8000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=8000, out_features=6500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(6500),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=6500, out_features=5000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(5000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=5000, out_features=3500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(3500),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=3500, out_features=2000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=2000, out_features=1028),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1028),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1028, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 5 , out_features= 1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=2100, out_features=1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=384)\n",
    "        )\n",
    "\n",
    "        # Morgan fingerprints & chemical descriptors MLP layers\n",
    "        self.mfp_cd_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 + 194, out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=384)\n",
    "        )\n",
    "\n",
    "        # MLP for smiles_embeddings\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=384 * 3 , out_features= 750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=256 + 256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "    # bptfs -> batch protein tuple feature structure\n",
    "    def forward(self, bmg, bpsf1, bpsf2, esm, custom, fegs, gae,\n",
    "                input_ids, attention_mask,\n",
    "                morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.pretrained_chemprop_model(bmg)\n",
    "        cp_fingerprints = self.fp_mlp(cp_fingerprints)\n",
    "\n",
    "        chemberta_embeddings = self.chemberta_model(input_ids, attention_mask)\n",
    "        #chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp_chem_descriptors = torch.cat([morgan_fingerprints, chemical_descriptors], dim=1)\n",
    "        mfp_chem_descriptors = self.mfp_cd_mlp(mfp_chem_descriptors)\n",
    "        \n",
    "        # Concatenate all 3 smiles embeddings along a new dimension (3x384) & pass them throw self-attention layer\n",
    "        smiles_embeddings = torch.stack([cp_fingerprints, chemberta_embeddings, mfp_chem_descriptors], dim=1).to(device)  # shape ->> (batch_size, 3, 384)\n",
    "        smiles_features = self.smiles_self_attention(smiles_embeddings)\n",
    "        smiles_embeddings = self.smiles_mlp(smiles_features).unsqueeze(1)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        custom_embeddings = self.custom_mlp(custom)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "        \n",
    "        # Structure features\n",
    "        if bpsf1.shape[1] > 128: feature_reducer_p1 = FeatureReducer(in_channels=722, out_channels=512, target_length=128).to(device)\n",
    "        else: feature_reducer_p1 = FeatureReducer_(in_channels=722, out_channels=512).to(device)\n",
    "        if bpsf2.shape[1] > 128: feature_reducer_p2 = FeatureReducer(in_channels=722, out_channels=512, target_length=128).to(device)\n",
    "        else: feature_reducer_p2 = FeatureReducer_(in_channels=722, out_channels=512).to(device)\n",
    "        bpsf1 = feature_reducer_p1(bpsf1)\n",
    "        bpsf2 = feature_reducer_p2(bpsf2)\n",
    "        #print(f'bpsf1 -> {bpsf1.shape}, bpsf2 -> {bpsf2.shape}')\n",
    "        inter_comp_prot = self.sigmoid(torch.einsum('bij,bkj->bik', self.W_p1(self.relu(bpsf1)), self.W_p2(self.relu(bpsf2))))\n",
    "        #print(f'inter_comp_prot -> {inter_comp_prot.shape}')\n",
    "        inter_comp_prot_sum = torch.einsum('bij->b', inter_comp_prot)\n",
    "        inter_comp_prot = torch.einsum('bij,b->bij', inter_comp_prot, 1/inter_comp_prot_sum)\n",
    "        #print(f'after, inter_comp_prot -> {inter_comp_prot.shape}')\n",
    "        \n",
    "        # compound-protein joint embedding\n",
    "        cp_embedding = self.tanh(torch.einsum('bij,bkj->bikj', bpsf1, bpsf2))\n",
    "        #print(cp_embedding.shape)\n",
    "        cp_embedding = torch.einsum('bijk,bij->bk', cp_embedding, inter_comp_prot)\n",
    "        #print(f'end, cp_embedding -> {cp_embedding.shape}')\n",
    "        \n",
    "        # Concatenate all 4 ppi embeddings along a new dimension (4x512) & pass them throw self-attention layer\n",
    "        ppi_embeddings = torch.stack([cp_embedding, esm_embeddings, custom_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 4, 320)\n",
    "        ppi_features = self.ppi_self_attention(ppi_embeddings)\n",
    "        ppi_features = self.ppi_mlp(ppi_features).unsqueeze(1)\n",
    "\n",
    "        #Cross-attention between smiles and PPI to capture the interaction relationships\n",
    "        ppi_QKV = ppi_features.permute(1, 0, 2)\n",
    "        smiles_QKV = smiles_embeddings.permute(1, 0, 2)\n",
    "        \n",
    "        smiles_att, _ = self.cross_attention(smiles_QKV, ppi_QKV, ppi_QKV)\n",
    "        ppi_att, _ = self.cross_attention(ppi_QKV, smiles_QKV, smiles_QKV)\n",
    "\n",
    "        # permute attention outputrs to match (batch_size, embed_dim, num_heads) shape\n",
    "        smiles_attn_output = (0.5* smiles_att.permute(1, 2, 0)) + (0.5* smiles_embeddings.permute(0, 2, 1))  # Add (residual connection) & apply weighted residual connection \n",
    "        ppi_attn_output = (0.5* ppi_att.permute(1, 2, 0)) + (0.5* ppi_features.permute(0, 2, 1))  # Add (residual connection) & apply weighted residual connection \n",
    "\n",
    "        # Drop the last dim in order to get (batch_size, embed_dim) & \n",
    "        # Pass cross-attention norm outputs throw max-pool layer before passing throw MLP layers\n",
    "        smiles_att = self.max_pool(smiles_attn_output.squeeze(2))\n",
    "        ppi_att = self.max_pool(ppi_attn_output.squeeze(2)) \n",
    "        combined_embeddings = torch.cat([smiles_att, ppi_att], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return output\n",
    "        #return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53864757-c71f-4d5b-8eb2-e1575a0d8a60",
   "metadata": {},
   "source": [
    "## MoleculeDatasets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c39209-cd24-42d8-83d7-0a278dcd6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training with prob\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, ds_):\n",
    "        # Initialize data and load other features\n",
    "        self.data = ds_\n",
    "        self.mapping_df = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "        self.esm = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'esm_features.csv'))\n",
    "        self.custom = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'custom_features.csv'))\n",
    "        self.fegs = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'fegs_features.csv'))\n",
    "        self.uniprots = self.data.drop(columns=['smiles', 'label'])\n",
    "\n",
    "        gae_path = f'GAE_FEATURES_WITH_PREDICTED_alpha_0.25.csv'\n",
    "        self.gae = pd.read_csv(os.path.join('datasets', 'GAE', gae_path))\n",
    "        gae_features_columns = self.gae.iloc[:, 9:509]\n",
    "        gae_uniprot_column = self.gae[['From']].rename(columns={'From': 'UniProt_ID'})\n",
    "        self.gae = pd.concat([gae_uniprot_column, gae_features_columns], axis=1)\n",
    "        \n",
    "        self.uniprots = self.data.drop(columns=['smiles', 'label'])\n",
    "\n",
    "        # Merge datasets\n",
    "        self.esm_features_ppi = self.merge_datasets(self.data, self.esm).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.custom_features_ppi = self.merge_datasets(self.data, self.custom).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.fegs_features_ppi = self.merge_datasets(self.data, self.fegs).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.gae_features_ppi = self.merge_datasets(self.data, self.gae).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "\n",
    "        # SMILES RDKit features - Morgan Fingerprints (r=4, nbits=1024) & chemical descriptors\n",
    "        self.smiles_morgan_fingerprints = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_morgan_fingerprints_dataset.csv'))\n",
    "        self.smiles_chemical_descriptors = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_chem_descriptors_mapping_dataset.csv'))\n",
    "\n",
    "        # Necessary features for ChemBERTa model\n",
    "        self.smiles_list = self.data['smiles'].tolist()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "        self.encoded_smiles = self.tokenizer(self.smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def merge_datasets(self, dataset, features_df):\n",
    "        # Existing merging logic\n",
    "        dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID'])\n",
    "        \n",
    "        features_df_renamed = features_df.add_suffix('_id2')\n",
    "        features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
    "        dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID', 'uniprot_id1', 'uniprot_id2'])\n",
    "        \n",
    "        return dataset.drop_duplicates()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx, 0]\n",
    "        label = np.array(self.data.iloc[idx, -1], dtype=np.float32)  \n",
    "        uniprots_tuple = (self.data.iloc[idx, 1], self.data.iloc[idx, 2]) # tuple that hold uniprot_id1 and uniprots_id2 -> for prob in testing phase\n",
    "        esm_features = np.array(self.esm_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        custom_features = np.array(self.custom_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        fegs_features = np.array(self.fegs_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        gae_features = np.array(self.gae_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "\n",
    "        input_ids = self.encoded_smiles[\"input_ids\"][idx]\n",
    "        attention_mask = self.encoded_smiles[\"attention_mask\"][idx]\n",
    "\n",
    "        # Retrieve precomputed RDKit Morgan fingerprints\n",
    "        morgan_fingerprint = self.smiles_morgan_fingerprints.loc[self.smiles_morgan_fingerprints['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemical_descriptors = self.smiles_chemical_descriptors.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        \n",
    "        return (smiles, uniprots_tuple, esm_features, custom_features, fegs_features, gae_features, \n",
    "                input_ids, attention_mask, morgan_fingerprint, chemical_descriptors, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f9fcc5-9cbc-4867-b143-81b158ecaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training without prob\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, ds_):\n",
    "        # Initialize data and load other features\n",
    "        self.data = ds_\n",
    "        self.mapping_df = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "        self.esm = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'esm_features.csv'))\n",
    "        self.custom = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'custom_features.csv'))\n",
    "        self.fegs = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'fegs_features.csv'))\n",
    "\n",
    "        gae_path = f'GAE_FEATURES_WITH_PREDICTED_alpha_0.25.csv'\n",
    "        self.gae = pd.read_csv(os.path.join('datasets', 'GAE', gae_path))\n",
    "        gae_features_columns = self.gae.iloc[:, 9:509]\n",
    "        gae_uniprot_column = self.gae[['From']].rename(columns={'From': 'UniProt_ID'})\n",
    "        self.gae = pd.concat([gae_uniprot_column, gae_features_columns], axis=1)\n",
    "        \n",
    "\n",
    "        # Merge datasets\n",
    "        self.esm_features_ppi = self.merge_datasets(self.data, self.esm).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.custom_features_ppi = self.merge_datasets(self.data, self.custom).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.fegs_features_ppi = self.merge_datasets(self.data, self.fegs).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.gae_features_ppi = self.merge_datasets(self.data, self.gae).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "\n",
    "        # SMILES RDKit features - Morgan Fingerprints (r=4, nbits=1024) & chemical descriptors\n",
    "        self.smiles_morgan_fingerprints = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_morgan_fingerprints_dataset.csv'))\n",
    "        self.smiles_chemical_descriptors = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_chem_descriptors_mapping_dataset.csv'))\n",
    "\n",
    "        # Necessary features for ChemBERTa model\n",
    "        self.smiles_list = self.data['smiles'].tolist()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "        self.encoded_smiles = self.tokenizer(self.smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def merge_datasets(self, dataset, features_df):\n",
    "        # Existing merging logic\n",
    "        dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID'])\n",
    "        \n",
    "        features_df_renamed = features_df.add_suffix('_id2')\n",
    "        features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
    "        dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID', 'uniprot_id1', 'uniprot_id2'])\n",
    "        \n",
    "        return dataset.drop_duplicates()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx, 0]\n",
    "        label = np.array(self.data.iloc[idx, -1], dtype=np.float32)  \n",
    "        esm_features = np.array(self.esm_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        custom_features = np.array(self.custom_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        fegs_features = np.array(self.fegs_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        gae_features = np.array(self.gae_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "\n",
    "        input_ids = self.encoded_smiles[\"input_ids\"][idx]\n",
    "        attention_mask = self.encoded_smiles[\"attention_mask\"][idx]\n",
    "\n",
    "        # Retrieve precomputed RDKit Morgan fingerprints\n",
    "        morgan_fingerprint = self.smiles_morgan_fingerprints.loc[self.smiles_morgan_fingerprints['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemical_descriptors = self.smiles_chemical_descriptors.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        \n",
    "        return (smiles, esm_features, custom_features, fegs_features, gae_features, \n",
    "                input_ids, attention_mask, morgan_fingerprint, chemical_descriptors, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d13657b1-0e95-4dbe-9845-20f1c6d08f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, ds_):\n",
    "        self.data = ds_\n",
    "        self.mapping_df = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter=\"\\t\")\n",
    "        self.esm = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'esm_features.csv'))\n",
    "        self.custom = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'custom_features.csv'))\n",
    "        self.fegs = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'fegs_features.csv'))\n",
    "\n",
    "        gae_path = f'GAE_FEATURES_WITH_PREDICTED_alpha_0.25.csv'\n",
    "        self.gae = pd.read_csv(os.path.join('datasets', 'GAE', gae_path))\n",
    "        gae_features_columns = self.gae.iloc[:, 9:509]\n",
    "        gae_uniprot_column = self.gae[['From']].rename(columns={'From': 'UniProt_ID'})\n",
    "        self.gae = pd.concat([gae_uniprot_column, gae_features_columns], axis=1)\n",
    "\n",
    "        # Merge datasets\n",
    "        self.esm_features_ppi = self.merge_datasets(self.data, self.esm).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.custom_features_ppi = self.merge_datasets(self.data, self.custom).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.fegs_features_ppi = self.merge_datasets(self.data, self.fegs).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "        self.gae_features_ppi = self.merge_datasets(self.data, self.gae).drop(columns=['smiles', 'label']).astype(np.float32)\n",
    "\n",
    "        # SMILES RDKit features - Morgan Fingerprints (r=4, nbits=1024) & chemical descriptors\n",
    "        self.smiles_morgan_fingerprints = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_morgan_fingerprints_dataset.csv'))\n",
    "        self.smiles_chemical_descriptors = pd.read_csv(os.path.join('datasets', 'MolDatasets', 'smiles_chem_descriptors_mapping_dataset.csv'))\n",
    "\n",
    "        # Necessary features for ChemBERTa model\n",
    "        self.smiles_list = self.data['smiles'].tolist()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "        self.encoded_smiles = self.tokenizer(self.smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Protein structure feature extraction\n",
    "        self.uniprots = ds_[['uniprot_id1', 'uniprot_id2']]\n",
    "\n",
    "        # Preload all protein structure feature files into a dictionary\n",
    "        self.protein_structure_dict = self.load_protein_structure_features()\n",
    "\n",
    "    def merge_datasets(self, dataset, features_df):\n",
    "        # Existing merging logic\n",
    "        dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID'])\n",
    "        \n",
    "        features_df_renamed = features_df.add_suffix('_id2')\n",
    "        features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
    "        dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
    "        dataset = dataset.drop(columns=['UniProt_ID', 'uniprot_id1', 'uniprot_id2'])\n",
    "        \n",
    "        return dataset.drop_duplicates()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_protein_structure_features(self):\n",
    "        \"\"\"Preload all protein structure feature CSVs into a dictionary.\"\"\"\n",
    "        protein_structure_dir = os.path.join('datasets', 'MolDatasets', 'ProteinStructureFeatures')\n",
    "        protein_structure_files = os.listdir(protein_structure_dir)\n",
    "        protein_structure_dict = {}\n",
    "        \n",
    "        for file in protein_structure_files:\n",
    "            uniprot_id_key = file.replace('_ifeature_omega.csv', '')\n",
    "            protein_structure_dict[uniprot_id_key] = pd.read_csv(os.path.join(protein_structure_dir, file)).iloc[:, 1:].astype(np.float32)\n",
    "        return protein_structure_dict\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data.iloc[idx, 0]\n",
    "        label = np.array(self.data.iloc[idx, -1], dtype=np.float32)  \n",
    "        esm_features = np.array(self.esm_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        custom_features = np.array(self.custom_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        fegs_features = np.array(self.fegs_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "        gae_features = np.array(self.gae_features_ppi.iloc[idx].values, dtype=np.float32)\n",
    "\n",
    "        input_ids = self.encoded_smiles[\"input_ids\"][idx]\n",
    "        attention_mask = self.encoded_smiles[\"attention_mask\"][idx]\n",
    "\n",
    "        # Retrieve precomputed RDKit Morgan fingerprints\n",
    "        morgan_fingerprint = self.smiles_morgan_fingerprints.loc[self.smiles_morgan_fingerprints['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "        chemical_descriptors = self.smiles_chemical_descriptors.loc[self.smiles_chemical_descriptors['SMILES'] == smiles].iloc[0, 1:].values.astype(np.float32)\n",
    "\n",
    "        # Protein structure feature extraction\n",
    "        prot1_sfp, prot2_sfp = self.uniprots.iloc[idx, 0], self.uniprots.iloc[idx, 1]\n",
    "        ans = self.checkProteins(prot1_sfp, prot2_sfp)\n",
    "        \n",
    "        if ans:\n",
    "            prot1_sf = self.protein_structure_dict[ans]\n",
    "            prot2_sf = self.protein_structure_dict[f'Q03164_with_{ans}']\n",
    "        else:\n",
    "            prot1_sf = self.protein_structure_dict[prot1_sfp]\n",
    "            prot2_sf = self.protein_structure_dict[prot2_sfp]\n",
    "\n",
    "        return (smiles, prot1_sf, prot2_sf, esm_features, custom_features, fegs_features, gae_features, \n",
    "                input_ids, attention_mask, morgan_fingerprint, chemical_descriptors, label)\n",
    "\n",
    "    def checkProteins(self, unip1, unip2):\n",
    "        if unip1 == 'Q03164':\n",
    "            return unip2\n",
    "        elif unip2 == 'Q03164':\n",
    "            return unip1\n",
    "        return None\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        smiles, prot1_sfs, prot2_sfs, esm_features, custom_features, fegs_features, gae_features, input_ids, attention_masks, morgan_fingerprints, chemical_descriptors, labels = zip(*batch)\n",
    "\n",
    "        # Convert lists of numpy arrays into tensors for protein structure features\n",
    "        prot1_sfs = [torch.tensor(p.values) for p in prot1_sfs]\n",
    "        prot2_sfs = [torch.tensor(p.values) for p in prot2_sfs]\n",
    "        \n",
    "        # Find the maximum number of rows (0th dimension) for prot1 and prot2 individually\n",
    "        max_len_prot1 = max([p.shape[0] for p in prot1_sfs])\n",
    "        max_len_prot2 = max([p.shape[0] for p in prot2_sfs])\n",
    "\n",
    "        # Pad the protein structure features along the 0th dimension (rows) for each protein individually\n",
    "        prot1_sfs_padded = torch.stack([torch.nn.functional.pad(p, (0, 0, 0, max_len_prot1 - p.shape[0]), \"constant\", 0) for p in prot1_sfs])\n",
    "        prot2_sfs_padded = torch.stack([torch.nn.functional.pad(p, (0, 0, 0, max_len_prot2 - p.shape[0]), \"constant\", 0) for p in prot2_sfs])\n",
    "    \n",
    "        esm_features = torch.tensor(esm_features, dtype=torch.float32)\n",
    "        custom_features = torch.tensor(custom_features, dtype=torch.float32)\n",
    "        fegs_features = torch.tensor(fegs_features, dtype=torch.float32)\n",
    "        gae_features = torch.tensor(gae_features, dtype=torch.float32)\n",
    "        \n",
    "        input_ids = torch.stack(input_ids)\n",
    "        attention_masks = torch.stack(attention_masks)\n",
    "        \n",
    "        morgan_fingerprints = torch.tensor(morgan_fingerprints, dtype=torch.float32)\n",
    "        chemical_descriptors = torch.tensor(chemical_descriptors, dtype=torch.float32)\n",
    "        \n",
    "        # Convert labels to a flat list of scalars and then to a tensor\n",
    "        flattened_labels = [label.item() for label in labels]\n",
    "        labels_tensor = torch.tensor(flattened_labels, dtype=torch.float32)\n",
    "        \n",
    "        return (smiles, prot1_sfs_padded, prot2_sfs_padded, esm_features, custom_features, fegs_features, gae_features, \n",
    "                input_ids, attention_masks, morgan_fingerprints, chemical_descriptors, labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96d8eb39-a557-4cdf-9a2e-e79487044aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(checkpoint_path,\n",
    "                   batch_size,\n",
    "                  dropout) -> nn.Module:\n",
    "    pretrained_chemprop_model = PretrainedChempropModel(checkpoints_path, batch_size)\n",
    "    chemberta_model = ChemBERTaPT()\n",
    "    ft_model = AUVG_PPI(pretrained_chemprop_model, chemberta_model, dropout).to(device)\n",
    "\n",
    "    PRINTM('Generated combined model for fine-tuning successfully !')\n",
    "    return ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e569e55-f80f-468c-9e95-14524143e452",
   "metadata": {},
   "source": [
    "## Train the Model on multi_ppim_fold_2_0.8 Folds ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd14c3-a534-43c5-bdc2-1aa6bccd110e",
   "metadata": {},
   "source": [
    "### Load & Prepare the Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b276610-4b98-48a7-80ed-b290391d67e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Folder content:\n",
      "\n",
      "['train_fold1.csv', 'train_fold3.csv', 'test_fold3.csv', 'train_fold5.csv', 'test_fold5.csv', 'test_fold2.csv', 'test_fold4.csv', 'train_fold4.csv', 'test_fold1.csv', 'train_fold2.csv']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_folder_path = os.path.join('datasets', 'finetune_dataset', 'multi_ppim_folds_2_0.8')\n",
    "all_files = os.listdir(ds_folder_path)\n",
    "\n",
    "PRINTM(f'Folder content:\\n\\n{all_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9295d4c-ae37-40f4-8e4b-f3a4f1fad797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# Read each CSV file into a dataframe and store it in the dictionary\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(ds_folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_name = file.replace('.csv', '_df')\n",
    "    dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d447c08-ad11-4177-96c9-b56a85a268c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "uniprot_mapping = pd.read_csv(os.path.join('datasets', 'idmapping_unip.tsv'), delimiter = \"\\t\")\n",
    "ppi_features_df = pd.read_csv(os.path.join('datasets', 'merged_ppi_features.csv'))\n",
    "PRINT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d601d174-176f-46c9-bdb6-e792ac9eaf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done inverse mapping !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df_name in dataframes.keys():\n",
    "    dataframes[df_name] = convert_uniprot_ids(dataframes[df_name], uniprot_mapping)\n",
    "    #dataframes[df_name] =data_augmentation_with_uniprots_order_switchings(dataframes[df_name])\n",
    "    #dataframes[df_name] = merge_datasets(dataframes[df_name], ppi_features_df)\n",
    "\n",
    "# Access each dataframe using its name\n",
    "train_fold1_df = dataframes['train_fold1_df']\n",
    "train_fold2_df = dataframes['train_fold2_df']\n",
    "train_fold3_df = dataframes['train_fold3_df']\n",
    "train_fold4_df = dataframes['train_fold4_df']\n",
    "train_fold5_df = dataframes['train_fold5_df']\n",
    "test_fold1_df = dataframes['test_fold1_df']\n",
    "test_fold2_df = dataframes['test_fold2_df']\n",
    "test_fold3_df = dataframes['test_fold3_df']\n",
    "test_fold4_df = dataframes['test_fold4_df']\n",
    "test_fold5_df = dataframes['test_fold5_df']\n",
    "\n",
    "PRINTM(f'Done inverse mapping !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea5f3f-beae-45e7-80ec-62d0099066ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = os.path.join('pt_chemprop_checkpoint_r4_', 'fold_0', 'model_0', 'checkpoints', 'best-epoch=39-val_loss=0.39.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda3362-a78e-41b0-b479-8d59641103f3",
   "metadata": {},
   "source": [
    "### Train a Model & Test on Each Fold ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa8a8b-2663-4240-9308-bd7add830b46",
   "metadata": {},
   "source": [
    "#### Fold number 1 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af29201-5e65-4924-9354-8abf1710ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f1 = generate_model(checkpoints_path, batch_size=32, dropout=0.3)\n",
    "f1_res = ft_model_f1.cross_validate(train_fold1_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=32, learning_rate=1e-5, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86402fe3-5b8a-49aa-a756-3022c5b3bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.76603\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.1, k=5, n=2, lr=1e-5, wd=1e-4 --> Overfit -> increase d\n",
    "f1_auc = ft_model_f1.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "748cbf82-f8ba-4421-a281-22ed1e4d26db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.78468\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-4 \n",
    "f1_auc = ft_model_f1.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2f0088c-7896-480f-9c48-51e623ee9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.80617\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-3 \n",
    "f1_auc = ft_model_f1.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccac4239-0391-4d15-9882-9fab6ec3a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.70589\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.3, k=5, n=2, lr=1e-5, wd=1e-3 \n",
    "f1_auc = ft_model_f1.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7930b-5a13-4b89-8e46-e5c51944e125",
   "metadata": {},
   "source": [
    "#### Fold number 2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647d02d-7015-4a97-bf00-97d21dfa27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f2 = generate_model(checkpoints_path, batch_size=32, dropout=0.3)\n",
    "f2_res = ft_model_f2.cross_validate(train_fold2_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=32, learning_rate=1e-5, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf05489a-a20c-4d50-80ae-44e122fc4228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.62343\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.1, k=5, n=2, lr=1e-5, wd=1e-4 --> Overfit -> increase d\n",
    "f2_auc = ft_model_f2.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e75d982-ce3a-48fd-8a78-b654f0f3b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.66849\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-4 \n",
    "f2_auc = ft_model_f2.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f649705-b2d1-4bcf-820d-f87e2f0725cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.68760\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-3\n",
    "f2_auc = ft_model_f2.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8960a2ab-914e-43bf-99aa-dffd82d65f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.72244\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.3, k=5, n=2, lr=1e-5, wd=1e-3\n",
    "f2_auc = ft_model_f2.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6178d88-b2c8-4573-8e0b-3739ae1fe750",
   "metadata": {},
   "source": [
    "#### Fold number 3 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11eca39e-ac59-4b99-98f2-4b6172a4b7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Generated combined model for fine-tuning successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "Start training !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation BCEWithLogitsLoss: 0.74370\n",
      "Validation Accuracy (>0.8): 0.67\n",
      "Validation AUC: 0.60483\n",
      "Epoch time: 3.98 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Validation BCEWithLogitsLoss: 0.70420\n",
      "Validation Accuracy (>0.8): 0.71\n",
      "Validation AUC: 0.68103\n",
      "Epoch time: 4.05 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Finish training !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 1 - Validation BCEWithLogitsLoss: 0.70251, Accuracy: 0.71, AUC: 0.68414\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "Start training !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation BCEWithLogitsLoss: 0.67303\n",
      "Validation Accuracy (>0.8): 0.75\n",
      "Validation AUC: 0.77063\n",
      "Epoch time: 4.04 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Validation BCEWithLogitsLoss: 0.57830\n",
      "Validation Accuracy (>0.8): 0.76\n",
      "Validation AUC: 0.84576\n",
      "Epoch time: 4.00 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Finish training !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 2 - Validation BCEWithLogitsLoss: 0.57739, Accuracy: 0.76, AUC: 0.84581\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "Start training !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation BCEWithLogitsLoss: 0.51620\n",
      "Validation Accuracy (>0.8): 0.76\n",
      "Validation AUC: 0.86629\n",
      "Epoch time: 4.01 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Validation BCEWithLogitsLoss: 0.50147\n",
      "Validation Accuracy (>0.8): 0.79\n",
      "Validation AUC: 0.88497\n",
      "Epoch time: 4.02 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Finish training !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 3 - Validation BCEWithLogitsLoss: 0.50285, Accuracy: 0.79, AUC: 0.88473\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "Start training !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation BCEWithLogitsLoss: 0.42495\n",
      "Validation Accuracy (>0.8): 0.82\n",
      "Validation AUC: 0.91352\n",
      "Epoch time: 4.03 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Validation BCEWithLogitsLoss: 0.39784\n",
      "Validation Accuracy (>0.8): 0.86\n",
      "Validation AUC: 0.93442\n",
      "Epoch time: 4.02 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Finish training !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 4 - Validation BCEWithLogitsLoss: 0.39437, Accuracy: 0.86, AUC: 0.93453\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "Start training !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation BCEWithLogitsLoss: 0.37168\n",
      "Validation Accuracy (>0.8): 0.87\n",
      "Validation AUC: 0.94579\n",
      "Epoch time: 4.05 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Validation BCEWithLogitsLoss: 0.35425\n",
      "Validation Accuracy (>0.8): 0.88\n",
      "Validation AUC: 0.94929\n",
      "Epoch time: 4.02 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Finish training !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 5 - Validation BCEWithLogitsLoss: 0.35332, Accuracy: 0.88, AUC: 0.94942\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Average Validation BCEWithLogitsLoss: 0.50609\n",
      "Average Validation Accuracy: 0.80\n",
      "Average Validation AUC: 0.85973\n"
     ]
    }
   ],
   "source": [
    "ft_model_f3 = generate_model(checkpoints_path, batch_size=32, dropout=0.3)\n",
    "f3_res = ft_model_f3.cross_validate(train_fold3_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=32, learning_rate=1e-5, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c01d7972-225f-4bff-9f70-0ea62a6a19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.72224\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.1, k=5, n=2, lr=1e-5, wd=1e-4 --> Overfir -> increase d\n",
    "f3_auc = ft_model_f3.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fdc3048-6cbc-4bbf-bb8e-b20d6c27a06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.77971\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-4 --> Overfit\n",
    "f3_auc = ft_model_f3.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b0c16f0-c7a7-4637-815c-76074067a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.75466\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-3 \n",
    "f3_auc = ft_model_f3.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8800c975-1df8-4faa-9970-4f00642f2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.68496\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#14/09 - d=0.3, k=5, n=2, lr=1e-5, wd=1e-3 \n",
    "f3_auc = ft_model_f3.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6468a6-6d67-46b9-bf81-b5ffdaae7f55",
   "metadata": {},
   "source": [
    "#### Folder number 4 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f586ef7-e04a-48de-a901-174d2d7b196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Generated combined model for fine-tuning successfully !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "Start training !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation BCEWithLogitsLoss: 0.70503\n",
      "Validation Accuracy (>0.8): 0.70\n",
      "Validation AUC: 0.62226\n",
      "Epoch time: 3.05 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Validation BCEWithLogitsLoss: 0.67812\n",
      "Validation Accuracy (>0.8): 0.71\n",
      "Validation AUC: 0.67787\n",
      "Epoch time: 3.04 minutes\n",
      "--------------------------------------------------------------------------------\n",
      "Finish training !\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 1 - Validation BCEWithLogitsLoss: 0.67847, Accuracy: 0.71, AUC: 0.67800\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 2/5\n"
     ]
    }
   ],
   "source": [
    "ft_model_f4 = generate_model(checkpoints_path, batch_size=32, dropout=0.3)\n",
    "f4_res = ft_model_f4.cross_validate(train_fold4_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=32, learning_rate=1e-5, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbe0ff19-0eac-4502-9160-4a4a72817457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.83464\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.1, k=5, n=2, lr=1e-5, wd=1e-4\n",
    "f4_auc = ft_model_f4.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eea02720-023d-4be1-b79d-40be024ac7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.78949\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-4\n",
    "f4_auc = ft_model_f4.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "810537d7-0f36-4e32-ab53-4e287991962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.84944\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.1, k=5, n=2, lr=1e-5, wd=1e-4\n",
    "f4_auc = ft_model_f4.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf68ad8-1bd6-40f8-96be-878c5df0ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.74260\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#14/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-3 --> underfit\n",
    "f4_auc = ft_model_f4.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4c251-a9cd-41b5-8ea9-e8c3b55356e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14/09 - d=0.3, k=5, n=2, lr=1e-5, wd=1e-3 --> underfit\n",
    "f4_auc = ft_model_f4.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be051e-278d-4bd3-b8e2-d90098292394",
   "metadata": {},
   "source": [
    "#### Folder number 5 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa2172-1e43-46ac-86af-bf5461820436",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f5 = generate_model(checkpoints_path, batch_size=32, dropout=0.3)\n",
    "f5_res = ft_model_f5.cross_validate(train_fold5_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=32, learning_rate=1e-5, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "885e83dc-80a2-4ec2-8cf0-f71684af7344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.77281\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.1, k=5, n=2, lr=1e-5, wd=1e-4\n",
    "f5_auc = ft_model_f5.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a357519-d39e-478d-a15c-7df314457c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.84392\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-4\n",
    "f5_auc = ft_model_f5.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25cc8747-e00b-4fdd-bf0c-000ec8a91f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.80041\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#13/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-4\n",
    "f5_auc = ft_model_f5.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03e1623c-c31e-447a-988f-72b72852fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.80463\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#14/09 - d=0.2, k=5, n=2, lr=1e-5, wd=1e-3\n",
    "f5_auc = ft_model_f5.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbec99-dc52-4c1a-b00b-e02242914854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14/09 - d=0.3, k=5, n=2, lr=1e-5, wd=1e-3\n",
    "f5_auc = ft_model_f5.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd02b8-4df1-4dd8-9b89-73b4a23933d1",
   "metadata": {},
   "source": [
    "## Train the Model on Finetuned Datasets (DLIP_folds_2_0.8)  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "318521e6-2548-4430-9b13-331d0d3d1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Folder content:\n",
      "\n",
      "['test_fold5.csv', 'test_fold2.csv', 'test_fold4.csv', 'test_fold3.csv', 'test_fold1.csv', 'train_fold4.csv', 'train_fold2.csv', 'train_fold3.csv', 'train_fold1.csv', 'train_fold5.csv']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_folder_path = os.path.join('datasets', 'finetune_dataset', 'DLIP_folds_2_0.8')\n",
    "all_files = os.listdir(ds_folder_path)\n",
    "\n",
    "PRINTM(f'Folder content:\\n\\n{all_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc4dc475-61cf-4ff8-bc04-be92905c86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# Read each CSV file into a dataframe and store it in the dictionary\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(ds_folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_name = file.replace('.csv', '_df')\n",
    "    dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a6d1065-1ad7-49c9-bdb6-51acd8868dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done inverse mapping & merging successfully !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df_name in dataframes.keys():\n",
    "    dataframes[df_name] = convert_uniprot_ids(dataframes[df_name], uniprot_mapping)\n",
    "    #dataframes[df_name] = merge_datasets(dataframes[df_name], ppi_features_df)\n",
    "\n",
    "# Access each dataframe using its name\n",
    "train_fold1_df = dataframes['train_fold1_df']\n",
    "train_fold2_df = dataframes['train_fold2_df']\n",
    "train_fold3_df = dataframes['train_fold3_df']\n",
    "train_fold4_df = dataframes['train_fold4_df']\n",
    "train_fold5_df = dataframes['train_fold5_df']\n",
    "test_fold1_df = dataframes['test_fold1_df']\n",
    "test_fold2_df = dataframes['test_fold2_df']\n",
    "test_fold3_df = dataframes['test_fold3_df']\n",
    "test_fold4_df = dataframes['test_fold4_df']\n",
    "test_fold5_df = dataframes['test_fold5_df']\n",
    "\n",
    "PRINTM(f'Done inverse mapping & merging successfully !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac6fd3-2976-4cf2-bc63-0475648caaff",
   "metadata": {},
   "source": [
    "#### Fold number 1 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1124747-606d-4c49-9685-541b9f2751df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f1_dlip = generate_model(checkpoints_path, batch_size=64, dropout=0.1)\n",
    "f1_dlip_res = ft_model_f1_dlip.cross_validate(train_fold1_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd55f1f-a88f-4975-87b9-35cfb767b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_auc = ft_model_f1_dlip.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss(),batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cabbfb-3583-4d68-ad53-3b0debe06b75",
   "metadata": {},
   "source": [
    "#### Fold number 2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158a226-7267-47eb-b3f5-5ad98cff73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f2_dlip = generate_model(checkpoints_path, batch_size=32, dropout=0.1)\n",
    "f2_dlip_res = ft_model_f2_dlip.cross_validate(train_fold2_df, num_folds=2, num_epochs=15,\n",
    "                     batch_size=32, learning_rate=0.0001, weight_decay=1e-5,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "616ed377-91a9-4774-9fe6-d5c83d310116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.75522\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ft_model_f2_dlip.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e549c4-121c-49b9-9dc6-912d332a07cc",
   "metadata": {},
   "source": [
    "#### Fold number 3 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8edb3-3b96-415f-ac39-47cd6c608f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f3_dlip = generate_model(checkpoints_path, batch_size=64, dropout=0.1)\n",
    "f3_dlip_res = ft_model_f3_dlip.cross_validate(train_fold3_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6924ae-1a4d-443d-b121-7a80c1996b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_auc = ft_model_f3_dlip.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bf64088-2c47-42ec-9bab-acd78a2646d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.75724\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7572357382550335"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3_auc = ft_model_f3_dlip.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCEWithLogitsLoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b84d6-2a20-41ea-89fd-4f39bc7a0f6d",
   "metadata": {},
   "source": [
    "#### Fold number 4 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4574efb2-3727-4238-8104-90f333c3cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f4_dlip = generate_model(checkpoints_path, batch_size=64, dropout=0.1)\n",
    "f4_dlip_res = ft_model_f4_dlip.cross_validate(train_fold4_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9d0ef-dd3e-4101-8725-bffc49956819",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f4_dlip.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2730dc6-8a20-4aae-b2dc-1608793f8c1f",
   "metadata": {},
   "source": [
    "#### Fold number 5 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462432a-a49c-4829-814d-44fcdd0d7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f5_dlip = generate_model(checkpoints_path, batch_size=64, dropout=0.1)\n",
    "f5_dlip_res = ft_model_f5_dlip.cross_validate(train_fold5_df, num_folds=5, num_epochs=2,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-3,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e5d22-315a-4193-b54c-b60abfa38bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f5_dlip.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471e7f4-7d04-4aa1-83cf-464dcc518d1b",
   "metadata": {},
   "source": [
    "## Train the Model on Finetuned Datasets (DLIP_folds_2_0.9)  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed08be16-4937-44bc-8cb9-8f8e7aed2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Folder content:\n",
      "\n",
      "['test_fold3.csv', 'test_fold2.csv', 'test_fold1.csv', 'test_fold5.csv', 'train_fold4.csv', 'train_fold2.csv', 'train_fold3.csv', 'train_fold5.csv', 'test_fold4.csv', 'train_fold1.csv']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_folder_path = os.path.join('datasets', 'finetune_dataset', 'DLIP_folds_3_0.9')\n",
    "all_files = os.listdir(ds_folder_path)\n",
    "\n",
    "PRINTM(f'Folder content:\\n\\n{all_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b684433-0cfb-4372-85ea-3ed8eb5f59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# Read each CSV file into a dataframe and store it in the dictionary\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(ds_folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_name = file.replace('.csv', '_df')\n",
    "    dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4550d26-8036-4f0f-89a5-a25e17d0efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done inverse mapping & merging successfully !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df_name in dataframes.keys():\n",
    "    dataframes[df_name] = convert_uniprot_ids(dataframes[df_name], uniprot_mapping)\n",
    "    dataframes[df_name] = merge_datasets(dataframes[df_name], ppi_features_df)\n",
    "\n",
    "# Access each dataframe using its name\n",
    "train_fold1_df = dataframes['train_fold1_df']\n",
    "train_fold2_df = dataframes['train_fold2_df']\n",
    "train_fold3_df = dataframes['train_fold3_df']\n",
    "train_fold4_df = dataframes['train_fold4_df']\n",
    "train_fold5_df = dataframes['train_fold5_df']\n",
    "test_fold1_df = dataframes['test_fold1_df']\n",
    "test_fold2_df = dataframes['test_fold2_df']\n",
    "test_fold3_df = dataframes['test_fold3_df']\n",
    "test_fold4_df = dataframes['test_fold4_df']\n",
    "test_fold5_df = dataframes['test_fold5_df']\n",
    "\n",
    "PRINTM(f'Done inverse mapping & merging successfully !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce778512-32a9-456a-81f3-d81de40a27cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>label</th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_1270_id2</th>\n",
       "      <th>Feature_1271_id2</th>\n",
       "      <th>Feature_1272_id2</th>\n",
       "      <th>Feature_1273_id2</th>\n",
       "      <th>Feature_1274_id2</th>\n",
       "      <th>Feature_1275_id2</th>\n",
       "      <th>Feature_1276_id2</th>\n",
       "      <th>Feature_1277_id2</th>\n",
       "      <th>Feature_1278_id2</th>\n",
       "      <th>Feature_1279_id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nc1ccc(CNC(=O)NC[C@H](NC(=O)[C@@H]2CCCN2S(=O)(...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.068394</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.142344</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>-0.057618</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052764</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>-0.061765</td>\n",
       "      <td>0.122240</td>\n",
       "      <td>-0.107535</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.064282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(O)CCNC(=O)c1ccc2c(c1)C(=O)N(CCC1CCNCC1)C2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.068394</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.142344</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>-0.057618</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046108</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>-0.160452</td>\n",
       "      <td>0.058708</td>\n",
       "      <td>-0.045173</td>\n",
       "      <td>-0.135907</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>-0.136259</td>\n",
       "      <td>-0.023708</td>\n",
       "      <td>0.164988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cc1cc(C)cc(S(=O)(=O)N2CCC[C@H]2C(=O)N[C@@H](CN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.068394</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.142344</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>-0.057618</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052764</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>-0.061765</td>\n",
       "      <td>0.122240</td>\n",
       "      <td>-0.107535</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.064282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(O)CC(NC(=O)CCC(=O)Nc1ccc2c(c1)CNC2)c1ccccc1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>-0.061126</td>\n",
       "      <td>-0.020618</td>\n",
       "      <td>0.032968</td>\n",
       "      <td>-0.081779</td>\n",
       "      <td>-0.046594</td>\n",
       "      <td>0.086232</td>\n",
       "      <td>-0.010491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046108</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>-0.160452</td>\n",
       "      <td>0.058708</td>\n",
       "      <td>-0.045173</td>\n",
       "      <td>-0.135907</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>-0.136259</td>\n",
       "      <td>-0.023708</td>\n",
       "      <td>0.164988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N=C(N)NCCC[C@@H]1NC(=O)[C@H]2COCCN2C(=O)[C@@H]...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>-0.060721</td>\n",
       "      <td>-0.051823</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>-0.062652</td>\n",
       "      <td>-0.053298</td>\n",
       "      <td>0.085150</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036552</td>\n",
       "      <td>-0.016408</td>\n",
       "      <td>-0.189185</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>-0.061862</td>\n",
       "      <td>-0.075560</td>\n",
       "      <td>0.090893</td>\n",
       "      <td>-0.115112</td>\n",
       "      <td>-0.029084</td>\n",
       "      <td>0.098593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  label  Feature_0  \\\n",
       "0  Nc1ccc(CNC(=O)NC[C@H](NC(=O)[C@@H]2CCCN2S(=O)(...      1   0.030220   \n",
       "1      O=C(O)CCNC(=O)c1ccc2c(c1)C(=O)N(CCC1CCNCC1)C2      1   0.030220   \n",
       "2  Cc1cc(C)cc(S(=O)(=O)N2CCC[C@H]2C(=O)N[C@@H](CN...      1   0.030220   \n",
       "3    O=C(O)CC(NC(=O)CCC(=O)Nc1ccc2c(c1)CNC2)c1ccccc1      1   0.013775   \n",
       "4  N=C(N)NCCC[C@@H]1NC(=O)[C@H]2COCCN2C(=O)[C@@H]...      1   0.016134   \n",
       "\n",
       "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0  -0.068394  -0.093040   0.142344  -0.101274  -0.057618   0.093081   \n",
       "1  -0.068394  -0.093040   0.142344  -0.101274  -0.057618   0.093081   \n",
       "2  -0.068394  -0.093040   0.142344  -0.101274  -0.057618   0.093081   \n",
       "3  -0.061126  -0.020618   0.032968  -0.081779  -0.046594   0.086232   \n",
       "4  -0.060721  -0.051823   0.028336  -0.062652  -0.053298   0.085150   \n",
       "\n",
       "   Feature_7  ...  Feature_1270_id2  Feature_1271_id2  Feature_1272_id2  \\\n",
       "0  -0.037972  ...          0.052764          0.007230         -0.032889   \n",
       "1  -0.037972  ...         -0.046108          0.018945         -0.160452   \n",
       "2  -0.037972  ...          0.052764          0.007230         -0.032889   \n",
       "3  -0.010491  ...         -0.046108          0.018945         -0.160452   \n",
       "4  -0.030032  ...         -0.036552         -0.016408         -0.189185   \n",
       "\n",
       "   Feature_1273_id2  Feature_1274_id2  Feature_1275_id2  Feature_1276_id2  \\\n",
       "0          0.013346         -0.002336         -0.061765          0.122240   \n",
       "1          0.058708         -0.045173         -0.135907          0.036617   \n",
       "2          0.013346         -0.002336         -0.061765          0.122240   \n",
       "3          0.058708         -0.045173         -0.135907          0.036617   \n",
       "4          0.029432         -0.061862         -0.075560          0.090893   \n",
       "\n",
       "   Feature_1277_id2  Feature_1278_id2  Feature_1279_id2  \n",
       "0         -0.107535         -0.056424          0.064282  \n",
       "1         -0.136259         -0.023708          0.164988  \n",
       "2         -0.107535         -0.056424          0.064282  \n",
       "3         -0.136259         -0.023708          0.164988  \n",
       "4         -0.115112         -0.029084          0.098593  \n",
       "\n",
       "[5 rows x 2562 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fold5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed4596-78bf-4498-afc3-2d67f77a0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dataframes.items():\n",
    "    null_counts = df.isnull().sum().sum()\n",
    "    PRINTM(f'Number of nan values in {df_name} is -> {null_counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674f908-229d-4d2c-b5ad-adb2efeccd05",
   "metadata": {},
   "source": [
    "#### Fold number 1 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b9a0431-028a-4665-93c6-145a2b7204c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945c8a3-350b-4dc7-bc7f-d82a549f4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f1_dlip_ = generate_model(checkpoints_path, batch_size=64, dropout=0.8)\n",
    "f1_dlip_res = ft_model_f1_dlip_.cross_validate(train_fold1_df, num_folds=10, num_epochs=5,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-4,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ddf1426-08fe-4388-ad58-57dfe17d6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.38214\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ft_model_f1_dlip_.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1950457-20d8-4d06-b138-87e76422f075",
   "metadata": {},
   "source": [
    "#### Fold number 2 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223efa5-b615-4323-8d90-8b8fdd262219",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f2_dlip_ = generate_model(checkpoints_path, batch_size=64, dropout=0.3)\n",
    "f2_dlip_res_ = ft_model_f2_dlip_.cross_validate(train_fold2_df, num_folds=10, num_epochs=5,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-4,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c73d38c-d8f5-4011-bcaf-9318de459680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.77614\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ft_model_f2_dlip_.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4cae1-687f-4857-a6fb-19ee8fc171ff",
   "metadata": {},
   "source": [
    "#### Fold number 3 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaea950-c333-486e-b77b-a9966c9fab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f3_dlip_ = generate_model(checkpoints_path, batch_size=64, dropout=0.3)\n",
    "f3_dlip_res_ = ft_model_f3_dlip_.cross_validate(train_fold3_df, num_folds=10, num_epochs=5,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-4,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cee2c129-1d47-4065-b623-f46354c3dd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.71449\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ft_model_f3_dlip_.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d5349-1e57-49a6-8eaf-323712777290",
   "metadata": {},
   "source": [
    "#### Fold number 4 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b69a3-7e51-429c-8b7d-28dd32915f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f4_dlip_ = generate_model(checkpoints_path, batch_size=64, dropout=0.3)\n",
    "f4_dlip_res_ = ft_model_f4_dlip_.cross_validate(train_fold4_df, num_folds=10, num_epochs=5,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-4,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58fc5c63-9318-42fc-bcc2-840c542a7460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.84679\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ft_model_f4_dlip_.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbef40d-2cc6-41d2-8db9-0f6fd57a1198",
   "metadata": {},
   "source": [
    "#### Fold number 5 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d950655-1ff6-43ac-bec6-7f49ac92d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f5_dlip_ = generate_model(checkpoints_path, batch_size=64, dropout=0.3)\n",
    "f5_dlip_res_ = ft_model_f5_dlip_.cross_validate(train_fold5_df, num_folds=10, num_epochs=5,\n",
    "                     batch_size=64, learning_rate=0.0001, weight_decay=1e-4,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10931e24-a779-4da4-9c64-86b2171e37b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Test AUC: 0.83778\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ft_model_f5_dlip_.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=64,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "391b1e51-0856-49c3-a0e5-c9234aa8d523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Folder content:\n",
      "\n",
      "['test_fold1.csv', 'test_fold3.csv', 'test_fold5.csv', 'train_val_fold2.csv', 'train_val_fold3.csv', 'train_val_fold5.csv', 'test_fold4.csv', 'train_val_fold4.csv', 'test_fold2.csv', 'train_val_fold1.csv']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_folder_path = os.path.join('datasets', 'finetune_dataset', 'original_folds PPIMI')\n",
    "all_files = os.listdir(ds_folder_path)\n",
    "\n",
    "PRINTM(f'Folder content:\\n\\n{all_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f8faf93-90c8-4352-af3d-8e37b5daec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# Read each CSV file into a dataframe and store it in the dictionary\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(ds_folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_name = file.replace('.csv', '_df')\n",
    "    dataframes[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88655676-bee3-426f-9030-8ddcf28ba8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame: test_fold1_df\n",
      "Updated DataFrame: test_fold3_df\n",
      "Updated DataFrame: test_fold5_df\n",
      "Updated DataFrame: train_val_fold2_df\n",
      "Updated DataFrame: train_val_fold3_df\n",
      "Updated DataFrame: train_val_fold5_df\n",
      "Updated DataFrame: test_fold4_df\n",
      "Updated DataFrame: train_val_fold4_df\n",
      "Updated DataFrame: test_fold2_df\n",
      "Updated DataFrame: train_val_fold1_df\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in dataframes.items():\n",
    "    # Replace 'na' with np.nan if necessary\n",
    "    df.replace('na', np.nan, inplace=True)\n",
    "    \n",
    "    # Identify rows where 'uniprot_id2' is NaN and replace them with 'uniprot_id1' values\n",
    "    df.loc[df['uniprot_id2'].isna(), 'uniprot_id2'] = df['uniprot_id1']\n",
    "    \n",
    "    print(f'Updated DataFrame: {df_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a709fd5-bafb-4fb0-9847-2be0d8e43d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>uniprot_id1</th>\n",
       "      <th>uniprot_id2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>COc1cnc2n1C(C)(Cc1ccc(Br)cc1)C(=O)N2c1cc(Cl)cc...</td>\n",
       "      <td>P62942</td>\n",
       "      <td>P62942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>COc1cccc2c(C(=O)C(=O)N3CCN(C(=O)c4ccccc4)CC3)c...</td>\n",
       "      <td>P62942</td>\n",
       "      <td>P62942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>COc1cc(-c2cn(C)c(=O)c3cnccc23)cc(OC)c1CN(C)C</td>\n",
       "      <td>P62942</td>\n",
       "      <td>P62942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6308</th>\n",
       "      <td>CC(C)c1ccccc1Sc1ccc(-c2ccnc(N3CCCC3)c2)cc1C(F)...</td>\n",
       "      <td>P62942</td>\n",
       "      <td>P62942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6309</th>\n",
       "      <td>CNc1cccc(CCOc2ccc(C[C@H](NC(=O)c3c(Cl)cncc3Cl)...</td>\n",
       "      <td>P62942</td>\n",
       "      <td>P62942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles uniprot_id1  \\\n",
       "6305  COc1cnc2n1C(C)(Cc1ccc(Br)cc1)C(=O)N2c1cc(Cl)cc...      P62942   \n",
       "6306  COc1cccc2c(C(=O)C(=O)N3CCN(C(=O)c4ccccc4)CC3)c...      P62942   \n",
       "6307       COc1cc(-c2cn(C)c(=O)c3cnccc23)cc(OC)c1CN(C)C      P62942   \n",
       "6308  CC(C)c1ccccc1Sc1ccc(-c2ccnc(N3CCCC3)c2)cc1C(F)...      P62942   \n",
       "6309  CNc1cccc(CCOc2ccc(C[C@H](NC(=O)c3c(Cl)cncc3Cl)...      P62942   \n",
       "\n",
       "     uniprot_id2  label  \n",
       "6305      P62942      0  \n",
       "6306      P62942      0  \n",
       "6307      P62942      0  \n",
       "6308      P62942      0  \n",
       "6309      P62942      0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fold5_df = dataframes['test_fold5_df']\n",
    "test_fold5_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7af288e3-c827-4ee2-8a7a-268e464ed6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done inverse mapping & merging successfully !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df_name in dataframes.keys():\n",
    "    dataframes[df_name] = convert_uniprot_ids(dataframes[df_name], uniprot_mapping)\n",
    "    dataframes[df_name] = merge_datasets(dataframes[df_name], esm_df)\n",
    "\n",
    "# Access each dataframe using its name\n",
    "train_fold1_df = dataframes['train_val_fold1_df']\n",
    "train_fold2_df = dataframes['train_val_fold2_df']\n",
    "train_fold3_df = dataframes['train_val_fold3_df']\n",
    "train_fold4_df = dataframes['train_val_fold4_df']\n",
    "train_fold5_df = dataframes['train_val_fold5_df']\n",
    "test_fold1_df = dataframes['test_fold1_df']\n",
    "test_fold2_df = dataframes['test_fold2_df']\n",
    "test_fold3_df = dataframes['test_fold3_df']\n",
    "test_fold4_df = dataframes['test_fold4_df']\n",
    "test_fold5_df = dataframes['test_fold5_df']\n",
    "\n",
    "PRINTM(f'Done inverse mapping & merging successfully !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da7103b9-1be3-430a-8932-ed343b517dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>label</th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_1270_id2</th>\n",
       "      <th>Feature_1271_id2</th>\n",
       "      <th>Feature_1272_id2</th>\n",
       "      <th>Feature_1273_id2</th>\n",
       "      <th>Feature_1274_id2</th>\n",
       "      <th>Feature_1275_id2</th>\n",
       "      <th>Feature_1276_id2</th>\n",
       "      <th>Feature_1277_id2</th>\n",
       "      <th>Feature_1278_id2</th>\n",
       "      <th>Feature_1279_id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nc1ccc(CNC(=O)NC[C@H](NC(=O)[C@@H]2CCCN2S(=O)(...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.068394</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.142344</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>-0.057618</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052764</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>-0.061765</td>\n",
       "      <td>0.122240</td>\n",
       "      <td>-0.107535</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.064282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(O)CCNC(=O)c1ccc2c(c1)C(=O)N(CCC1CCNCC1)C2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.068394</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.142344</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>-0.057618</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046108</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>-0.160452</td>\n",
       "      <td>0.058708</td>\n",
       "      <td>-0.045173</td>\n",
       "      <td>-0.135907</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>-0.136259</td>\n",
       "      <td>-0.023708</td>\n",
       "      <td>0.164988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cc1cc(C)cc(S(=O)(=O)N2CCC[C@H]2C(=O)N[C@@H](CN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.068394</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>0.142344</td>\n",
       "      <td>-0.101274</td>\n",
       "      <td>-0.057618</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052764</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>-0.032889</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>-0.061765</td>\n",
       "      <td>0.122240</td>\n",
       "      <td>-0.107535</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.064282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(O)CC(NC(=O)CCC(=O)Nc1ccc2c(c1)CNC2)c1ccccc1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>-0.061126</td>\n",
       "      <td>-0.020618</td>\n",
       "      <td>0.032968</td>\n",
       "      <td>-0.081779</td>\n",
       "      <td>-0.046594</td>\n",
       "      <td>0.086232</td>\n",
       "      <td>-0.010491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046108</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>-0.160452</td>\n",
       "      <td>0.058708</td>\n",
       "      <td>-0.045173</td>\n",
       "      <td>-0.135907</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>-0.136259</td>\n",
       "      <td>-0.023708</td>\n",
       "      <td>0.164988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N=C(N)NCCC[C@@H]1NC(=O)[C@H]2COCCN2C(=O)[C@@H]...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>-0.060721</td>\n",
       "      <td>-0.051823</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>-0.062652</td>\n",
       "      <td>-0.053298</td>\n",
       "      <td>0.085150</td>\n",
       "      <td>-0.030032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036552</td>\n",
       "      <td>-0.016408</td>\n",
       "      <td>-0.189185</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>-0.061862</td>\n",
       "      <td>-0.075560</td>\n",
       "      <td>0.090893</td>\n",
       "      <td>-0.115112</td>\n",
       "      <td>-0.029084</td>\n",
       "      <td>0.098593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  label  Feature_0  \\\n",
       "0  Nc1ccc(CNC(=O)NC[C@H](NC(=O)[C@@H]2CCCN2S(=O)(...      1   0.030220   \n",
       "1      O=C(O)CCNC(=O)c1ccc2c(c1)C(=O)N(CCC1CCNCC1)C2      1   0.030220   \n",
       "2  Cc1cc(C)cc(S(=O)(=O)N2CCC[C@H]2C(=O)N[C@@H](CN...      1   0.030220   \n",
       "3    O=C(O)CC(NC(=O)CCC(=O)Nc1ccc2c(c1)CNC2)c1ccccc1      1   0.013775   \n",
       "4  N=C(N)NCCC[C@@H]1NC(=O)[C@H]2COCCN2C(=O)[C@@H]...      1   0.016134   \n",
       "\n",
       "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0  -0.068394  -0.093040   0.142344  -0.101274  -0.057618   0.093081   \n",
       "1  -0.068394  -0.093040   0.142344  -0.101274  -0.057618   0.093081   \n",
       "2  -0.068394  -0.093040   0.142344  -0.101274  -0.057618   0.093081   \n",
       "3  -0.061126  -0.020618   0.032968  -0.081779  -0.046594   0.086232   \n",
       "4  -0.060721  -0.051823   0.028336  -0.062652  -0.053298   0.085150   \n",
       "\n",
       "   Feature_7  ...  Feature_1270_id2  Feature_1271_id2  Feature_1272_id2  \\\n",
       "0  -0.037972  ...          0.052764          0.007230         -0.032889   \n",
       "1  -0.037972  ...         -0.046108          0.018945         -0.160452   \n",
       "2  -0.037972  ...          0.052764          0.007230         -0.032889   \n",
       "3  -0.010491  ...         -0.046108          0.018945         -0.160452   \n",
       "4  -0.030032  ...         -0.036552         -0.016408         -0.189185   \n",
       "\n",
       "   Feature_1273_id2  Feature_1274_id2  Feature_1275_id2  Feature_1276_id2  \\\n",
       "0          0.013346         -0.002336         -0.061765          0.122240   \n",
       "1          0.058708         -0.045173         -0.135907          0.036617   \n",
       "2          0.013346         -0.002336         -0.061765          0.122240   \n",
       "3          0.058708         -0.045173         -0.135907          0.036617   \n",
       "4          0.029432         -0.061862         -0.075560          0.090893   \n",
       "\n",
       "   Feature_1277_id2  Feature_1278_id2  Feature_1279_id2  \n",
       "0         -0.107535         -0.056424          0.064282  \n",
       "1         -0.136259         -0.023708          0.164988  \n",
       "2         -0.107535         -0.056424          0.064282  \n",
       "3         -0.136259         -0.023708          0.164988  \n",
       "4         -0.115112         -0.029084          0.098593  \n",
       "\n",
       "[5 rows x 2562 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fold5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b68809-ab1e-49ef-902e-9dee2c145dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dataframes.items():\n",
    "    null_counts = df.isnull().sum().sum()\n",
    "    PRINTM(f'Number of nan values in {df_name} is -> {null_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e0dcf-d2f0-4e23-b5ab-1ec60d422d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f1 = generate_model(checkpoints_path, batch_size=32, dropout=0.5)\n",
    "f1_res = ft_model_f1.cross_validate(train_fold1_df, num_folds=5, num_epochs=10,\n",
    "                     batch_size=32, learning_rate=0.0001, weight_decay=1e-5,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b79d5-4961-48c4-a65d-1e647725bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f1.test_model(test_fold1_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac3d16-15a7-441d-b2e0-5a1db910e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f2 = generate_model(checkpoints_path, batch_size=32, dropout=0.5)\n",
    "f2_res = ft_model_f2.cross_validate(train_fold2_df, num_folds=5, num_epochs=10,\n",
    "                     batch_size=32, learning_rate=0.0001, weight_decay=1e-5,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bb2da-3bd2-4259-9ab0-aaca72f93af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f2.test_model(test_fold2_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87087d-093a-4300-847b-b1b7984e1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f3 = generate_model(checkpoints_path, batch_size=32, dropout=0.5)\n",
    "f3_res = ft_model_f3.cross_validate(train_fold3_df, num_folds=5, num_epochs=10,\n",
    "                     batch_size=32, learning_rate=0.0001, weight_decay=1e-5,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8174af-0dd4-4751-9f33-9adfc86f9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f3.test_model(test_fold3_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd1775-1770-4e70-b8b6-c04a9f516b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f4 = generate_model(checkpoints_path, batch_size=32, dropout=0.5)\n",
    "f4_res = ft_model_f4.cross_validate(train_fold4_df, num_folds=5, num_epochs=10,\n",
    "                     batch_size=32, learning_rate=0.0001, weight_decay=1e-5,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d377e2-c962-4631-a835-78ced5eadf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f4.test_model(test_fold4_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dff27c-4555-4a7a-95bf-f90f5df61732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f5 = generate_model(checkpoints_path, batch_size=32, dropout=0.5)\n",
    "f5_res = ft_model_f5.cross_validate(train_fold5_df, num_folds=5, num_epochs=10,\n",
    "                     batch_size=32, learning_rate=0.0001, weight_decay=1e-5,\n",
    "                     shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba86247-1899-498b-8f41-0dfaefdd96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_f5.test_model(test_fold5_df,\n",
    "                         criterion= nn.BCELoss() ,batch_size=32,\n",
    "                         shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda70b8a-026a-4c29-95c5-8bc02c5e5e7d",
   "metadata": {},
   "source": [
    "# TODO -> Section to keep ; models with data augmentation and prob in testing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e02b1c1-5b34-4c25-9bc4-a29a546cabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without attention & with prob\n",
    "class AUVG_PPI(nn.Module):\n",
    "    def __init__(self, pretrained_chemprop_model, chemberta_model, dropout):\n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.pretrained_chemprop_model = pretrained_chemprop_model\n",
    "        self.chemberta_model = chemberta_model\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1280),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1280),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1280, out_features=640),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(640),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=640, out_features=320)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=578),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(578),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=578, out_features=320)\n",
    "        )        \n",
    "\n",
    "        self.custom_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=4700 + 4700 , out_features=4700),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(4700),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=4700, out_features=2560),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2560),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=2560, out_features=1280),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1280),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1280, out_features=640),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(640),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=640, out_features=320), \n",
    "        )\n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=320)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=320 + 320 + 320 + 320 , out_features=480),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(480),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=480, out_features=300),\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=2100, out_features=1050),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1050),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1050, out_features=600), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(600),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=600, out_features=350)\n",
    "        )\n",
    "\n",
    "        self.mfp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features= 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=350)\n",
    "        )\n",
    "\n",
    "        self.smiles_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=350 + 384 + 350 + 194, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=300)\n",
    "        )\n",
    "\n",
    "        # Additional layrs in order to concatinate chemprop fingerprints, chemBERTa embeddings & ppi features all together\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=300 + 300, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, bmg, esm, custom, fegs, gae,\n",
    "                input_ids, attention_mask,\n",
    "                morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.pretrained_chemprop_model(bmg)\n",
    "        cp_fingerprints = self.fp_mlp(cp_fingerprints)\n",
    "\n",
    "        # Forward pass ids & attention mask in through chemBERTa pretrained model in order to get embeddings\n",
    "        chemberta_embeddings = self.chemberta_model(input_ids, attention_mask)\n",
    "        \n",
    "        # Forward pass SMILES morgan fingerprints embeddings through MLP layer, then concate them with\n",
    "        # chemprop embeddings and SMILES molecular descriptors embeddings, and pass all together through another\n",
    "        # MLP layer\n",
    "        mfp = self.mfp_mlp(morgan_fingerprints)\n",
    "        combined_smiles_features_embeddings = torch.cat([cp_fingerprints,chemberta_embeddings, mfp, chemical_descriptors], dim=1).to(device)\n",
    "        smiles_embeddings = self.smiles_mlp(combined_smiles_features_embeddings)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        #ppi_features = proteins.to(device)\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        custom_embeddings = self.custom_mlp(custom)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "        combined_ppi_features_embeddings = torch.cat([esm_embeddings, custom_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)\n",
    "        ppi_features = self.ppi_mlp(combined_ppi_features_embeddings)\n",
    "\n",
    "        # Concatinate chemprop fingerprints embeddings, chemberta embeddings and PPI embeddings together into one tensor\n",
    "        # Afterwards, pass them through MLP layer and make prediction\n",
    "        combined_embeddings = torch.cat([smiles_embeddings, ppi_features], dim=1).to(device)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "    def train_model(self, num_epochs, train_loader, val_loader, optimizer, criterion, device):\n",
    "        PRINTM(f'Start training !')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_smiles, _, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc, batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_smiles, batch_esm_features,batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc)\n",
    "\n",
    "                #outputs = self(batch_smiles, batch_protein_features, batch_input_ids, batch_attention_mas)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                #running_loss += loss.item()\n",
    "                #if i % 100 == 99 and i > 0:\n",
    "                    #print(f\"Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100:.4f}\")\n",
    "                    #running_loss = 0.0\n",
    "    \n",
    "            # Validate the model on the validation set\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "            PRINTC()\n",
    "            print(f\"Epoch: {epoch+1}\")\n",
    "            print(f\"Validation BCELoss: {val_loss:.5f}\")\n",
    "            print(f\"Validation Accuracy (>0.8): {val_accuracy:.2f}\")\n",
    "            print(f\"Validation AUC: {val_auc:.5f}\")\n",
    "            print(f\"Epoch time: {epoch_time:.2f} minutes\")\n",
    "            PRINTC()\n",
    "    \n",
    "        print(\"Finish training !\")\n",
    "\n",
    "    def test_model(self, test_dataset, criterion, batch_size, shuffle, device):\n",
    "        test_dataset = MoleculeDataset(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        self.eval()\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        # Initialize prob_df\n",
    "        prob_df = pd.DataFrame(columns=['uniprot_id1', 'uniprot_id2', 'output_prob', 'label'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, batch_uniprots_tuple, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc, batch_labels) in test_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "    \n",
    "                outputs = self(batch_smiles, batch_esm_features, batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc)\n",
    "    \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "                # Get the output probabilities and labels\n",
    "                output_probs = outputs.squeeze().cpu().numpy()\n",
    "                labels = batch_labels.cpu().numpy()\n",
    "        \n",
    "                # Add to prob_df\n",
    "                for i in range(len(batch_uniprots_tuple)):\n",
    "                    uniprot_id1, uniprot_id2 = batch_uniprots_tuple[i][0], batch_uniprots_tuple[i][1]\n",
    "                    prob_df = prob_df.append({\n",
    "                        'uniprot_id1': uniprot_id1,\n",
    "                        'uniprot_id2': uniprot_id2,\n",
    "                        'output_prob': output_probs[i],\n",
    "                        'label': labels[i]\n",
    "                    }, ignore_index=True)\n",
    "        \n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "        # calc average probabilities for duplicate UniProt ID pairs\n",
    "        prob_df['pair'] = prob_df.apply(lambda row: tuple(sorted([row['uniprot_id1'], row['uniprot_id2']])), axis=1)\n",
    "        avg_prob_df = prob_df.groupby('pair').agg({'output_prob': 'mean', 'label': 'first'}).reset_index()\n",
    "    \n",
    "        # extract final all_labels and all_outputs\n",
    "        all_labels = avg_prob_df['label'].tolist()\n",
    "        all_outputs = avg_prob_df['output_prob'].tolist()\n",
    "    \n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        test_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        \n",
    "        print(f\"Test BCELoss: {test_loss:.5f}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Test AUC: {test_auc:.5f}\")\n",
    "    \n",
    "        return prob_df, all_labels, all_outputs\n",
    "\n",
    "    def validate_model(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, _, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "    \n",
    "                outputs = self(batch_smiles, batch_esm_features,batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas,\n",
    "                              batch_morgan, batch_chem_desc)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                all_labels.extend(batch_labels.cpu().numpy())  \n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())  \n",
    "    \n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_auc = roc_auc_score(all_labels, all_outputs)  \n",
    "        return val_loss, accuracy, val_auc\n",
    "\n",
    "    def cross_validate(self, dataset,\n",
    "                       num_folds=5,num_epochs=10,\n",
    "                       batch_size=32,\n",
    "                       learning_rate=0.0001, weight_decay=1e-5,\n",
    "                       shuffle=True, device='cuda'):\n",
    "        kf = KFold(n_splits=num_folds, shuffle=shuffle)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            \n",
    "            print(f\"Fold {fold+1}/{num_folds}\")\n",
    "            \n",
    "            # Split dataset\n",
    "            train_subset = dataset.iloc[train_idx].reset_index(drop=True)\n",
    "            val_subset = dataset.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "            train_dataset = MoleculeDataset(train_subset)\n",
    "            val_dataset = MoleculeDataset(val_subset)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "            \n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            \n",
    "            self.train_model(num_epochs, train_loader, val_loader, optimizer, criterion, device)\n",
    "            \n",
    "            # Validate the model\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            fold_results.append((val_loss, val_accuracy, val_auc))\n",
    "\n",
    "            PRINTC()\n",
    "            print(f\"Fold {fold+1} - Validation BCELoss: {val_loss:.5f}, Accuracy: {val_accuracy:.2f}, AUC: {val_auc:.5f}\")\n",
    "            PRINTC()\n",
    "            \n",
    "        avg_val_loss = sum([result[0] for result in fold_results]) / num_folds\n",
    "        avg_val_accuracy = sum([result[1] for result in fold_results]) / num_folds\n",
    "        avg_val_auc = sum([result[2] for result in fold_results]) / num_folds\n",
    "        \n",
    "        print(f\"\\nAverage Validation BCELoss: {avg_val_loss:.5f}\")\n",
    "        print(f\"Average Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "        print(f\"Average Validation AUC: {avg_val_auc:.5f}\")\n",
    "        \n",
    "        return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c940f-566b-445f-bb21-7818901a2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Attention - v2_1 with prob\n",
    "class AUVG_PPI(nn.Module):\n",
    "    def __init__(self, pretrained_chemprop_model, chemberta_model, dropout):\n",
    "        \n",
    "        super(AUVG_PPI, self).__init__()\n",
    "        self.pretrained_chemprop_model = pretrained_chemprop_model\n",
    "        self.chemberta_model = chemberta_model\n",
    "        self.dropout = dropout\n",
    "        self.ppi_self_attention = custom_self_attention(512, 8, 0.2)\n",
    "        self.smiles_self_attention = custom_self_attention(128, 4, 0.2)\n",
    "        self.cross_attention = nn.MultiheadAttention(512, 8, 0.2)\n",
    "        self.max_pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # PPI Features MLP layers: (esm, custom, fegs, gae)\n",
    "        self.esm_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1280 + 1280 , out_features=1750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1750, out_features=1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1000, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.fegs_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=578 + 578, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )        \n",
    "\n",
    "        self.custom_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=4700 + 4700 , out_features=8000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=8000, out_features=6500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(6500),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=6500, out_features=5000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(5000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=5000, out_features=3500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(3500),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=3500, out_features=2000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2000),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=2000, out_features=1028),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1028),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1028, out_features=512)\n",
    "        )\n",
    "\n",
    "        self.gae_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=500 + 500, out_features=750),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(750),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=750, out_features=512)\n",
    "        )\n",
    "\n",
    "        # MLP for ppi_features\n",
    "        self.ppi_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 4 , out_features= 1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512)\n",
    "        )\n",
    "        \n",
    "        self.fp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=2100, out_features=1536),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1536, out_features=1024), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128)\n",
    "        )\n",
    "\n",
    "        self.mfp_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features= 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128)\n",
    "        )\n",
    "\n",
    "        self.chemberta_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = 384, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128)\n",
    "        )\n",
    "\n",
    "        self.chem_descriptors_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = 194, out_features=128)\n",
    "        )\n",
    "\n",
    "        # Additional layrs in order to concatinate chemprop fingerprints, chemBERTa embeddings & ppi features all together\n",
    "        self.additional_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=256 + 256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, bmg, esm, custom, fegs, gae,\n",
    "                input_ids, attention_mask,\n",
    "                morgan_fingerprints, chemical_descriptors):\n",
    "        # Forward pass batch mol graph through pretrained chemprop model in order to get fingerprints embeddings\n",
    "        # Afterwards, pass the fingerprints through MLP layer\n",
    "        cp_fingerprints = self.pretrained_chemprop_model(bmg)\n",
    "        cp_fingerprints = self.fp_mlp(cp_fingerprints)\n",
    "\n",
    "        chemberta_embeddings = self.chemberta_model(input_ids, attention_mask)\n",
    "        chemberta_embeddings = self.chemberta_mlp(chemberta_embeddings)\n",
    "        mfp = self.mfp_mlp(morgan_fingerprints)\n",
    "        chemical_descriptors = self.chem_descriptors_mlp(chemical_descriptors)\n",
    "        \n",
    "        # Concatenate all 4 smiles embeddings along a new dimension (4x194) & pass them throw self-attention layer\n",
    "        smiles_embeddings = torch.stack([cp_fingerprints,chemberta_embeddings, mfp, chemical_descriptors], dim=1).to(device)  # shape ->> (batch_size, 4, 194)\n",
    "        smiles_features = self.smiles_self_attention(smiles_embeddings)\n",
    "        smiles_embeddings = smiles_features.unsqueeze(1)\n",
    "\n",
    "        # Pass all PPI features  through MLP layers, and then pass them all together into another MLP layer\n",
    "        #ppi_features = proteins.to(device)\n",
    "        esm_embeddings = self.esm_mlp(esm)\n",
    "        custom_embeddings = self.custom_mlp(custom)\n",
    "        fegs_embeddings = self.fegs_mlp(fegs)\n",
    "        gae_embeddings = self.gae_mlp(gae)\n",
    "\n",
    "        # Concatenate all 4 ppi embeddings along a new dimension (4x320) & pass them throw self-attention layer\n",
    "        ppi_embeddings = torch.stack([esm_embeddings, custom_embeddings, fegs_embeddings, gae_embeddings], dim=1).to(device)  # shape ->> (batch_size, 4, 320)\n",
    "        ppi_features = self.ppi_self_attention(ppi_embeddings)\n",
    "        ppi_features = self.ppi_mlp(ppi_features).unsqueeze(1)\n",
    "\n",
    "        #Cross-attention between smiles and PPI to capture the interaction relationships\n",
    "        ppi_QKV = ppi_features.permute(1, 0, 2)\n",
    "        smiles_QKV = smiles_embeddings.permute(1, 0, 2)\n",
    "        \n",
    "        smiles_att, _ = self.cross_attention(smiles_QKV, ppi_QKV, ppi_QKV)\n",
    "        ppi_att, _ = self.cross_attention(ppi_QKV, smiles_QKV, smiles_QKV)\n",
    "\n",
    "        # permute attention outputrs to match (batch_size, embed_dim, num_heads) shape\n",
    "        smiles_attn_output = (0.5* smiles_att.permute(1, 2, 0)) + (0.5* smiles_embeddings.permute(0, 2, 1))  # Add (residual connection) & apply weighted residual connection \n",
    "        ppi_attn_output = (0.5* ppi_att.permute(1, 2, 0)) + (0.5* ppi_features.permute(0, 2, 1))  # Add (residual connection) & apply weighted residual connection \n",
    "\n",
    "        # Drop the last dim in order to get (batch_size, embed_dim) & \n",
    "        # Pass cross-attention norm outputs throw max-pool layer before passing throw MLP layers\n",
    "        smiles_att = self.max_pool(smiles_attn_output.squeeze(2))\n",
    "        ppi_att = self.max_pool(ppi_attn_output.squeeze(2)) \n",
    "        combined_embeddings = torch.cat([smiles_att, ppi_att], dim=1)\n",
    "        output = self.additional_layers(combined_embeddings)\n",
    "        \n",
    "        return self.sigmoid(output)\n",
    "\n",
    "    def train_model(self, num_epochs, train_loader, val_loader, optimizer, criterion, device):\n",
    "        PRINTM(f'Start training !')\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for (batch_smiles, _, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc, batch_labels) in train_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_smiles, batch_esm_features,batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas,batch_morgan, batch_chem_desc)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "            # Validate the model on the validation set\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            end_time = time.time()\n",
    "            epoch_time = (end_time - start_time) / 60\n",
    "            PRINTC()\n",
    "            print(f\"Epoch: {epoch+1}\")\n",
    "            print(f\"Validation BCELoss: {val_loss:.5f}\")\n",
    "            print(f\"Validation Accuracy (>0.8): {val_accuracy:.2f}\")\n",
    "            print(f\"Validation AUC: {val_auc:.5f}\")\n",
    "            print(f\"Epoch time: {epoch_time:.2f} minutes\")\n",
    "            PRINTC()\n",
    "    \n",
    "        print(\"Finish training !\")\n",
    "\n",
    "    def test_model(self, test_dataset, criterion, batch_size, shuffle, device):\n",
    "        test_dataset = MoleculeDataset(test_dataset)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        self.eval()\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        # Initialize prob_df\n",
    "        prob_df = pd.DataFrame(columns=['uniprot_id1', 'uniprot_id2', 'output_prob', 'label'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, batch_uniprots_tuple, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc, batch_labels) in test_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "    \n",
    "                outputs = self(batch_smiles, batch_esm_features, batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc)\n",
    "    \n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "                # Get the output probabilities and labels\n",
    "                output_probs = outputs.squeeze().cpu().numpy()\n",
    "                labels = batch_labels.cpu().numpy()\n",
    "        \n",
    "                # Add to prob_df\n",
    "                for i in range(len(batch_uniprots_tuple)):\n",
    "                    uniprot_id1, uniprot_id2 = batch_uniprots_tuple[i][0], batch_uniprots_tuple[i][1]\n",
    "                    prob_df = prob_df.append({\n",
    "                        'uniprot_id1': uniprot_id1,\n",
    "                        'uniprot_id2': uniprot_id2,\n",
    "                        'output_prob': output_probs[i],\n",
    "                        'label': labels[i]\n",
    "                    }, ignore_index=True)\n",
    "        \n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "        # calc average probabilities for duplicate UniProt ID pairs (ensures only one pair is taken into consideration)\n",
    "        prob_df['pair'] = prob_df.apply(lambda row: tuple(sorted([row['uniprot_id1'], row['uniprot_id2']])), axis=1)\n",
    "        avg_prob_df = prob_df.groupby('pair').agg({'output_prob': 'mean', 'label': 'first'}).reset_index()\n",
    "    \n",
    "        # extract final all_labels and all_outputs\n",
    "        all_labels = avg_prob_df['label'].tolist()\n",
    "        all_outputs = avg_prob_df['output_prob'].tolist()\n",
    "    \n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        test_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        \n",
    "        print(f\"Test BCELoss: {test_loss:.5f}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Test AUC: {test_auc:.5f}\")\n",
    "    \n",
    "        return prob_df, test_auc\n",
    "\n",
    "    def validate_model(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for (batch_smiles, _, batch_esm_features, batch_custom_features, batch_fegs_features, batch_gae_features,\n",
    "                 batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc ,batch_labels) in val_loader:\n",
    "                # Move tensors to the configured device\n",
    "                batch_attention_mas = batch_attention_mas.to(device)\n",
    "                batch_input_ids = batch_input_ids.to(device)\n",
    "                batch_esm_features = batch_esm_features.to(device)\n",
    "                batch_custom_features = batch_custom_features.to(device)\n",
    "                batch_fegs_features = batch_fegs_features.to(device)\n",
    "                batch_gae_features = batch_gae_features.to(device)\n",
    "                batch_morgan = batch_morgan.to(device)\n",
    "                batch_chem_desc = batch_chem_desc.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "    \n",
    "                outputs = self(batch_smiles, batch_esm_features,batch_custom_features,\n",
    "                               batch_fegs_features, batch_gae_features, batch_input_ids, batch_attention_mas, batch_morgan, batch_chem_desc)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                all_labels.extend(batch_labels.cpu().numpy())  \n",
    "                all_outputs.extend(outputs.squeeze().cpu().numpy())  \n",
    "    \n",
    "                predicted = (outputs.squeeze() > 0.8).float()\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_auc = roc_auc_score(all_labels, all_outputs)  \n",
    "        return val_loss, accuracy, val_auc\n",
    "\n",
    "    def cross_validate(self, dataset, num_folds=5,num_epochs=10, batch_size=32, learning_rate=0.0001, weight_decay=1e-5, shuffle=True, device='cuda'):\n",
    "        kf = KFold(n_splits=num_folds, shuffle=shuffle)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "            \n",
    "            print(f\"Fold {fold+1}/{num_folds}\")\n",
    "            \n",
    "            # Split dataset\n",
    "            train_subset = dataset.iloc[train_idx].reset_index(drop=True)\n",
    "            val_subset = dataset.iloc[val_idx].reset_index(drop=True)\n",
    "            \n",
    "            train_dataset = MoleculeDataset(train_subset)\n",
    "            val_dataset = MoleculeDataset(val_subset)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "            \n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            \n",
    "            self.train_model(num_epochs, train_loader, val_loader, optimizer, criterion, device)\n",
    "            \n",
    "            # Validate the model\n",
    "            val_loss, val_accuracy, val_auc = self.validate_model(val_loader, criterion, device)\n",
    "            fold_results.append((val_loss, val_accuracy, val_auc))\n",
    "\n",
    "            PRINTC()\n",
    "            print(f\"Fold {fold+1} - Validation BCELoss: {val_loss:.5f}, Accuracy: {val_accuracy:.2f}, AUC: {val_auc:.5f}\")\n",
    "            PRINTC()\n",
    "            \n",
    "        avg_val_loss = sum([result[0] for result in fold_results]) / num_folds\n",
    "        avg_val_accuracy = sum([result[1] for result in fold_results]) / num_folds\n",
    "        avg_val_auc = sum([result[2] for result in fold_results]) / num_folds\n",
    "        \n",
    "        print(f\"\\nAverage Validation BCELoss: {avg_val_loss:.5f}\")\n",
    "        print(f\"Average Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "        print(f\"Average Validation AUC: {avg_val_auc:.5f}\")\n",
    "        \n",
    "        return fold_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop v2",
   "language": "python",
   "name": "chemprop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
