{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wui7sMVJP0R5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, AdamW, get_linear_schedule_with_warmup , BertModel\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/sise/home/adamu/thesis_new/datasets/PPIMI_datastes_my_samping/final_dataset_2_0.8_Multi_PPIMI.csv\")\n",
        "esm_features = pd.read_csv(\"/sise/home/adamu/thesis_new/feature_extraction/outputs/esm_features.csv\")\n",
        "uniprot_mapping = pd.read_csv(\"/sise/home/adamu/thesis_new/feature_extraction/data/idmapping_2024_06_04.tsv\",  delimiter = \"\\t\")\n",
        "uniprot_mapping"
      ],
      "metadata": {
        "id": "1v1XylO_P1UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_uniprot_ids(dataset, mapping_df):\n",
        "    # Create a dictionary from the mapping dataframe\n",
        "    mapping_dict = mapping_df.set_index('From')['Entry'].to_dict()\n",
        "\n",
        "    # Map the uniprot_id1 and uniprot_id2 columns to their respective Entry values\n",
        "    dataset['uniprot_id1'] = dataset['uniprot_id1'].map(mapping_dict)\n",
        "    dataset['uniprot_id2'] = dataset['uniprot_id2'].map(mapping_dict)\n",
        "    return dataset.drop_duplicates()\n",
        "\n",
        "def merge_datasets(dataset, features_df):\n",
        "    # Merge features for uniprot_id1\n",
        "    dataset = dataset.merge(features_df, how='left', left_on='uniprot_id1', right_on='UniProt_ID', suffixes=('', '_id1'))\n",
        "    dataset = dataset.drop(columns=['UniProt_ID'])\n",
        "\n",
        "    # Merge features for uniprot_id2\n",
        "    features_df_renamed = features_df.add_suffix('_id2')\n",
        "    features_df_renamed = features_df_renamed.rename(columns={'UniProt_ID_id2': 'UniProt_ID'})\n",
        "    dataset = dataset.merge(features_df_renamed, how='left', left_on='uniprot_id2', right_on='UniProt_ID', suffixes=('', '_id2'))\n",
        "    dataset = dataset.drop(columns=['UniProt_ID', \"uniprot_id1\", \"uniprot_id2\"])\n",
        "\n",
        "    return dataset.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "uIU1-tkCP4Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features_to_dataframe(dataset):\n",
        "    dataset = convert_uniprot_ids(dataset, uniprot_mapping)\n",
        "    dataset = merge_datasets(dataset, esm_features)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "VwG4WGwkP6V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = add_features_to_dataframe(dataset)"
      ],
      "metadata": {
        "id": "mGgRGHYuP8cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_dict = dict()\n",
        "my_splits_path = \"/sise/home/adamu/thesis_new/datasets/folds/my folds/cold start PPI/\"\n",
        "for fold in range(1,6):\n",
        "    train_file = os.path.join(my_splits_path, f'train_fold{fold}.csv')\n",
        "    test_file = os.path.join(my_splits_path, f'test_fold{fold}.csv')\n",
        "    train_df_current = pd.read_csv(train_file)\n",
        "    test_df_current = pd.read_csv(test_file)\n",
        "    print(f\"shape of train in fold {fold} before adding features: {train_df_current.shape}\")\n",
        "    print(f\"shape of test in fold {fold} before adding features: {test_df_current.shape}\")\n",
        "    train_df_current = add_features_to_dataframe(train_df_current)\n",
        "    test_df_current = add_features_to_dataframe(test_df_current)\n",
        "    print(f\"shape of train in fold {fold} after adding features: {train_df_current.shape}\")\n",
        "    print(f\"shape of test in fold {fold} after adding features: {test_df_current.shape}\")\n",
        "    train_test_dict[f\"fold{fold}\"] = (train_df_current, test_df_current)"
      ],
      "metadata": {
        "id": "QZl8DP_eP_XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train, test) in train_test_dict.items():\n",
        "    print(f\"{fold}\")\n",
        "    rows_with_nan = test[test.isna().any(axis=1)]\n",
        "    if not rows_with_nan.empty:\n",
        "        nan_cols = rows_with_nan.columns[rows_with_nan.isna().any()].tolist() + [\"uniprot_id1\", \"uniprot_id2\"]\n",
        "        print(rows_with_nan[nan_cols])\n",
        "    else:\n",
        "        print(\"No NaN values found in this fold.\")\n",
        "    rows_with_nan = train[train.isna().any(axis=1)]\n",
        "    if not rows_with_nan.empty:\n",
        "        nan_cols = rows_with_nan.columns[rows_with_nan.isna().any()].tolist() + [\"uniprot_id1\", \"uniprot_id2\"]\n",
        "        print(rows_with_nan[nan_cols])\n",
        "    else:\n",
        "        print(\"No NaN values found in this fold.\")"
      ],
      "metadata": {
        "id": "WzsDGWiBQCy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_dict[f\"fold1\"][0]"
      ],
      "metadata": {
        "id": "u7R5VRS3QDg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encoded_smiles, ppi_features, labels):\n",
        "        self.encoded_smiles = encoded_smiles\n",
        "        self.ppi_features = torch.tensor(ppi_features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        # Debugging print statements\n",
        "        print(f\"Encoded SMILES size: {len(self.encoded_smiles['input_ids'])}\")\n",
        "        print(f\"PPI Features size: {self.ppi_features.shape}\")\n",
        "        print(f\"Labels size: {self.labels.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            \"input_ids\": self.encoded_smiles[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encoded_smiles[\"attention_mask\"][idx],\n",
        "            \"ppi_features\": self.ppi_features[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "        return item\n",
        "\n",
        "class ChemBERTaWithPPI(nn.Module):\n",
        "    def __init__(self, model_name, ppi_feature_size, hidden_size1=1024, hidden_size2=512, hidden_size3 = 256):\n",
        "        super(ChemBERTaWithPPI, self).__init__()\n",
        "        self.chemberta = RobertaModel.from_pretrained(model_name)\n",
        "        self.ppi_fc = nn.Linear(ppi_feature_size, hidden_size1)  # Ensure output matches BERT's dimension\n",
        "        self.hidden_layer1 = nn.Linear(1408, hidden_size1)  # First hidden layer\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size1, hidden_size2)  # New added hidden layer\n",
        "        self.hidden_layer3 = nn.Linear(hidden_size2, hidden_size3)  # New added hidden layer\n",
        "        self.classifier = nn.Linear(hidden_size3, 1)  # Adjusted for the output of the second hidden layer\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, ppi_features):\n",
        "        ppi_out = self.ppi_fc(ppi_features)\n",
        "\n",
        "        # Expand the dimensions of ppi_out to match bert_output[0]\n",
        "        ppi_out = ppi_out.unsqueeze(1).expand(-1, input_ids.size(1), -1)\n",
        "\n",
        "        bert_output = self.chemberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        integrated_output = torch.cat((bert_output[0], ppi_out), dim=-1)  # Concatenate along the last dimension\n",
        "\n",
        "        # Average pooling over the sequence length dimension\n",
        "        pooled_output = integrated_output.mean(dim=1)\n",
        "\n",
        "        # Passing through the first hidden layer with a ReLU activation function\n",
        "        hidden_output1 = F.relu(self.hidden_layer1(pooled_output))\n",
        "\n",
        "        # Passing through the second hidden layer with a ReLU activation function\n",
        "        hidden_output2 = F.relu(self.hidden_layer2(hidden_output1))\n",
        "\n",
        "        # Passing through the second hidden layer with a ReLU activation function\n",
        "        hidden_output3 = F.relu(self.hidden_layer3(hidden_output2))\n",
        "\n",
        "        logits = self.classifier(hidden_output3)\n",
        "        return logits\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "for fold in range(3, 6):\n",
        "    print(\"fold:\", fold)\n",
        "    # Load your data\n",
        "    train_df = train_test_dict[f\"fold{fold}\"][0]\n",
        "    valid_df = train_test_dict[f\"fold{fold}\"][1]\n",
        "    len_train = len(train_df)\n",
        "    len_val = len(valid_df)\n",
        "    dataset = pd.concat([train_df, valid_df], axis=0).reset_index(drop=True)\n",
        "    train_df = dataset.iloc[:len_train, :]\n",
        "    test_df = dataset.iloc[len_train:, :]\n",
        "    dataset.iloc[:, 2:] = dataset.iloc[:, 2:].astype(float)\n",
        "    train_df.iloc[:, 2:] = train_df.iloc[:, 2:].astype(float)\n",
        "    valid_df.iloc[:, 2:] = valid_df.iloc[:, 2:].astype(float)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    # Extract SMILES strings, PPI features, and labels\n",
        "    smiles_list = dataset['smiles'].tolist()\n",
        "    ppi_features = dataset.iloc[:, 2:].values  # Excluding SMILES and label columns\n",
        "    train_labels = train_df[\"label\"].values\n",
        "    valid_labels = valid_df[\"label\"].values\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
        "    encoded_smiles = tokenizer(smiles_list, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Convert these splits into their respective Datasets and DataLoaders\n",
        "    train_dataset = CustomDataset(tokenizer(train_df['smiles'].tolist(), truncation=True, padding=True, return_tensors=\"pt\"), train_df.iloc[:, 2:].values, train_labels)\n",
        "    valid_dataset = CustomDataset(tokenizer(valid_df['smiles'].tolist(), truncation=True, padding=True, return_tensors=\"pt\"), valid_df.iloc[:, 2:].values, valid_labels)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Check lengths of DataLoader and Dataset\n",
        "    print(f\"Length of train DataLoader: {len(train_dataloader.dataset)}\")\n",
        "    print(f\"Length of valid DataLoader: {len(valid_dataloader.dataset)}\")\n",
        "\n",
        "    # Assuming you have your data loaded in a DataLoader named `dataloader`\n",
        "    model_name = \"DeepChem/ChemBERTa-77M-MTR\"\n",
        "    model = ChemBERTaWithPPI(model_name, ppi_feature_size=2560).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    num_epochs = 50\n",
        "    patience = 11\n",
        "    best_auc = 0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=True)\n",
        "        for batch in progress_bar:\n",
        "            # Move batch data to the chosen device\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            ppi_features = batch[\"ppi_features\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids, attention_mask, ppi_features)\n",
        "            loss = criterion(logits.squeeze(-1), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Optionally, update the progress bar description with the current loss\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_dataloader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                ppi_features = batch[\"ppi_features\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                logits = model(input_ids, attention_mask, ppi_features)\n",
        "                predictions = torch.sigmoid(logits).squeeze(-1)\n",
        "                all_preds.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - Validation AUC: {auc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            epochs_without_improvement = 0\n",
        "            # Optionally, save the best model\n",
        "            # torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement == patience:\n",
        "                print(\"Early stopping due to no improvement in validation AUC.\")\n",
        "                break\n"
      ],
      "metadata": {
        "id": "qGKzg1NTQIF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}