





import os
import pickle  # In order to save DataFrame dictionary

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from joblib import dump, load  # For saving & loading trained models

from sklearn.metrics import (
    roc_curve,
    auc,
    roc_auc_score,
    make_scorer,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import label_binarize
from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.utils import resample  # For executing bootstrap to get more accurate roc_auc_score when testing best models

import xgboost as xgb

from rdkit import Chem
from rdkit.Chem import AllChem, PandasTools, Descriptors, rdmolops

import deepchem as dc
from deepchem.feat import RDKitDescriptors
from deepchem.models import GraphConvModel
from deepchem.hyper import GridHyperparamOpt, HyperparamOpt
from deepchem.splits.splitters import RandomGroupSplitter
from deepchem.trans import undo_transforms
from deepchem.trans.transformers import BalancingTransformer

import warnings
warnings.filterwarnings('ignore')





def PRINT(text) -> None: print(f"{'~'*80}\n{text}\n{'~'*80}")

def is_numeric(value):
    """
    Checks if a given value can be converted to a float, indicating numeric nature.

    Parameters:
    - value: Any value to check.
    """
    try:
        float(value)
        return True
    except (ValueError, TypeError):
        return False

def print_dict_meaningful(dictionary):
    """
    Prints key-value pairs of a dictionary, formatting numeric values to 3 decimal places.

    Parameters:
    - dictionary: A dictionary to print.
    """
    for key, value in dictionary.items():
        if is_numeric(value):
            formatted_value = "{:.3f}".format(float(value))
        else:
            formatted_value = value
        print(f'{key}: {formatted_value}')

def plot_uniprot_numeric_label_frequency(df, UniProt) -> None:
    """
    Plots a countplot of NumericUniProtTargetLabels for a specific UniProt ID.

    Parameters:
    - df: DataFrame containing the data.
    - UniProt: UniProt ID for which the countplot is generated.e
    """
    plt.figure(figsize=(10, 6))
    sns.countplot(x='NumericUniProtTargetLabels', data=df)
    plt.title(f'Countplot of NumericUniProtTargetLabels for UniProt {UniProt}')
    plt.xlabel('NumericUniProtTargetLabels')
    plt.ylabel('Frequency')
    plt.show()








pwd


pred_dataset_path = "data/dataset_for_prediction.csv"
ChEMBL_integrin_modified_dataset_path = "data/integrins_chembl_corrected.csv"


pred_df = pd.read_csv(pred_dataset_path)

pred_df.head(5)





pred_df = pred_df.rename(columns={'uniprot_id1':'uniprot_id'})

PRINT(f'Renamed column name: {pred_df.columns[1]}')


chmbl_df = pd.read_csv(ChEMBL_integrin_modified_dataset_path)

chmbl_df.head(5)








unique_proteins = pred_df["uniprot_id"].unique()


PRINT(f"The unique proteins we want to predict their partners in the PPI are :\n {unique_proteins}\n\nThe are total {len(unique_proteins)} such proteins")





# 3 proteins we need to fix their models & predictions due to problem in data scraping phase
unique_proteins = ['P05556', 'P05106', 'P08648']


protein_dataframes = {}

for protein in unique_proteins:
    # Initialize an empty list to store rows for the current protein
    rows_for_protein = []

    # Iterate over each row in the ChEMBL DataFrame
    for index, row in chmbl_df.iterrows():
        # Check if the current protein is present in any of the UniProt columns
        if protein in row[['UniProt1', 'UniProt2', 'UniProt3', 'UniProt4', 'UniProt5']].values:
            # Determine the correct order (UniProt1 and UniProt2) in the new data frame
            if row['UniProt1'] == protein:
                relevant_info = [row['Canonical SMILES(RDKit)'], row['UniProt1'], row['UniProt2']]
            elif row['UniProt2'] == protein:
                relevant_info = [row['Canonical SMILES(RDKit)'], row['UniProt2'], row['UniProt1']]
            else:
                relevant_info = []

            if relevant_info:
                rows_for_protein.append(relevant_info)
                
    if rows_for_protein:
        protein_dataframes[protein] = pd.DataFrame(rows_for_protein, columns=['SMILES', 'UniProt1', 'UniProt2'])



protein_dataframes['P05556'].head()





directory_path = 'obj'

# Save the dictionary to a file in the specified directory
with open(os.path.join(directory_path, 'data_frames_dictionary_modified.pkl'), 'wb') as file:
    pickle.dump(protein_dataframes, file)





out_dir = 'data/modified/unique UniProt csv files'


for protein, df in protein_dataframes.items():
    try:
        # Generate csv file name with the desired format
        file_name = f'{protein}.csv'

        # Specify full path
        out_path = os.path.join(out_dir, file_name)

        # Save current data frame as csv file
        df.to_csv(out_path, index=False)

        PRINT(f'Saved data frame for {protein} as {file_name}')
        
    except Exception as e:
        PRINT(f'Error!\nVerify path name and the data')
    





PRINT(f'We have {len(protein_dataframes.items())} data frames to visualize information about their data distributions')


PRINT(f'UniProt_ids -> {unique_proteins}')








def one_hot_encoding(df):
    
    df_encoded = pd.get_dummies(df[['UniProt1', 'UniProt2']], prefix='', prefix_sep='').astype(int)
    df_encoded = pd.concat([df[['SMILES']], df_encoded], axis=1)
    return df_encoded





def visualize_dist(df, target_prot)-> None:
    # Melt the DataFrame to long format for Seaborn countplot
    
    df_melted = df.melt(var_name='Protein', value_name='Interaction Status')
    df_melted['Interaction Status'] = df_melted['Interaction Status'].astype(str)
    
    # Set the size of the plot
    sns.set(rc={'figure.figsize':(12, 8)})
    
    sns.set_context("notebook", rc={"lines.linewidth": 2.5})
    # Create a grouped count plot
    sns.countplot(x='Protein', hue='Interaction Status', palette=["lightgrey","skyblue"], data=df_melted)
    
    # Add labels and title
    plt.xlabel('Protein')
    plt.ylabel('Count')
    plt.title(f'PPI with -> {target_prot}')

    sns.despine()
    sns.set_theme(style="whitegrid")
    sns.despine(offset=10, trim=True)
    sns.set_context("notebook")
    plt.show()





def filter_proteins_list(df, columns_to_remove):
    
    filtered_columns = [col for col in df.columns if col not in columns_to_remove]
    filtered_columns_list = list(filtered_columns)
    return filtered_columns





first_df = protein_dataframes[unique_proteins[0]]

first_df.head(2)





first_df_encoded = one_hot_encoding(first_df)


print(first_df_encoded.columns)


first_df_encoded.head(3)


filtered_columns = filter_proteins_list(first_df_encoded, columns_to_remove = ['SMILES', 'P05556'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_1 = first_df_encoded[filtered_columns]

temp_df_1.head(2)


visualize_dist(temp_df_1, unique_proteins[0])





PRINT(f'The size of the data frame is -> {len(first_df)}')
print(f'Number of times O75578 appears -> {len(first_df[first_df["UniProt2"] == "O75578"])}')
print(f'Number of times P06756 appears -> {len(first_df[first_df["UniProt2"] == "P06756"])}')
print(f'Number of times P08648 appears -> {len(first_df[first_df["UniProt2"] == "P08648"])}')
print(f'Number of times P13612 appears -> {len(first_df[first_df["UniProt2"] == "P13612"])}')
print(f'Number of times P17301 appears -> {len(first_df[first_df["UniProt2"] == "P17301"])}')
print(f'Number of times P23229 appears -> {len(first_df[first_df["UniProt2"] == "P23229"])}')
print(f'Number of times P56199 appears -> {len(first_df[first_df["UniProt2"] == "P56199"])}')
print(f'Number of times Q13797 appears -> {len(first_df[first_df["UniProt2"] == "Q13797"])}')

PRINT('Done.')





second_df = protein_dataframes[unique_proteins[1]]

second_df.head(2)





second_df_encoded = one_hot_encoding(second_df)


second_df_encoded.columns


second_df_encoded.head(3)


filtered_columns = filter_proteins_list(second_df_encoded, columns_to_remove = ['SMILES', 'P05106'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_2 = second_df_encoded[filtered_columns]

temp_df_2.head(5)


visualize_dist(temp_df_2, unique_proteins[1])





PRINT(f'The size of the data frame is -> {len(second_df)}')
print(f'Number of time P06756 appears -> {len(second_df[second_df["UniProt2"] == "P06756"])}')
print(f'Number of time P08514 appears -> {len(second_df[second_df["UniProt2"] == "P08514"])}')
print(f'Number of time P17301 appears -> {len(second_df[second_df["UniProt2"] == "P17301"])}')
print(f'Number of time P26006 appears -> {len(second_df[second_df["UniProt2"] == "P26006"])}')

PRINT('Done.')





third_df = protein_dataframes[unique_proteins[2]]

third_df.head(2)





third_df_encoded = one_hot_encoding(third_df)


third_df_encoded.columns


third_df_encoded.head(3)


filtered_columns = filter_proteins_list(third_df_encoded, columns_to_remove = ['SMILES', 'P08648'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_3 = third_df_encoded[filtered_columns]

temp_df_3.head(5)


visualize_dist(temp_df_3, unique_proteins[2])





PRINT(f'The size of the data frame is -> {len(third_df)}')
print(f'Number of time P05556 appears -> {len(third_df[third_df["UniProt2"] == "P05556"])}')
print(f'Number of time P06756 appears -> {len(third_df[third_df["UniProt2"] == "P06756"])}')
print(f'Number of time P32297 appears -> {len(third_df[third_df["UniProt2"] == "P32297"])}')

PRINT('Done.')


#encoded_dir_path = 'data/modified/one hot encoded csv files for training'

first_df_encoded.to_csv(os.path.join('data/modified/one hot encoded csv files for training/first_df_encoded.csv'), index=False)
second_df_encoded.to_csv(os.path.join('data/modified/one hot encoded csv files for training/second_df_encoded.csv'), index=False)
third_df_encoded.to_csv(os.path.join('data/modified/one hot encoded csv files for training/third_df_encoded.csv'), index=False)

PRINT('Done.')














def gc_model_builder(**model_params):
    """
    Helper function that constructs and configures a GraphConvModel for the PPI prediction task. 
    This function is intended to be used to provide the necessary model for hyperparameter tuning 
    with the `GridHyperparamOpt()` object.

    Parameters:
    - learning_rate (float): The learning rate for the optimizer.
    - dropout (float): Dropout rate to prevent overfitting.
    - batch_normalize (bool): Whether to apply batch normalization.
    - n_classes (int): Number of classes for classification.
    
    Returns:
    - GraphConvModel: Configured instance of GraphConvModel for PPI prediction tas.
    """
    n_classes = 5 # NEED TO SPECIFY !
    learning_rate = model_params['learning_rate']
    dropout = model_params['dropout']
    batch_normalize = model_params['batch_normalize']
    
    return GraphConvModel(n_tasks=1,
                          dropout=dropout,
                          mode='classification',
                          batch_normalize=batch_normalize,
                          n_classes=n_classes,
                          learning_rate=learning_rate
                          )


def custom_roc_auc_score(y_true, y_pred, multi_class='ovr', average='weighted'):
    return roc_auc_score(y_true, y_pred, multi_class=multi_class, average=average)


def execute_hyperparameter_tuning_for_graph_conv(csv_data, df, params):
    """
    Perform hyperparameter tuning for a Graph Convolutional Model using a grid search approach.

    Parameters:
    - csv_data (str or pd.DataFrame): Path to a CSV file containing molecular data for `deepchem.data.CSVLoader.featurize()` object
    - df (pd.DataFrame): A Pandas DataFrame containing the data set for the model.
    - params (dict): Dictionary of hyperparameters to be tuned.

    Returns:
    - list: A list containing the best hyperparameters and detailed results of the hyperparameter search.
    """
    
    tasks = ['NumericUniProtTargetLabels']
    featurizer = dc.feat.ConvMolFeaturizer()
    loader = dc.data.CSVLoader(tasks=tasks,
                               smiles_field='SMILES',
                               featurizer=featurizer)
    
    #splitter = dc.splits.RandomSplitter()
    splitter = dc.splits.RandomStratifiedSplitter()

    mean_roc_auc_metric = dc.metrics.Metric(
        metric=custom_roc_auc_score,
        task_averager=np.mean,
        mode='classification',
        n_tasks=1
    )
    dataset = loader.featurize(csv_data)

    res = splitter.train_test_split(dataset, train_frac=0.7) 
    train_dataset, valid_dataset = res
    
    # Create a hyperparameter optimization object
    opt = GridHyperparamOpt(gc_model_builder)
    _, best_hyperparams, all_results = opt.hyperparam_search(params, train_dataset, valid_dataset, mean_roc_auc_metric)

    return [best_hyperparams, all_results]


def generate_graph_conv_model(dropout, batch_normalize, n_classes, learning_rate, model_dir):
    """
    Generate a Graph Convolutional Neural Network (GraphConvModel) for classification tasks.

    Parameters:
    - dropout (float): Dropout rate to apply in the model.
    - batch_normalize (bool): Whether to apply batch normalization.
    - n_classes (int): Number of classes for classification.
    - learning_rate (float): Learning rate for model training.
    - model_dir (str): Directory to save the trained model.

    Returns:
    - model (GraphConvModel): The configured GraphConvModel for classification.
    """
    batch_size = 64
    model = GraphConvModel(n_tasks=1, 
                           dropout=dropout,
                           batch_size=batch_size,
                           batch_normalize=batch_normalize,
                           mode='classification',
                           model_dir=model_dir,
                           n_classes=n_classes,
                           learning_rate=learning_rate)
    
    return model


def GenerateBoxplotForModelPreformaceVisualization(UniProt, cv_folds, training_score_list, validation_score_list) ->None :
    """
    Generate a boxplot to visualize the performance of a model on training and validation sets.

    Parameters:
    - UniProt (str): The UniProt ID.
    - cv_folds (int): Number of cross-validation folds.
    - training_score_list (list): List of training scores for each fold.
    - validation_score_list (list): List of validation scores for each fold.

    Returns:
    - None
    """
    
    data = {
        'Group': ["Training"] * cv_folds + ["Validation"] * cv_folds,
        'Score': training_score_list + validation_score_list
        }

    sns.boxplot(x="Group", y="Score", data=data)
    
    plt.title(label=f"{UniProt} Mean-Roc-Auc-Score Boxplot Graph", 
              fontsize=15, 
              color="blue")
    
    plt.show()


def get_class_labels(predicted_probs):
    """
    Extract class labels from predicted probabilities.

    Parameters:
    - predicted_probs (numpy.ndarray): Array containing predicted probabilities for each class.

    Returns:
    - class_labels (numpy.ndarray): Array containing the class labels corresponding to the highest probability.
    """
    # Remove the extra dimension
    squeezed_probs = np.squeeze(predicted_probs, axis=1)
    
    # Get the class labels
    class_labels = np.argmax(squeezed_probs, axis=1)
    return class_labels








def calculate_descriptors(smiles):
    """
    Helper function that takes a molecule's SMILES value and generates a list of the best 8 features 
    found to be the most significant for our PPI prediction task.
    
    Params:
    - smiles (str): Molecule's SMILES value as a string.
    
    Returns:
    - list: A list of 8 features generated from the molecule's SMILES.
    """
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        descriptors = [
            Descriptors.MolWt(mol),
            Descriptors.NumValenceElectrons(mol),
            Descriptors.TPSA(mol),
            Descriptors.MolLogP(mol),
            Descriptors.NumHeteroatoms(mol),
            Descriptors.NumRotatableBonds(mol),
            Descriptors.HeavyAtomCount(mol),
            Descriptors.FractionCSP3(mol)
        ]
        return descriptors
    else:
        return [None] * 8  # Return None for each descriptor if SMILES cannot be parsed


def GenerateFeaturesByMoleculeSMILES(df) -> pd.DataFrame:
    """
    Takes a DataFrame containing data for a PPI prediction task and adds features using the 
    `calculate_descriptors(smiles)` feature augmentation helper function.
    
    Params:
    - df (pd.DataFrame): DataFrame containing data for the task.
    
    Returns:
    - pd (pd.DataFrame): The same DataFrame after adding the new features.
    """
    df_ = df.copy()
    # Apply the `calculate_descriptors` method in order to generate 8 new features for df
    df_['MolecularDescriptors'] = df_['SMILES'].apply(calculate_descriptors)

    # Transfer the array at each row under the 'MolecularDescriptors' column into column with their corresponding names & drop the colunn
    df_[['MolWt', 'NumValenceElectrons', 'TPSA', 'MolLogP', 'NumHeteroatoms', 'NumRotatableBonds', 'HeavyAtomCount', 'FractionCSP3']] = pd.DataFrame(df_['MolecularDescriptors'].tolist(), index=df_.index)
    df_.drop(columns=['MolecularDescriptors'], axis=1, inplace=True)

    # Reorder the columns names so that the label column will be the last column in df
    df_ = df_[['SMILES', 'MolWt', 'NumValenceElectrons', 'TPSA', 'MolLogP', 'NumHeteroatoms', 'NumRotatableBonds', 'HeavyAtomCount', 'FractionCSP3', 'NumericUniProtTargetLabels']]

    return df_





def GenerateMorganFingerprintsFeaturesByMoleculeSMILES(df, size, radius) -> pd.DataFrame:
    """
    Generate Morgan fingerprints features for molecules based on their SMILES representation.

    Parameters:
    - df (pd.DataFrame): DataFrame containing data for the task.
    - size (int): Size of the circular fingerprint (number of bits).
    - radius (int): Radius parameter for the circular fingerprint.

    Returns:
    - pd.DataFrame: DataFrame with Morgan fingerprints features added.
    """

    # Define the CircularFingerprint featurizer to generate Morgan Fingerprints features
    featurizer = dc.feat.CircularFingerprint(size=size, radius=radius)
    
    # Convert SMILES to features using the featurizer
    X = [featurizer.featurize(smiles) for smiles in df['SMILES']]
    X_flat = [x.flatten() for x in X]  
    feature_columns = [f'Feature_{i}' for i in range(len(X_flat[0]))]  
    df_features = pd.DataFrame(X_flat, columns=feature_columns)
    
    # Combine the features with the original dataframe
    df_combined = pd.concat([df, df_features], axis=1)

    df_with_morgan_fingerprints_features = df_combined

    return df_with_morgan_fingerprints_features






def GenerateRandomForestModel(df, weight_dict, n_bootstrap_samples=100, if_binary=True, bootstrap=True):
    """
    Takes data frame with columns ['SMILES', ... molecule fetures ..., 'NumericUniProtTargetLabels'], traing and evaluate Random Forest Classifier
    model after choosing the best hyperparameters by `GrudSearchCV`. The function also takes `weight_dict`, which is dictionary of weights assigned
    for each class in case of imbalanced data, or 'balanced' if the data is balanced.

    Params:
    df - data frame
    weight_dict - dictionary of weight, e.g., {0:1, 1:1.8, 2:1, 3:1.3}. In case the data balanced, pass 'balanced' instead.
    n_bootstrap_samples - the number of bootstrap samples to create for ROC AUC calculation
    if_binary - Boolean, True if the number of class is 2, else specify False
    bootstrap - Boolean, True if the data isn't unbalanced much, else Flase.

    Return:
    tuple - (best_rf_model, model_preformance_dictionary)
    """
    
    # Drop SMILES' and labels columns
    X = df.drop(['SMILES', 'NumericUniProtTargetLabels'], axis=1)
    y = df['NumericUniProtTargetLabels']
    
    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    
    # Generate RF model for hyperparameter tuning phase
    rf_model = RandomForestClassifier(class_weight=weight_dict, random_state=42)

    # Use StratifiedKFold for cross-validation
    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Define a parameter distribution
    param_grid = {
        'n_estimators': [50, 100, 150, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10, 15],
        'min_samples_leaf': [1, 2, 4, 8]
    }
    
    # Use a custom scoring functions for GridSearchCV
    scoring = {
        'Accuracy Score': make_scorer(accuracy_score, average='weighted'),
        'Precision Score': make_scorer(precision_score, average='weighted'),
        'Recall Score': make_scorer(recall_score, average='weighted'),
        'F1 Score': make_scorer(f1_score, average='weighted'),
        'Roc Auc Score': make_scorer(roc_auc_score, needs_proba=True, average='weighted', multi_class='ovr')
    }
    
    # Perform GridSearchCV with StratifiedKFold & get the best hyperparameters
    grid_search = GridSearchCV(rf_model, param_grid, cv=stratified_kfold, scoring=scoring, refit='Roc Auc Score')
    grid_search.fit(X_train, y_train)
    
    best_params = grid_search.best_params_
    
    # Create a Random Forest classifier with the best hyperparameters
    best_rf_model = RandomForestClassifier(class_weight=weight_dict, random_state=42, **best_params)
        
    # Train the model on the entire training set
    best_rf_model.fit(X_train, y_train)
    
    # Make predictions on the test set in order to evaluate with the rest metrices
    y_test_pred = best_rf_model.predict(X_test)
    
    # Evaluate the model on the test set
    accuracy_test = accuracy_score(y_test, y_test_pred)
    precision_test = precision_score(y_test, y_test_pred, average='weighted')
    recall_test = recall_score(y_test, y_test_pred, average='weighted')
    f1_test = f1_score(y_test, y_test_pred, average='weighted')
    conf_matrix_test = confusion_matrix(y_test, y_test_pred)

    # Print classification report
    print('Classification Report:')
    print(classification_report(y_test, y_test_pred))
    
    if bootstrap:
        # Initialize arrays to store ROC AUC scores from bootstrap samples
        roc_auc_scores = np.zeros(n_bootstrap_samples)
        
        # Perform bootstrap resampling and calculate ROC AUC scores
        for i in range(n_bootstrap_samples):
            # Sample with replacement from the test set
            X_bootstrap, y_bootstrap = resample(X_test, y_test, stratify=y_test, random_state=i)
            
            if if_binary:
                y_bootstrap_pred = best_rf_model.predict(X_bootstrap)
            else:
                y_bootstrap_pred = best_rf_model.predict_proba(X_bootstrap)
    
            
            # Calculate ROC AUC score
            roc_auc_scores[i] = roc_auc_score(y_bootstrap, y_bootstrap_pred, average='weighted', multi_class='ovr')
        
        # Calculate the mean and standard deviation of bootstrap ROC AUC scores
        roc_auc_mean = np.mean(roc_auc_scores)
        roc_auc_std = np.std(roc_auc_scores)
        PRINT(f'Mean ROC AUC: {roc_auc_mean:.3f}, Std Dev ROC AUC: {roc_auc_std:.3f}')

        return (best_rf_model, {
            'accuracy': accuracy_test,
            'roc_auc_mean_score': roc_auc_mean,
            'roc_auc_std_score': roc_auc_std,
            'precision': precision_test,
            'recall': recall_test,
            'f1_score': f1_test,
            'confusion_matrix': conf_matrix_test.tolist()
        })

    # In case bootstrap is False    
    else:
        if if_binary:
            y_test_pred = best_rf_model.predict(X_test)
        else:
            y_test_pred = best_rf_model.predict_proba(X_test)
                        
        roc_auc_test = roc_auc_score(y_test, y_test_pred, average='weighted', multi_class='ovr')
        PRINT(f'ROC AUC {roc_auc_test:.3f}')

        return (best_rf_model, {
            'accuracy': accuracy_test,
            'roc_auc_score': roc_auc_test,
            'precision': precision_test,
            'recall': recall_test,
            'f1_score': f1_test,
            'confusion_matrix': conf_matrix_test.tolist()
        })





def GenerateXGBoostModel(df, weight_dict, n_bootstrap_samples=100, if_binary=True, bootstrap=True):
    """
    Takes data frame with columns ['SMILES', ... molecule fetures ..., 'NumericUniProtTargetLabels'], traing and evaluate XGBoost model after 
    choosing the best hyperparameters by `GrudSearchCV`. The function also takes `weight_dict`, which is dictionary of weights assigned
    for each class in case of imbalanced data, or 'balanced' if the data is balanced.

    Params:
    df - data frame
    weight_dict - dictionary of weight, e.g., {0:1, 1:1.8, 2:1, 3:1.3}. In case the data balanced, pass 'balanced' instead.
    if_binary - Boolean, True if the number of class is 2, else specify False
    bootstrap - Boolean, True if the data isn't unbalanced much, else Flase.

    Return:
    tuple - (best_xbg_model, model_preformance_dictionary)
    """
    # Drop 'SMILES' and labels columns
    X = df.drop(['SMILES', 'NumericUniProtTargetLabels'], axis=1)
    y = df['NumericUniProtTargetLabels']
    
    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
        
    # Generate XGB model for hyperparameter tuning phase
    xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(set(y_train)), random_state=42)
    
    # Use StratifiedKFold for cross-validation
    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [3, 5],
        'learning_rate': [0.01, 0.1],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0],
        'gamma': [0, 0.2],
        'min_child_weight': [1, 5],
        'reg_alpha': [0, 0.5],
        'reg_lambda': [0, 0.5],
    }

    # Use a custom scoring functions for GridSearchCV
    scoring = {
        'Accuracy Score': make_scorer(accuracy_score, average='weighted'),
        'Precision Score': make_scorer(precision_score, average='weighted'),
        'Recall Score': make_scorer(recall_score, average='weighted'),
        'F1 Score': make_scorer(f1_score, average='weighted'),
        'Roc Auc Score': make_scorer(roc_auc_score, needs_proba=True, average='weighted', multi_class='ovr')
    }
        
    # Perform GridSearchCV with StratifiedKFold & extract the best hyperparameters
    grid_search = GridSearchCV(xgb_model, param_grid, cv=stratified_kfold, scoring=scoring, refit='Roc Auc Score')
    grid_search.fit(X_train, y_train)
    
    best_params = grid_search.best_params_
    
    # Create an XGBoost classifier with the best hyperparameters
    best_xgb_model = xgb.XGBClassifier(objective='multi:softmax',
                                       num_class=len(set(y_train)),
                                       random_state=42, **best_params)
    
    # Calculate sample weights for each instance based on class weights
    sample_weights = compute_sample_weight(weight_dict, y_train)
    
    # Train the model on the entire training set with sample weights
    best_xgb_model.fit(X_train, y_train, sample_weight=sample_weights)

   # Make predictions on the test set in order to evaluate with the rest metrices
    y_test_pred = best_xgb_model.predict(X_test)
    
    # Evaluate the model on the test set
    accuracy_test = accuracy_score(y_test, y_test_pred)
    precision_test = precision_score(y_test, y_test_pred, average='weighted')
    recall_test = recall_score(y_test, y_test_pred, average='weighted')
    f1_test = f1_score(y_test, y_test_pred, average='weighted')
    conf_matrix_test = confusion_matrix(y_test, y_test_pred)

    # Print classification report
    print('Classification Report:')
    print(classification_report(y_test, y_test_pred))
    
    if bootstrap:
        # Initialize arrays to store ROC AUC scores from bootstrap samples
        roc_auc_scores = np.zeros(n_bootstrap_samples)
        
        # Perform bootstrap resampling and calculate ROC AUC scores
        for i in range(n_bootstrap_samples):
            # Sample with replacement from the test set
            X_bootstrap, y_bootstrap = resample(X_test, y_test, stratify=y_test, random_state=i)
            
            if if_binary:
                y_bootstrap_pred = best_xgb_model.predict(X_bootstrap)
            else:
                y_bootstrap_pred = best_xgb_model.predict_proba(X_bootstrap)
    
            
            # Calculate ROC AUC score
            roc_auc_scores[i] = roc_auc_score(y_bootstrap, y_bootstrap_pred, average='weighted', multi_class='ovr')
        
        # Calculate the mean and standard deviation of bootstrap ROC AUC scores
        roc_auc_mean = np.mean(roc_auc_scores)
        roc_auc_std = np.std(roc_auc_scores)
        PRINT(f'Mean ROC AUC: {roc_auc_mean:.3f}, Std Dev ROC AUC: {roc_auc_std:.3f}')

        return (best_xgb_model, {
            'accuracy': accuracy_test,
            'roc_auc_mean_score': roc_auc_mean,
            'roc_auc_std_score': roc_auc_std,
            'precision': precision_test,
            'recall': recall_test,
            'f1_score': f1_test,
            'confusion_matrix': conf_matrix_test.tolist()
        })

    # In case bootstrap is False    
    else:
        if if_binary:
            y_test_pred = best_xgb_model.predict(X_test)
        else:
            y_test_pred = best_xgb_model.predict_proba(X_test)
                        
        roc_auc_test = roc_auc_score(y_test, y_test_pred, average='weighted', multi_class='ovr')
        PRINT(f'ROC AUC {roc_auc_test:.3f}')

        return (best_xgb_model, {
            'accuracy': accuracy_test,
            'roc_auc_score': roc_auc_test,
            'precision': precision_test,
            'recall': recall_test,
            'f1_score': f1_test,
            'confusion_matrix': conf_matrix_test.tolist()
        })


def plot_model_comparison_countplots(df):
    """
    Plot bar plots for model performance comparison.

    Parameters:
    - df: DataFrame containing model metrics. Columns represent models, and rows represent metrics.
    """
    
    evaluation_metrices = ['Accuracy', 'Roc Auc Mean Score', 'Roc Auc Std Score', 'Precision', 'Recall', 'F1_score']
    
    # Reshape the DataFrame using melt
    melted_df = pd.melt(df.reset_index(), id_vars=['index'], var_name='Model', value_name='Score')

    fig, axes = plt.subplots(nrows=2, ncols=len(df.index)//2, figsize=(10, 6))
    fig.suptitle('Model Performance Bar Plots', fontsize=16)

    # Plot bar plots for each metric
    for i, _ in enumerate(df.index):
        row, col = divmod(i, len(df.index)//2)
        ax = axes[row, col]
        sns.barplot(x='Model', y='Score', data=melted_df[melted_df['index'] == i], ax=ax)
        ax.set_title(f'Plot {chr(65 + i)} - {evaluation_metrices[i]}')
        ax.set_xlabel('Model')
        ax.set_ylabel(f'{evaluation_metrices[i]} Scores')
        ax.tick_params(axis='x', labelsize=8)

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()











df_dict = protein_dataframes


prot_ls = list(df_dict.keys())

PRINT(f'Unique proteins -> {prot_ls}')


csv_dir_path = 'data/modified/one hot encoded csv files for training' 





def generate_df_for_training_(UniProt_str, csv_file_name, one_hot_encoded_csv):
    """
    Generate and prepare a DataFrame for model training.

    Parameters:
    - df (pd.DataFrame): Original DataFrame containing the dataset for training.
    - UniProt_str (str): String identifier for the specific UniProt.
    - csv_file_name (str): Name of the CSV file containing UniProt-specific dataset.
    - one_hot_encoded_csv (str): Name of the CSV file containing one-hot encoded labels.

    Returns:
    - tuple: A tuple containing two DataFrames: one for model training and the other with UniProt-specific data.
    """

    # Define the directories for CSV files
    csv_dir = 'data\\modified\\unique UniProt csv files'
    csv_dir_ohe = 'data\\modified\\one hot encoded csv files for training'

    # Read the UniProt-specific CSV file and the one-hot encoded CSV file
    curr_df = pd.read_csv(os.path.join(csv_dir, csv_file_name))
    ohe_df = pd.read_csv(os.path.join(csv_dir_ohe, one_hot_encoded_csv))

    # Drop unnecessary column and rename the target column
    curr_df.drop('UniProt1', axis=1, inplace=True)
    curr_df = curr_df.rename(columns={'UniProt2':'UniProtTargetLabels'})

    # Extract the list of labels from the one-hot encoded DataFrame
    labels = ohe_df.columns[2:].tolist()

    # Print the UniProt model labels
    PRINT(f'{UniProt_str} model labels -> {labels}')

    # Create a mapping of column names to indices for label encoding
    column_name_to_index = {label: i for i, label in enumerate(labels)}

    # Map the 'labels' column in df to column indices
    curr_df['NumericUniProtTargetLabels'] = curr_df['UniProtTargetLabels'].map(column_name_to_index)

    # Shuffle the rows
    curr_df = curr_df.sample(frac=1, random_state=42).reset_index(drop=True)

    # Create a DataFrame for model training by dropping the original 'UniProtTargetLabels' column
    df_for_model = curr_df.drop('UniProtTargetLabels', axis=1)

    PRINT(f'Finished generating DataFrames for UniProt -> {UniProt_str}.')
    
    # Return the tuple of DataFrames & the label mapping dictionary
    return (df_for_model, curr_df, column_name_to_index)









P05556_df_t = pd.read_csv(os.path.join('data/modified/unique UniProt csv files', 'P05556.csv'))


P05556_df_t.head(5)


P05556_df_t_ = P05556_df_t[~P05556_df_t['UniProt2'].isin(['O75578', 'P23229'])]


PRINT(f'The number of rows we filtered from our dataframe -> {P05556_df_t.shape[0] - P05556_df_t_.shape[0]}\nFiltered dataframa shape is -> {P05556_df_t_.shape}')


# Save the filtered data frame as csv file
P05556_df_t_.to_csv(os.path.join('data/modified/unique UniProt csv files', 'P05556_.csv'), index=False)

PRINT('Saved.')


# Generate new one-hot-encoded df of the filtered dataframe & save it
second_df_encoded_ = one_hot_encoding(P05556_df_t_)

second_df_encoded_.head(3)


second_df_encoded_.to_csv(os.path.join('data/modified/one hot encoded csv files for training', 'first_df_encoded_.csv'), index=False)

PRINT('Saved.')





P05556_df_for_training, P05556_df_with_uniprotes_col, mapped_label_dict_P05556 = generate_df_for_training_('P05556', 'P05556_.csv', 'first_df_encoded_.csv')


P05556_df_for_training.head(2)


P05556_df_with_uniprotes_col.head(2)


plot_uniprot_numeric_label_frequency(df=P05556_df_for_training, UniProt='P05556')


PRINT(f'The mapped labels in ("UniProt": "index_label") format:\n\n{mapped_label_dict_P05556}')





PRINT(P05556_df_for_training['SMILES'].apply(type).value_counts())


# Identify rows with 'float' values in the 'SMILES' column
float_rows = P05556_df_for_training['SMILES'].apply(lambda x: isinstance(x, float))

# Display the rows with 'float' values
float_rows_data = P05556_df_for_training[float_rows]

PRINT(float_rows_data)


# Drop rows with 'float' values in the 'SMILES' column
P05556_df_for_training = P05556_df_for_training[~float_rows]


PRINT(P05556_df_for_training['SMILES'].apply(type).value_counts())
PRINT('Done.')











P05556_df_for_training_ = GenerateFeaturesByMoleculeSMILES(df=P05556_df_for_training)


P05556_df_for_training_.head(2)


weight_dict = 'balanced'

rf_model_tuple_P05556 = GenerateRandomForestModel(df=P05556_df_for_training_, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training Random Forest Multicalss Classifier Model for UniProt P055556 using RKDitDescriptors features')


PRINT(f'The results of Random Forest Multiclass Classifier model\nusing RKDitDescriptors for UniProt P05556 are:')
print_dict_meaningful(rf_model_tuple_P05556[1])
PRINT(f'Done.')





weight_dict = 'balanced'

xgb_model_tuple_P05556 = GenerateXGBoostModel(df=P05556_df_for_training_, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training XGBoost Multicalss Classifier Model for UniProt P05556 using RKDitDescriptors features')


PRINT(f'The results of XGBoost Multiclass Classifier model\nusing RKDitDescriptors for UniProt P05556 are:')
print_dict_meaningful(xgb_model_tuple_P05556[1])
PRINT(f'Done.')





P05556_df_for_training__ = GenerateMorganFingerprintsFeaturesByMoleculeSMILES(df=P05556_df_for_training, size=1024, radius=2)


P05556_df_for_training__.head(2)





original_rows = P05556_df_for_training__.shape[0]

P05556_df_for_training__ = P05556_df_for_training__.dropna()

# Calculate the number of dropped rows
dropped_rows = original_rows - P05556_df_for_training__.shape[0]

PRINT(f"{dropped_rows} rows were dropped.")


weight_dict = 'balanced'

rf_model_tuple_P05556_ = GenerateRandomForestModel(df=P05556_df_for_training__, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training Random Forest Multicalss Classifier Model for UniProt P055556 using Morgan Fingerprints features')


PRINT(f'The results of Random Forest Multiclass Classifier using\nMorgan Fingerprints features model for UniProt P05556 are:')
print_dict_meaningful(rf_model_tuple_P05556_[1])
PRINT(f'Done.')





weight_dict = 'balanced'

xgb_model_tuple_P05556_ = GenerateXGBoostModel(df=P05556_df_for_training__, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training XGBoost Multicalss Classifier Model for UniProt P05556 using Morgan Fingerprints features')


PRINT(f'The results of XGBoost Multiclass Classifier using\nMorgan Fingerprints features model for UniProt P05556 are:')
print_dict_meaningful(xgb_model_tuple_P05556_[1])
PRINT(f'Done.')








df_for_preformance_comparison = pd.DataFrame({
    'RM_RDKit': list(rf_model_tuple_P05556[1].values()),
    'XGB_RDKit': list(xgb_model_tuple_P05556[1].values()),
    'RM_MF': list(rf_model_tuple_P05556_[1].values()),
    'XGB_MF': list(xgb_model_tuple_P05556_[1].values())
}, index=rf_model_tuple_P05556[1].keys())

df_for_preformance_comparison.head(7)


# Drop index columm
df_for_preformance_comparison.drop(df_for_preformance_comparison.index[-1], inplace=True)

# Drop the last row (i.e., confusion_matrix row)
df_for_preformance_comparison.reset_index(drop=True, inplace=True)


df_for_preformance_comparison.head(6)


plot_model_comparison_countplots(df=df_for_preformance_comparison)





path = os.path.join('models\\Best Model of each UniProt Modified', 'rf_P05556_RDKD.joblib')
dump(rf_model_tuple_P05556[0], path)

PRINT('Model Saved')





P05106_df_for_training, P05106_df_with_uniprotes_col, mapped_label_dict_P05106 = generate_df_for_training_('P05106', 'P05106.csv', 'second_df_encoded.csv')


P05106_df_for_training.head(3)


P05106_df_with_uniprotes_col.head(3)


plot_uniprot_numeric_label_frequency(df=P05106_df_for_training, UniProt='P05106')


PRINT(f'The mapped labels in ("UniProt": "index_label") format:\n\n{mapped_label_dict_P05106}')





PRINT(P05106_df_for_training['SMILES'].apply(type).value_counts())


# Identify rows with 'float' values in the 'SMILES' column
float_rows = P05106_df_for_training['SMILES'].apply(lambda x: isinstance(x, float))

# Display the rows with 'float' values
float_rows_data = P05106_df_for_training[float_rows]

PRINT(float_rows_data)


# Drop rows with 'float' values in the 'SMILES' column
P05106_df_for_training = P05106_df_for_training[~float_rows]


PRINT(P05106_df_for_training['SMILES'].apply(type).value_counts())
PRINT('Done.')











P05106_df_for_training_ = GenerateFeaturesByMoleculeSMILES(df=P05106_df_for_training)


P05106_df_for_training_.head(2)


weight_dict = 'balanced'

rf_model_tuple_P05106 = GenerateRandomForestModel(df=P05106_df_for_training_, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training Random Forest Multicalss Classifier Model for UniProt P05106 using RKDitDescriptors features')


PRINT(f'The results of Random Forest Multiclass Classifier model\nusing RDKitDescriptors features for UniProt P05106 are:')
print_dict_meaningful(rf_model_tuple_P05106[1])
PRINT(f'Done.')





weight_dict = 'balanced'

xgb_model_tuple_P05106 = GenerateXGBoostModel(df=P05106_df_for_training_, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training XGBoost Multicalss Classifier Model for UniProt P05106 using RKDitDescriptors features')


PRINT(f'The results of the best XGBoost Multiclass Classifier model for\nUniProt P05106 are:')
print_dict_meaningful(xgb_model_tuple_P05106[1])
PRINT(f'Done.')





P05106_df_for_training__ = GenerateMorganFingerprintsFeaturesByMoleculeSMILES(df=P05106_df_for_training, size=1024, radius=2)


P05106_df_for_training__.head(2)





original_rows = P05106_df_for_training__.shape[0]

P05106_df_for_training__ = P05106_df_for_training__.dropna()

# Calculate the number of dropped rows
dropped_rows = original_rows - P05106_df_for_training__.shape[0]

PRINT(f"{dropped_rows} rows were dropped.")


# Conver target labels into int
P05106_df_for_training__['NumericUniProtTargetLabels'] = P05106_df_for_training__['NumericUniProtTargetLabels'].astype(int)


weight_dict = 'balanced'

rf_model_tuple_P05106_ = GenerateRandomForestModel(df=P05106_df_for_training__, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training Random Forest Multicalss Classifier Model for UniProt P05106 using Morgan Fingerprints features')


PRINT(f'The results of Random Forest Multiclass Classifier model\nusing Morgan Fingerprints features for UniProt P05106 are:')
print_dict_meaningful(rf_model_tuple_P05106_[1])
PRINT(f'Done.')





weight_dict = 'balanced'

xgb_model_tuple_P05106_ = GenerateXGBoostModel(df=P05106_df_for_training__, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training XGBoost Multicalss Classifier Model for UniProt P05106 using Morgan Fingerprints features')


PRINT(f'The results of XGBoost Multiclass Classifier model\nusing Morgan Fingerprints features for UniProt P05106 are:')
print_dict_meaningful(xgb_model_tuple_P05106_[1])
PRINT(f'Done.')








df_for_preformance_comparison = pd.DataFrame({
    'RM_RDKit': list(rf_model_tuple_P05106[1].values()),
    'XGB_RDKit': list(xgb_model_tuple_P05106[1].values()),
    'RM_MF': list(rf_model_tuple_P05106_[1].values()),
    'XGB_MF': list(xgb_model_tuple_P05106_[1].values())
}, index=rf_model_tuple_P05106[1].keys())

df_for_preformance_comparison.head(7)


df_for_preformance_comparison.drop(df_for_preformance_comparison.index[-1], inplace=True)
df_for_preformance_comparison.reset_index(drop=True, inplace=True)


df_for_preformance_comparison.head(6)


plot_model_comparison_countplots(df=df_for_preformance_comparison)








path = os.path.join('models/Best Model of each UniProt Modified', 'rf_P05106_RDKD.joblib')
dump(rf_model_tuple_P05106[0], path)

PRINT('Model Saved')





P08648_df_for_training, P08648_df_with_uniprotes_col, mapped_label_dict_P08648 = generate_df_for_training_('P08648', 'P08648.csv', 'third_df_encoded.csv')


P08648_df_for_training.head(3)


P08648_df_with_uniprotes_col.head(3)


plot_uniprot_numeric_label_frequency(df=P08648_df_for_training, UniProt='P08648')


PRINT(f'The mapped labels in ("UniProt": "index_label") format:\n\n{mapped_label_dict_P08648}')





# Identify rows with 'float' values in the 'SMILES' column
float_rows = P08648_df_for_training['SMILES'].apply(lambda x: isinstance(x, float))


# Display the rows with 'float' values
float_rows_data = P08648_df_for_training[float_rows]

PRINT(float_rows_data)


original_rows = P08648_df_for_training.shape[0]

P08648_df_for_training = P08648_df_for_training.dropna()

# Calculate the number of dropped rows
dropped_rows = original_rows - P08648_df_for_training.shape[0]

PRINT(f"{dropped_rows} rows were dropped.")











P08648_df_for_training_ = GenerateFeaturesByMoleculeSMILES(df=P08648_df_for_training)


P08648_df_for_training_.head(3)


weight_dict = 'balanced'

rf_model_tuple_P08648 = GenerateRandomForestModel(df=P08648_df_for_training_, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training Random Forest Multicalss Classifier Model for UniProt P08648 using RKDitDescriptors features')





PRINT(f'The results of the best Random Forest Multiclass Classifier model\nusing RDKit Descriptors features for UniProt P05106 are:')
print_dict_meaningful(rf_model_tuple_P08648[1])
PRINT(f'Done.')





weight_dict = 'balanced'

xgb_model_tuple_P08648 = GenerateXGBoostModel(df=P08648_df_for_training_, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training XGBoost Multicalss Classifier Model for UniProt P08648 using RKDitDescriptors features')


PRINT(f'The results of the best Random Forest Multiclass Classifier model\nusing RDKit Descriptors features for UniProt P05106 are:')
print_dict_meaningful(xgb_model_tuple_P08648[1])
PRINT(f'Done.')





P08648_df_for_training__ = GenerateMorganFingerprintsFeaturesByMoleculeSMILES(df=P08648_df_for_training, size=1024, radius=2)





original_rows = P08648_df_for_training__.shape[0]

P08648_df_for_training__ = P08648_df_for_training__.dropna()

# Calculate the number of dropped rows
dropped_rows = original_rows - P08648_df_for_training__.shape[0]

PRINT(f"{dropped_rows} rows were dropped.")


P05106_df_for_training__.head(5)


weight_dict = 'balanced'

rf_model_tuple_P08648_ = GenerateRandomForestModel(df=P08648_df_for_training__, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training Random Forest Multicalss Classifier Model for UniProt P08648 using Morgan Fingerprints features')


PRINT(f'The results of Random Forest Multiclass Classifier model\nusing Morgan Fingerprints features for UniProt P08648 are:')
print_dict_meaningful(rf_model_tuple_P08648_[1])
PRINT(f'Done.')





weight_dict = 'balanced'

xgb_model_tuple_P08648_ = GenerateXGBoostModel(df=P08648_df_for_training__, weight_dict=weight_dict, if_binary=False, bootstrap=True)
PRINT(f'Done training XGBoost Multicalss Classifier Model for UniProt P08648 using Morgan Fingerprints features')


PRINT(f'The results of the best XGBoost Multiclass Classifier model\nusing Morgan Fingerprints features for UniProt P08648 are:')
print_dict_meaningful(xgb_model_tuple_P08648_[1])
PRINT(f'Done.')








df_for_preformance_comparison = pd.DataFrame({
    'RM_RDKit': list(rf_model_tuple_P08648[1].values()),
    'XGB_RDKit': list(xgb_model_tuple_P08648[1].values()),
    'RM_MF': list(rf_model_tuple_P08648_[1].values()),
    'XGB_MF': list(xgb_model_tuple_P08648_[1].values())
}, index=rf_model_tuple_P08648[1].keys())

df_for_preformance_comparison.head(7)


df_for_preformance_comparison.drop(df_for_preformance_comparison.index[-1], inplace=True)
df_for_preformance_comparison.reset_index(drop=True, inplace=True)


df_for_preformance_comparison.head(6)


plot_model_comparison_countplots(df=df_for_preformance_comparison)





path = os.path.join('models/Best Model of each UniProt Modified', 'xgb_P08648_MF.joblib')
dump(xgb_model_tuple_P08648_[0], path)

PRINT('Model Saved')





final_df_path = 'data/dataset_for_prediction.csv'


f_df = pd.read_csv(final_df_path)

PRINT(f'Loaded the final data frame')
f_df.head(10)


f_df.rename(columns={'uniprot_id1':'UniProtTarget', 'smiles':'SMILES'}, inplace=True)


f_df.head(3)


# Verify that there is no duplicated rows in our prediction dataset, i.e., duplicated [SMILES, UniProtTarget] rows
duplicated_rows_df = f_df[f_df.duplicated()]

PRINT(f'Number of duplicated rows if the prediction dataframe -> {duplicated_rows_df.shape[0]}')


target_dataframes = {}

# Iterate over unique UniProtTarget values
for target_value in f_df['UniProtTarget'].unique():
    # Filter the dataframe for the current UniProtTarget value
    target_df = f_df[f_df['UniProtTarget'] == target_value].copy()
       
    target_dataframes[target_value] = target_df


target_dataframes.keys()





P05556_label_dict = {0: 'P06756', 1: 'P08648', 2:'P13612',
                     3: 'P17301', 4:'P56199', 5: 'Q13797'
                    }



P05556_pred = target_dataframes['P05556'].copy()

P05556_pred.head(2)


PRINT(f'Shepe:\n\n{P05556_pred.shape}')


P05556_pred = P05556_pred.reset_index(drop=True)

PRINT(f'Reseted the indexes of the data frame in order to avoid issues with features generation')


P05556_pred.drop(['UniProtTarget'],axis=1, inplace=True)


# Genera copy of the data frame for our features
P05556_pred_ = P05556_pred.copy()

# Apply the `calculate_descriptors` method in order to generate 8 new features for df
P05556_pred_['MolecularDescriptors'] = P05556_pred_['SMILES'].apply(calculate_descriptors)

# Transfer the array at each row under the 'MolecularDescriptors' column into column with their corresponding names & drop the colunn
P05556_pred_[['MolWt', 'NumValenceElectrons', 'TPSA', 'MolLogP', 'NumHeteroatoms', 'NumRotatableBonds', 'HeavyAtomCount', 'FractionCSP3']] = pd.DataFrame(P05556_pred_['MolecularDescriptors'].tolist(), index=P05556_pred_.index)
P05556_pred_.drop(columns=['MolecularDescriptors'], axis=1, inplace=True)

# Reorder the columns names so that the label column will be the last column in df
P05556_pred_ = P05556_pred_[['SMILES', 'MolWt', 'NumValenceElectrons', 'TPSA', 'MolLogP', 'NumHeteroatoms', 'NumRotatableBonds', 'HeavyAtomCount', 'FractionCSP3']]

PRINT(f'Done generating features for prediction !')


P05556_pred_.head(2)


PRINT(f'Shape after generating features using RKDirDescriptors feature generation:\n\n{P05556_pred_.shape}')


rf_P05556 = load('models/Best Model of each UniProt Modified/rf_P05556_RDKD.joblib')
PRINT(f'Loaded Model Successfully !')


# Drop SMILES column for prediction
df_for_model_P05556 = P05556_pred_.drop(['SMILES'], axis=1)


df_for_model_P05556.head(2)


# Generate predictions on unseen data
predictions = rf_P05556.predict(df_for_model_P05556)

PRINT(f'Finished predicting on unseen data.')


PRINT(f'Prediction shape: {predictions.shape}\n\nVisualize few predictions:\n\n{predictions[30:80]}')





# Generate list holding UniProts instead their labeled classes
labeled_predictions = [P05556_label_dict[prediction] for prediction in predictions]


PRINT(f'Labeled prediction (UniProt):\n\n{labeled_predictions[30:45]}')


P05556_pred['PredictedUniProtPartner'] = labeled_predictions

PRINT('Merged the Predictions !')


P05556_pred.head(3)


P05556_pred.to_csv(os.path.join('predictions','P05556_pred_.csv'), index=False)

PRINT('Predictions Saved!')





P05106_label_dict = {0: 'P06756', 1: 'P08514', 2: 'P17301', 3: 'P26006'}


P05106_pred = target_dataframes['P05106'].copy()


P05106_pred.head(2)





P05106_pred = P05106_pred.reset_index(drop=True)

PRINT(f'Reseted the indexes of the data frame in order to avoid issues with features generation')


PRINT(f'Shape:\n\n{P05106_pred.shape}')


P05106_pred.drop(['UniProtTarget'],axis=1, inplace=True)


# Genera copy of the data frame for our features
P05106_pred_ = P05106_pred.copy()

# Apply the `calculate_descriptors` method in order to generate 8 new features for df
P05106_pred_['MolecularDescriptors'] = P05106_pred_['SMILES'].apply(calculate_descriptors)

# Transfer the array at each row under the 'MolecularDescriptors' column into column with their corresponding names & drop the colunn
P05106_pred_[['MolWt', 'NumValenceElectrons', 'TPSA', 'MolLogP', 'NumHeteroatoms', 'NumRotatableBonds', 'HeavyAtomCount', 'FractionCSP3']] = pd.DataFrame(P05106_pred_['MolecularDescriptors'].tolist(), index=P05106_pred_.index)
P05106_pred_.drop(columns=['MolecularDescriptors'], axis=1, inplace=True)

# Reorder the columns names so that the label column will be the last column in df
P05106_pred_ = P05106_pred_[['SMILES', 'MolWt', 'NumValenceElectrons', 'TPSA', 'MolLogP', 'NumHeteroatoms', 'NumRotatableBonds', 'HeavyAtomCount', 'FractionCSP3']]

PRINT(f'Done generating features for prediction !')


P05106_pred_.head(2)


PRINT(f'Shape after generating features using Morgan Fingerprints:\n\n{P05106_pred_.shape}')


rf_P05106 = load('models/Best Model of each UniProt Modified/rf_P05106_RDKD.joblib')
PRINT(f'Model Loaded Successfully !')


# Drop SMILES column for prediction
df_for_model_P05106 = P05106_pred_.drop(['SMILES'], axis=1)


predictions = rf_P05106.predict(df_for_model_P05106)

PRINT(f'Finished predicting on unseen data.')


PRINT(f'Prediction shape: {predictions.shape}\n\nVisualize few predictions:\n\n{predictions[:30]}')


# Generate list holding UniProts instead their labeled classes
labeled_predictions = [P05106_label_dict[prediction] for prediction in predictions]


PRINT(f'Labeled predictions (UniProt):\n\n{labeled_predictions[:15]}')


# Merge predictions with our data frame
P05106_pred['PredictedUniProtPartner'] = labeled_predictions

PRINT('Merged the Predictions !')


P05106_pred.head(5)


P05106_pred.to_csv(os.path.join('predictions','P05106_pred_.csv'), index=False)

PRINT('Predictions Saved!')





P08648_label_dict = {0: 'P05556', 1: 'P06756', 2: 'P32297'}


P08648_pred = target_dataframes['P08648'].copy()

P08648_pred.head(2)


P08648_pred = P08648_pred.reset_index(drop=True)

PRINT(f'Reseted the indexes of the data frame in order to avoid issues with features generation')


PRINT(f'Visualize data frame shape: {P08648_pred.shape}')


P08648_pred.drop(['UniProtTarget'],axis=1, inplace=True)


P08648_pred_ = GenerateMorganFingerprintsFeaturesByMoleculeSMILES(df=P08648_pred, size=1024, radius=2)
PRINT(f'Done generating features for prediction !')


P08648_pred_.head(2)


PRINT(f'Shape after generating features using Morgan Fingerprints:\n\n{P08648_pred_.shape}')


xgb_P08648 = load('models/Best Model of each UniProt Modified/xgb_P08648_MF.joblib')
PRINT(f'Loaded Model Successfully !')


# Drop SMILES column for predictions
df_for_model_P08648 = P08648_pred_.drop(['SMILES'], axis=1)


df_for_model_P08648.head(2)


# Generate predictions on unseen data
predictions = xgb_P08648.predict(df_for_model_P08648)

PRINT(f'Finished predicting on unseen data.')


PRINT(f'Prediction shape: {predictions.shape}\n\nVisualize few predictions:\n\n{predictions[0:10]}')


predictions = [int(value) for value in predictions]





# Generate list holding UniProts instead their labeled classes
labeled_predictions = [P08648_label_dict[prediction] for prediction in predictions]


# Merge predictions with our data frame
P08648_pred['PredictedUniProtPartner'] = labeled_predictions

PRINT('Merged the Predictions !')


P08648_pred.head(2)


P08648_pred.to_csv(os.path.join('predictions','P08648_pred_.csv'), index=False)

PRINT('Predictions Saved!')








prediction_dir = 'predictions'


P13612_df = pd.read_csv(os.path.join(prediction_dir, 'P13612_pred.csv'))


P13612_df['UniProtTarget'] = 'P13612'


P13612_df.head(3)


P05556_df = pd.read_csv(os.path.join(prediction_dir, 'P05556_pred_.csv'))
P05556_df['UniProtTarget'] = 'P05556'
P05556_df.head(3)


P05107_df = pd.read_csv(os.path.join(prediction_dir, 'P05107_pred.csv'))
P05107_df['UniProtTarget'] = 'P05107'
P05107_df.head(3)


P05106_df = pd.read_csv(os.path.join(prediction_dir, 'P05106_pred_.csv'))
P05106_df['UniProtTarget'] = 'P05106'
P05106_df.head(3)


P08648_df = pd.read_csv(os.path.join(prediction_dir, 'P08648_pred_.csv'))
P08648_df['UniProtTarget'] = 'P08648'
P08648_df.head(3)


P17301_df = pd.read_csv(os.path.join(prediction_dir, 'P17301_pred.csv'))
P17301_df['UniProtTarget'] = 'P17301'
P17301_df.head(3)





df_list = [P13612_df, P05556_df, P05107_df, P05106_df, P08648_df, P17301_df]


combined_df = pd.concat(df_list, ignore_index=True)


combined_df


new_order = ['SMILES', 'UniProtTarget', 'PredictedUniProtPartner']


combined_df = combined_df[new_order]


combined_df.head(3)


combined_df.to_csv('prediction_df_modified.csv', index=False)

PRINT('SAVED & DONE !')





old_df = pd.read_csv(os.path.join('data','dataset_for_prediction.csv'))


PRINT(f'Shapes check:\n\n{old_df.shape[0]}\n\nvs.\n\n{combined_df.shape[0]}')





PRINT(f'-------------------------------------------------------------')





import os
import pandas as pd

def PRINT(text) -> None: print(f"{80*'-'}\n{text}\n{80*'-'}")


timbal_dataset_df = pd.read_csv(os.path.join('data', 'timbal_triplets.csv'))

PRINT(f'Loaded Timbal dataset csv file to pandas data frame successfully')


timbal_dataset_df.head()


predictions_df = pd.read_csv('predictions.csv')

PRINT(f'Loaded predictions csv file into pandas data frame successfully !')


predictions_df.head()


timbal_columns = list(timbal_dataset_df.columns)
predictions_columns = list(predictions_df.columns)


PRINT(f'Timbal data frame columns -> {timbal_columns}\n\nPredictions data frame columns -> {predictions_columns}')


merged_df = pd.merge(predictions_df, timbal_dataset_df, left_on=['SMILES', 'UniProtTarget'], right_on=['smiles', 'uniprot_target'], how='left')

PRINT(f'Merged the data frames by molecule SMILES value successfully !')


merged_df.shape


n= merged_df.duplicated().sum()

n


merged_df.columns


merged_df['target_name'].unique()


PRINT(f'Merged data frame columns -> {list(merged_df.columns)}')


merged_df.drop(columns=['timbal_v2_id', 'smiles', 'uniprot_target'], axis=1, inplace=True)


# Rename the column 'target_name' to 'Target Pref Name'
merged_df.rename(columns={'target_name': 'Target Pref Name'}, inplace=True)


merged_df.head()


merged_df.shape



