


import deepchem as dc
import numpy as pd
import pandas as pd
import seaborn as sns
import pickle # Inorder to save data frame dictionary
import os


def PRINT(text) -> None: print(f"{'~'*80}\n{text}\n{'~'*80}")








pwd


pred_dataset_path = "data/dataset_for_prediction.csv"
ChEMBL_integrin_dataset_path = "data/ChEMBL_Integrins.csv"


pred_df = pd.read_csv(pred_dataset_path)

pred_df.head(5)





pred_df = pred_df.rename(columns={'uniprot_id1':'uniprot_id'})

PRINT(f'Renamed column name: {pred_df.columns[1]}')


chmbl_df = pd.read_csv(ChEMBL_integrin_dataset_path)

chmbl_df.head(5)








unique_proteins = pred_df["uniprot_id1"].unique()


PRINT(f"The unique proteins we want to predict their partners in the PPI are :\n {unique_proteins}\n\nThe are total {len(unique_proteins)} such proteins")





protein_dataframes = {}

for protein in unique_proteins:
    # Initialize an empty list to store rows for the current protein
    rows_for_protein = []

    # Iterate over each row in the ChEMBL DataFrame
    for index, row in chmbl_df.iterrows():
        # Check if the current protein is present in any of the UniProt columns
        if protein in row[['UniProt1', 'UniProt2', 'UniProt3', 'UniProt4', 'UniProt5']].values:
            # Determine the correct order (UniProt1 and UniProt2) in the new data frame
            if row['UniProt1'] == protein:
                relevant_info = [row['Canonical SMILES(RDKit)'], row['UniProt1'], row['UniProt2']]
            elif row['UniProt2'] == protein:
                relevant_info = [row['Canonical SMILES(RDKit)'], row['UniProt2'], row['UniProt1']]
            else:
                relevant_info = []

            if relevant_info:
                rows_for_protein.append(relevant_info)
                
    if rows_for_protein:
        protein_dataframes[protein] = pd.DataFrame(rows_for_protein, columns=['SMILES', 'UniProt1', 'UniProt2'])



protein_dataframes['P13612']





directory_path = 'obj'

# Save the dictionary to a file in the specified directory
with open(os.path.join(directory_path, 'data_frames_dictionary.pkl'), 'wb') as file:
    pickle.dump(protein_dataframes, file)





out_dir = 'unique UniProt csv files'


for protein, df in protein_dataframes.items():
    try:
        # Generate csv file name with the desired format
        file_name = f'{protein}.csv'

        # Specify full path
        out_path = os.path.join(out_dir, file_name)

        # Save current data frame as csv file
        df.to_csv(out_path, index=False)

        PRINT(f'Saved data frame for {protein} as {file_name}')
        
    except Exception as e:
        PRINT(f'Error!\nVerify path name and the data')
    





PRINT(f'We have {len(protein_dataframes.items())} data frames to visualize information about their data distributions')


PRINT(f'UniProt_ids -> {unique_proteins}')








def one_hot_encoding(df):
    
    df_encoded = pd.get_dummies(df[['UniProt1', 'UniProt2']], prefix='', prefix_sep='').astype(int)
    df_encoded = pd.concat([df[['SMILES']], df_encoded], axis=1)
    return df_encoded





def visualize_dist(df, target_prot)-> None:
    # Melt the DataFrame to long format for Seaborn countplot
    df_melted = df.melt(var_name='Protein', value_name='Interaction Status')

    # Set the size of the plot
    sns.set(rc={'figure.figsize':(12, 8)})
    
    sns.set_context("notebook", rc={"lines.linewidth": 2.5})
    # Create a grouped count plot
    sns.countplot(x='Protein', hue='Interaction Status', palette=["lightgrey", "skyblue"], data=df_melted)

    # Add labels and title
    plt.xlabel('Protein')
    plt.ylabel('Count')
    plt.title(f'PPI with -> {target_prot}')

    sns.despine()
    sns.set_theme(style="whitegrid")
    sns.despine(offset=10, trim=True)
    sns.set_context("notebook")
    plt.show()





def filter_proteins_list(df, columns_to_remove):
    
    filtered_columns = [col for col in df.columns if col not in columns_to_remove]
    filtered_columns_list = list(filtered_columns)
    return filtered_columns





first_df = protein_dataframes[unique_proteins[0]]

first_df.head(2)





first_df_encoded = one_hot_encoding(first_df)


print(first_df_encoded.columns)


first_df_encoded.head(3)


filtered_columns = filter_proteins_list(first_df_encoded, columns_to_remove = ['SMILES', 'P13612'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_1 = first_df_encoded[filtered_columns]

temp_df_1.head(2)


visualize_dist(temp_df_1, unique_proteins[0])








PRINT(f'The size of the data frame is -> {len(first_df)}')
print(f'Number of times P05556 appears -> {len(first_df[first_df["UniProt2"] == "P05556"])}')
print(f'Number of times P26010 appears -> {len(first_df[first_df["UniProt2"] == "P26010"])}')
print(f'Size check -> {({len(first_df)} == {(len(first_df[first_df["UniProt2"] == "P05556"]) + len(first_df[first_df["UniProt2"] == "P26010"]))})}')

PRINT('Done.')





second_df = protein_dataframes[unique_proteins[1]]

second_df.head(2)





second_df_encoded = one_hot_encoding(second_df)


second_df_encoded.columns


second_df_encoded.head(3)


filtered_columns = filter_proteins_list(second_df_encoded, columns_to_remove = ['SMILES', 'P05556'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_2 = second_df_encoded[filtered_columns]

temp_df_2.head(5)


visualize_dist(temp_df_2, unique_proteins[1])








PRINT(f'The size of the data frame is -> {len(second_df)}')
print(f'Number of time O75578 appears -> {len(second_df[second_df["UniProt2"] == "O75578"])}')
print(f'Number of time P23229 appears -> {len(second_df[second_df["UniProt2"] == "P23229"])}')
print(f'Number of time P56199 appears -> {len(second_df[second_df["UniProt2"] == "P56199"])}')
print(f'Number of time Q13797 appears -> {len(second_df[second_df["UniProt2"] == "Q13797"])}')
print(f'Number of time P17301 appears -> {len(second_df[second_df["UniProt2"] == "P17301"])}')
print(f'Number of time P05106 appears -> {len(second_df[second_df["UniProt2"] == "P05106"])}')
print(f'Number of time P06756 appears -> {len(second_df[second_df["UniProt2"] == "P06756"])}')
print(f'Number of time P08648 appears -> {len(second_df[second_df["UniProt2"] == "P08648"])}')
print(f'Number of time P13612 appears -> {len(second_df[second_df["UniProt2"] == "P13612"])}')

PRINT('Done.')





third_df = protein_dataframes[unique_proteins[2]]

third_df.head(2)





third_df_encoded = one_hot_encoding(third_df)


third_df_encoded.columns


third_df_encoded.head(3)


filtered_columns = filter_proteins_list(third_df_encoded, columns_to_remove = ['SMILES', 'P05106'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_3 = third_df_encoded[filtered_columns]

temp_df_3.head(5)


visualize_dist(temp_df_3, unique_proteins[2])





PRINT(f'The size of the data frame is -> {len(third_df)}')
print(f'Number of time P17301 appears -> {len(third_df[third_df["UniProt2"] == "P17301"])}')
print(f'Number of time P05556 appears -> {len(third_df[third_df["UniProt2"] == "P05556"])}')
print(f'Number of time P26006 appears -> {len(third_df[third_df["UniProt2"] == "P26006"])}')
print(f'Number of time P06756 appears -> {len(third_df[third_df["UniProt2"] == "P06756"])}')
print(f'Number of time P08514 appears -> {len(third_df[third_df["UniProt2"] == "P08514"])}')

PRINT('Done.')





fourth_df = protein_dataframes[unique_proteins[3]]

fourth_df.head(2)





fourth_df_encoded = one_hot_encoding(fourth_df)


fourth_df_encoded.columns


fourth_df_encoded.head(3)


filtered_columns = filter_proteins_list(fourth_df_encoded, columns_to_remove=['SMILES', 'P05107'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_4 = fourth_df_encoded[filtered_columns]

temp_df_4.head(2)


visualize_dist(temp_df_4, unique_proteins[3])








PRINT(f'The size of the data frame is -> {len(fourth_df)}')
print(f'Number of time P11215 appears -> {len(fourth_df[fourth_df["UniProt2"] == "P11215"])}')
print(f'Number of time P20701 appears -> {len(fourth_df[fourth_df["UniProt2"] == "P20701"])}')

PRINT('Done.')





fifth_df = protein_dataframes[unique_proteins[4]]

fifth_df.head(2)





fifth_df_encoded = one_hot_encoding(fifth_df)


fifth_df_encoded.columns


fifth_df_encoded.head(3)


filtered_columns = filter_proteins_list(fifth_df_encoded, columns_to_remove=['SMILES', 'P08648'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_5 = fifth_df_encoded[filtered_columns]


visualize_dist(temp_df_5, unique_proteins[4])








PRINT(f'The size of the data frame is -> {len(fifth_df)}')
print(f'Number of time P05556 appears -> {len(fifth_df[fifth_df["UniProt2"] == "P05556"])}')
print(f'Number of time P0756 appears -> {len(fifth_df[fifth_df["UniProt2"] == "P06756"])}')

PRINT('Done.')





sixth_df = protein_dataframes[unique_proteins[5]]

sixth_df.head(2)





sixth_df_encoded = one_hot_encoding(sixth_df)


sixth_df_encoded.columns


sixth_df_encoded.head(3)


filtered_columns = filter_proteins_list(sixth_df_encoded, columns_to_remove=['SMILES', 'P17301'])
PRINT(f'Filtered columns -> {filtered_columns}')


temp_df_6 = sixth_df_encoded[filtered_columns]

temp_df_6.head(2)


visualize_dist(temp_df_6, unique_proteins[5])





PRINT(f'The size of the data frame is -> {len(sixth_df)}')
print(f'Number of time P05106 appears -> {len(sixth_df[sixth_df["UniProt2"] == "P05106"])}')
print(f'Number of time P05556 appears -> {len(sixth_df[sixth_df["UniProt2"] == "P05556"])}')

PRINT('Done.')





encoded_dir_path = 'one hot encoded csv files for training'

first_df_encoded.to_csv(os.path.join('one hot encoded csv files for training/first_df_encoded.csv'), index=False)
second_df_encoded.to_csv(os.path.join('one hot encoded csv files for training/second_df_encoded.csv'), index=False)
third_df_encoded.to_csv(os.path.join('one hot encoded csv files for training/third_df_encoded.csv'), index=False)
fourth_df_encoded.to_csv(os.path.join('one hot encoded csv files for training/fourth_df_encoded.csv'), index=False)
fifth_df_encoded.to_csv(os.path.join('one hot encoded csv files for training/fifth_df_encoded.csv'), index=False)
sixth_df_encoded.to_csv(os.path.join('one hot encoded csv files for training/sixth_df_encoded.csv'), index=False)

PRINT('Done.')








import pickle # In order to load the saved data frames dictionary
from rdkit import Chem
from rdkit.Chem import PandasTools
from rdkit.Chem import Descriptors
from rdkit.Chem import rdmolops


import deepchem as dc
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from deepchem.feat import RDKitDescriptors
from deepchem.models import GraphConvModel
from deepchem.hyper import GridHyperparamOpt
from deepchem.splits.splitters import RandomGroupSplitter
from deepchem.hyper import HyperparamOpt
from deepchem.trans import undo_transforms
from deepchem.trans.transformers import BalancingTransformer








dict_path = 'obj/data_frames_dictionary.pkl'


try:
    with open('obj/data_frames_dictionary.pkl', 'rb') as file:
        df_dict = pickle.load(file)
        PRINT(f'Done.')
except Exception as e:
    PRINT(f'Error in loading the saved data frames dicitonary from obj dir')


prot_ls = list(df_dict.keys())

PRINT(f'Unique proteins -> {prot_ls}')





csv_dir_path = 'one hot encoded csv files for training' 


first_ds = pd.read_csv(os.path.join(csv_dir_path, 'first_df_encoded.csv'))


first_ds.head(3)


first_ds.drop('P13612', axis=1, inplace=True)

first_ds.head(2)


def generate_graph_conv_model(n_tasks, learning_rate):
    batch_size = 64
    model = GraphConvModel(n_tasks=n_tasks, 
                           batch_size=batch_size,
                           mode = 'classification',
                           model_dir = 'models',
                           n_classes=2,
                          learning_rate=learning_rate)
    return model           


def gc_model_builder(**model_params):
    learning_rate = model_params['learning_rate']
    dropout = model_params['dropout']
    batch_size = model_params['batch_size']
    
    return GraphConvModel(n_tasks=n_tasks, 
                           batch_size=batch_size,
                           mode = 'classification',
                           model_dir = 'models',
                           n_classes=2,
                          learning_rate=learning_rate)


import deepchem as dc
import numpy as np
from deepchem.models import GraphConvModel
from deepchem.hyper import GridHyperparamOpt

# Define the hyperparameter grid
params_dict = {
    'learning_rate': [1e-3, 1e-4],
    'dropout': [0.2, 0.5],
    'batch_size': [32, 64]
}

tasks = list(first_ds.columns[1:])
featurizer = dc.feat.ConvMolFeaturizer()
loader = dc.data.CSVLoader(tasks=tasks,
                           smiles_field='SMILES',
                           featurizer=featurizer)
dataset = loader.featurize('first_ds_for_train.csv')

splitter = dc.splits.RandomSplitter()
metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean, mode='classification'),
           dc.metrics.Metric(dc.metrics.accuracy_score, np.mean, mode='classification')]

training_score_list = []
validation_score_list = []
transformers = []
cv_folds = 10

# Define the model builder function
def model_builder(learning_rate, dropout, batch_size):
    return GraphConvModel(n_tasks=2,
                          batch_size=batch_size,
                          mode='classification',
                          model_dir='models',
                          n_classes=2,
                          learning_rate=learning_rate,
                          dropout=dropout)

# Create a hyperparameter optimization object
opt = GridHyperparamOpt(model_builder, params_dict)

for i in range(0, cv_folds):
    # Get the hyperparameters from the optimization object
    hyperparams = opt.get_next_hyperparams()

    # Build the model using the hyperparameters
    model = model_builder(**hyperparams)

    res = splitter.train_valid_test_split(dataset)
    train_dataset, valid_dataset, test_dataset = res

    model.fit(train_dataset, nb_epoch=10)

    # Train step
    train_scores = model.evaluate(train_dataset, metrics, transformers)
    training_score_list.append(train_scores['mean-roc_auc_score'])

    # Validation step
    validation_scores = model.evaluate(valid_dataset, metrics, transformers)
    validation_score_list.append(validation_scores['mean-roc_auc_score'])

# Print the best hyperparameters found during the search
best_hyperparams = opt.get_best_hyperparams()
print("Best Hyperparameters:", best_hyperparams)



params_dict = {
    'learning_rate': [1e-3, 1e-4],
    'dropout': [0.2, 0.5],
    'batch_size': [32, 64]
}

opt = GridHyperparamOpt(GraphConvModel,{},  params_dict)


tasks = list(first_ds.columns[1:])
featurizer = dc.feat.ConvMolFeaturizer()
loader = dc.data.CSVLoader(tasks=tasks,
                           smiles_field='SMILES',
                           featurizer=featurizer)
dataset = loader.featurize('first_ds_for_train.csv')


dataset


splitter = dc.splits.RandomSplitter()


metrics = [dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean, mode='classification')]


res = splitter.train_valid_test_split(dataset)
train_dataset, valid_dataset, test_dataset = res


PRINT(f'Shapes (train, valid, test) -> : {(train_dataset.X.shape, valid_dataset.X.shape, test_dataset.X.shape)}')


training_score_list = []
validation_score_list = []
transformers = []
cv_folds = 10


for i in range(0, cv_folds):
    model = generate_graph_conv_model(2)
    res = splitter.train_valid_test_split(dataset)
    
    train_dataset, valid_dataset, test_dataset = res
    
    model.fit(train_dataset, nb_epoch=10)
    
    # Train step
    train_scores = model.evaluate(train_dataset, metrics,
                                  transformers)

    training_score_list.append(
        train_scores['mean-roc_auc_score'])
    
    # Validation step
    validation_scores = model.evaluate(valid_dataset, metrics,
                                       transformers)
    validation_score_list.append(
        validation_scores['mean-roc_auc_score'])


PRINT(training_score_list)
PRINT(validation_score_list)


# Create a DataFrame with the data
data = {
    'Group': ["training"] * cv_folds + ["validation"] * cv_folds,
    'Score': training_score_list + validation_score_list
}

# Create a boxplot
sns.boxplot(x="Group", y="Score", data=data)

# Show the plot
plt.show()


pred = [x.flatten() for x in model.predict(valid_dataset)]


tasks


pred_df = pd.DataFrame(pred, columns=['0_P05556','1_P05556','0_P26010','1_P26010'])


pred_df.to_csv('pred_temp.csv', index=False)


valid_dataset.y.shape


y_transposed = valid_dataset.y.transpose()


y_transposed.shape


pred_df[tasks[0]] = [int(x) for x in y_transposed[0]]
pred_df[tasks[1]] = [int(x) for x in y_transposed[1]]
pred_df["SMILES"] = valid_dataset.ids


pred_df.head(10)


# Store the score for later
first_model_store_train_1 = training_score_list
first_model_store_valid_1 = validation_score_list



